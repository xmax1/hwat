{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path exists, leaving alone\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from jax.config import config\n",
    "config.update('jax_disable_jit', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "from functools import partial, reduce\n",
    "import wandb\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import random as rnd\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.training import common_utils\n",
    "from flax import linen as nn, jax_utils\n",
    "import optax\n",
    "\n",
    "from pyfig import Pyfig\n",
    "\n",
    "c = Pyfig(wandb_mode='disabled', notebook=True)\n",
    "c.print()\n",
    "\n",
    "class FermiNet(nn.Module):\n",
    "  \n",
    "  n_sv: int\n",
    "  n_pv: int\n",
    "  n_fb: int\n",
    "  n_fb_out: int\n",
    "  n_det: int\n",
    "\n",
    "  af_fb: Callable\n",
    "  af_fb_out: Callable\n",
    "  af_pseudo: Callable\n",
    "\n",
    "  compute_s_emb: Callable\n",
    "  compute_p_emb: Callable\n",
    "  \n",
    "  n_e: int\n",
    "  n_u: int\n",
    "  n_d: int = n_e-n_u\n",
    "  p_mask_u, p_mask_d = create_masks(n_e, n_u)\n",
    "  compute_s_perm: Callable = partial(compute_s_perm, p_mask_u=p_mask_u, p_mask_d=p_mask_d, n_u=n_u)\n",
    "\n",
    "  pbc:          bool = False\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(_i, x):\n",
    "    \n",
    "    n_e, n_u, n_d, n_det = _i.n_e, _i.n_u, _i.n_d, _i.n_det\n",
    "\n",
    "    x_s_var = _i.compute_s_emb(x)\n",
    "    x_p_var = _i.compute_p_emb(x)\n",
    "\n",
    "    x_s_res = x_p_res = 0.\n",
    "    for _ in range(_i.n_fb):\n",
    "        x_p_var = x_p_res = _i.af_fb(nn.Dense(_i.n_pv)(x_p_var)) + x_p_res\n",
    "        \n",
    "        x_s_var = _i.compute_s_perm(x_s_var, x_p_var)\n",
    "        x_s_var = x_s_res = _i.af_fb(nn.Dense(_i.n_sv)(x_s_var)) + x_s_res\n",
    "\n",
    "    x_w = jnp.concatentate([x_s_var, x_p_var], axis=-1)\n",
    "    x_w = _i.af_fb_out(nn.Dense(_i.n_fb_out)(x_w))\n",
    "    x_w = _i.af_pseudo(nn.Dense(n_det*n_e)(x_w))\n",
    "    x_wu, x_wd = jnp.split(x_w, [n_u, n_d], axis=0)\n",
    "    \n",
    "    orb_u = jnp.split(x_wu * jnp.exp(-nn.Dense(n_u*n_det)), n_det, axis=1)\n",
    "    orb_d = jnp.split(x_wd * jnp.exp(-nn.Dense(n_d*n_det)), n_det, axis=1)\n",
    "\n",
    "    log_psi, sgn = logabssumdet(orb_u, orb_d)\n",
    "\n",
    "    return log_psi\n",
    "\n",
    "model = FermiNet(**c.dict)\n",
    "n_e = 10\n",
    "rng_model = rnd.PRNGKey(1)\n",
    "x = jnp.ones(n_e, 3)\n",
    "params = model.init(rng_model, x)\n",
    "model.apply(params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_s_perm(x, x_p, p_mask_u, p_mask_d, n_u):\n",
    "    n_e, _ = x_p.shape\n",
    "    n_u_ish, _ = x.shape\n",
    "    n_d = n_e - n_u_ish\n",
    "    n_u = n_e - n_d\n",
    "\n",
    "    xu, xd = jnp.split(x, [n_u, n_d], axis=0)\n",
    "    mean_xu = jnp.mean(xu, axis=0, keepdims=True)\n",
    "    mean_xd = jnp.mean(xd, axis=0, keepdims=True)\n",
    "\n",
    "    x_p = jnp.expand_dims(x_p, axis=0)\n",
    "    sum_p_u = (p_mask_u * x_p).sum((1, 2)) / float(n_u)\n",
    "    sum_p_d = (p_mask_d * x_p).sum((1, 2)) / float(n_d)\n",
    "\n",
    "    x = jnp.concatenate((x, mean_xu, mean_xd, sum_p_u, sum_p_d), axis=-1)\n",
    "    return jnp.split(x, [n_u, n_d], axis=0)\n",
    "\n",
    "def logabssumdet(orb_u, orb_d=None):\n",
    "    \n",
    "    xs = [orb_u, orb_d] if not orb_d is None else [orb_u]\n",
    "    \n",
    "    dets = [x.reshape(-1) for x in xs if x.shape[-1] == 1]\n",
    "    dets = reduce(lambda a,b: a*b, dets) if len(dets)>0 else 1\n",
    "\n",
    "    slogdets = [jnp.linalg.slogdet(x) for x in xs if x.shape[-1] > 1]\n",
    "    \n",
    "    if len(slogdets) > 0: # at least 2 electon in at least 1 orbital\n",
    "        sign_in, logdet = reduce(lambda a,b: (a[0]*b[0], a[1]+b[1]), slogdets)\n",
    "        maxlogdet = jnp.max(logdet)\n",
    "        det = sign_in * dets * jnp.exp(logdet-maxlogdet)\n",
    "    else:\n",
    "        maxlogdet = 0\n",
    "        det = dets\n",
    "\n",
    "    psi_ish = jnp.sum(det)\n",
    "    sgn_psi = jnp.sign(psi_ish)\n",
    "    log_psi = jnp.log(jnp.abs(psi_ish)) + maxlogdet\n",
    "    return log_psi, sgn_psi\n",
    "\n",
    "def create_masks(n_electrons, n_up):\n",
    "    ups = jnp.ones(n_electrons)\n",
    "    ups[n_up:] = 0.\n",
    "    downs = (ups-1.)*-1.\n",
    "\n",
    "    pairwise_up_mask = []\n",
    "    pairwise_down_mask = []\n",
    "    for electron in range(n_electrons):\n",
    "        mask_up = jnp.zeros((n_electrons, n_electrons))\n",
    "        mask_up[electron, :] = ups\n",
    "        pairwise_up_mask.append(mask_up)\n",
    "        # mask_up = mask_up[eye_mask].reshape(-1) # for when drop diagonal enforced\n",
    "        mask_down = jnp.zeros((n_electrons, n_electrons))\n",
    "        mask_down[electron, :] = downs\n",
    "        pairwise_down_mask.append(mask_down)\n",
    "\n",
    "    pairwise_up_mask = jnp.stack(pairwise_up_mask, axis=0)[..., None]\n",
    "    pairwise_down_mask = jnp.stack(pairwise_down_mask, axis=0)[..., None]\n",
    "    return pairwise_up_mask, pairwise_down_mask\n",
    "\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(d:dict):\n",
    "    ...\n",
    "    metrics = lax.pmean(metrics, axis_name='b')\n",
    "\n",
    "    return metric\n",
    "\n",
    "\n",
    "def compute_energy():\n",
    "    \n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_u' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 64\u001b[0m\n\u001b[1;32m     60\u001b[0m         acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mmean(p_mask)\n\u001b[1;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m x, acc\n\u001b[0;32m---> 64\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mFermiNet\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m     66\u001b[0m   n_sv: \u001b[39mint\u001b[39m\n\u001b[1;32m     67\u001b[0m   n_pv: \u001b[39mint\u001b[39m\n",
      "Cell \u001b[0;32mIn [28], line 81\u001b[0m, in \u001b[0;36mFermiNet\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m n_e: \u001b[39mint\u001b[39m\n\u001b[1;32m     80\u001b[0m n_u: \u001b[39mint\u001b[39m\n\u001b[0;32m---> 81\u001b[0m n_d: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m n_e\u001b[39m-\u001b[39mn_u\n\u001b[1;32m     82\u001b[0m p_mask_u, p_mask_d \u001b[39m=\u001b[39m create_masks(n_e, n_u)\n\u001b[1;32m     83\u001b[0m compute_s_perm: Callable \u001b[39m=\u001b[39m partial(compute_s_perm, p_mask_u\u001b[39m=\u001b[39mp_mask_u, p_mask_d\u001b[39m=\u001b[39mp_mask_d, n_u\u001b[39m=\u001b[39mn_u)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_u' is not defined"
     ]
    }
   ],
   "source": [
    "n_e = 10\n",
    "x = jnp.ones((2, n_e, 3))\n",
    "xu, xd = x.split(2, axis=1)\n",
    "\n",
    "def init_walker(xu, xd):\n",
    "    rng = rnd.PRNGKey(c.seed)\n",
    "    rng_u, rng_d = rnd.split(rng, 2)\n",
    "    return jnp.concatenate([rnd.normal(rng_u, xu.shape), rnd.normal(rng_d, xd.shape)], axis=1)\n",
    "\n",
    "from jax import pmap\n",
    "\n",
    "class SampleState():\n",
    "    \"\"\"\n",
    "    @struct.dataclass\n",
    "    struct.PyTreeNode means jax transformations *do not affect it* eg pmap\n",
    "    fn.apply << apply vs fn() << apply_fn\"\"\"\n",
    "    \n",
    "    def __init__(_i, step=0, move_std=0.02, corr_len=20):\n",
    "        _i.acc_target = 0.5\n",
    "        _i.corr_len = corr_len//2\n",
    "        _i.move_std = move_std\n",
    "        _i.step = step\n",
    "    \n",
    "    def __call__(_i, x, state:TrainState):\n",
    "        _i.rng, rng_0, rng_1, rng_move = rnd.split(_i.rng, 4)\n",
    "\n",
    "        x, acc_0 = sample(rng_0, x, state, _i.corr_len, _i.move_std)\n",
    "        move_std_1 = jnp.clip(_i.move_std + 0.001*rnd.normal(rng_move))\n",
    "        x, acc_1 = sample(rng_1, x, state, _i.corr_len, move_std_1)\n",
    "\n",
    "        mask = jnp.array((_i.acc_target-acc_0)**2 < (_i.acc_target-acc_1)**2, dtype=jnp.float32)\n",
    "        not_mask = ((mask-1.)*-1.)\n",
    "        _i.move_std = mask*_i.move_std + not_mask*move_std_1\n",
    "        return x\n",
    "\n",
    "def sample(rng, x, state:TrainState, corr_len, move_std):\n",
    "\n",
    "    def move(x, rng, move_std):\n",
    "        x = x + rnd.normal(rng, x.shape)*move_std\n",
    "        return x\n",
    "\n",
    "    to_prob = lambda log_psi: jnp.exp(log_psi)**2\n",
    "    \n",
    "    p = to_prob(state(x))\n",
    "    \n",
    "    acc = 0.0\n",
    "    for _ in range(corr_len//2):\n",
    "        rng, rng_move, rng_alpha = rnd.split(rng)\n",
    "        \n",
    "        x_1 = move(x, rng_move, move_std)\n",
    "        p_1 = to_prob(state(x_1))\n",
    "\n",
    "        p_mask = (p_1 / p) > rnd.uniform(rng_alpha, p_1.shape)\n",
    "        p = jnp.where(p_mask, p_1, p)\n",
    "        p_mask = jnp.expand_dims(p, axis=(-1, -2))\n",
    "        x = jnp.where(p_mask, x_1, x)\n",
    "\n",
    "        acc += jnp.mean(p_mask)\n",
    "\n",
    "    return x, acc\n",
    "\n",
    "\n",
    "# x = init_walker(xu, xd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[[0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               ...,\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ]],\n",
       "\n",
       "              [[0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               ...,\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ]],\n",
       "\n",
       "              [[0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               ...,\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ]],\n",
       "\n",
       "              ...,\n",
       "\n",
       "              [[0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               ...,\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ]],\n",
       "\n",
       "              [[0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               ...,\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ]],\n",
       "\n",
       "              [[0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               ...,\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ],\n",
       "               [0.31068158, 1.        , 0.9456457 ]]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# jax.tree_map(lambda x: x.shape, params) # Check the parameters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_train_state(c: Pyfig):\n",
    "  model = CNN()\n",
    "  params = model.init(c.rng_init, jnp.ones([1, 28, 28, 1]))['params']\n",
    "  tx = optax.sgd(c.lr)\n",
    "  return TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "state = create_train_state(c)\n",
    "state = jax_utils.replicate(state)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, state, b):\n",
    "  \n",
    "  b_energy = compute_energy(state)\n",
    "  \n",
    "  def loss_fn(p):\n",
    "    model_out = state.apply_fn({'params':p}, b)\n",
    "    log_psi, sgn = model_out\n",
    "    return log_psi, sgn\n",
    "  \n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  (log_psi, sgn), grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  \n",
    "  variables = {\n",
    "    'E batch'           : b_energy,\n",
    "    r'$\\log\\psi$ batch' : log_psi,\n",
    "    r'sgn$(\\cdot)$'     : sgn\n",
    "  }\n",
    "  \n",
    "  metrics = compute_metric(b_energy, log_psi, sgn)\n",
    "  return state, metrics\n",
    "\n",
    "train_step = jax.pmap(train_step, axis_name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior_mean_variance(img, t, x0, v, ddpm):\n",
    "\n",
    "    beta = ddpm_param.betas[t,None,None,None] # only needed when t > 0\n",
    "    alpha = ddpm_param.alphas[t,None,None,None]\n",
    "    alpha_bar = ddpm_param.alphas_bar[t,None,None,None]\n",
    "    alpha_bar_last = ddpm_param.alphas_bar[t-1,None,None,None]\n",
    "    sqrt_alpha_bar_last = ddpm_param.sqrt_alphas_bar[t-1,None,None,None]\n",
    "\n",
    "    coef_x0 = beta * sqrt_alpha_bar_last / (1. - alpha_bar)\n",
    "    coef_xt = (1. - alpha_bar_last) * jnp.sqrt(alpha) / ( 1- alpha_bar)        \n",
    "    posterior_mean = coef_x0 * x0 + coef_xt * img\n",
    "        \n",
    "    posterior_variance = beta * (1 - alpha_bar_last) / (1. - alpha_bar)\n",
    "    posterior_log_variance = jnp.log(jnp.clip(posterior_variance, a_min = 1e-20))\n",
    "\n",
    "    return posterior_mean, posterior_log_variance\n",
    "\n",
    "\n",
    "def ddpm_sample_step(state, rng, x, t, x0_last, ddpm_param: ddpm_param):\n",
    " \n",
    "    batched_t = jnp.ones((x.shape[0],), dtype=jnp.int32) * t\n",
    "    \n",
    "    if c.ddpm.self_condition:\n",
    "        x0, v = model_predict(state, x, x0_last, batched_t, ddpm_param, use_ema=True) \n",
    "    else:\n",
    "        x0, v = model_predict(state, x, None, batched_t, ddpm_param, use_ema=True)\n",
    "    \n",
    "    x0 = jnp.clip(x0,-1.,1.) # make sure x0 between [-1,1]\n",
    "    posterior_mean, posterior_log_variance = get_posterior_mean_variance(x, t, x0, v, ddpm_param)\n",
    "    x = posterior_mean + jnp.exp(0.5 *  posterior_log_variance) * jax.random.normal(rng, x.shape) \n",
    "    return x, x0\n",
    "\n",
    "sample_step = functools.partial(\n",
    "    ddpm_sample_step, \n",
    "    ddpm_param=ddpm_param, \n",
    "    self_condition=c.ddpm.self_condition, \n",
    "    is_pred_x0=c.ddpm.is_pred_x0\n",
    ")\n",
    "\n",
    "p_sample_step = jax.pmap(sample_step, axis_name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_params_to_ema(state):\n",
    "   state = state.replace(params_ema = state.params)\n",
    "   return state\n",
    "\n",
    "def apply_ema_decay(state, ema_decay):\n",
    "    params_ema = jax.tree_map(lambda p_ema, p: p_ema * ema_decay + p * (1. - ema_decay), state.params_ema, state.params)\n",
    "    state = state.replace(params_ema = params_ema)\n",
    "    return state\n",
    "\n",
    "p_apply_ema = jax.pmap(apply_ema_decay, in_axes=(0, None), axis_name='batch')\n",
    "p_copy_params_to_ema = jax.pmap(copy_params_to_ema, axis_name='batch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "def to_wandb_config(d, parent_key: str = '', sep: str ='.'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(to_wandb_config(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            if isinstance(v, Path):\n",
    "                v = str(v)\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "wandb.init(\n",
    "    job_type=c.wandb.job_type,\n",
    "    entity=c.wandb.entity,\n",
    "    project=c.project,\n",
    "    config=to_wandb_config(c.dict),\n",
    "    settings=wandb.Settings(start_method='fork'),  # idk why this is an issue\n",
    "    dir=c.exp_path,\n",
    ")\n",
    "\n",
    "wandb.define_metric(\"*\", step_metric=\"train/step\")\n",
    "\n",
    "# # Display a project workspace\n",
    "# %wandb USERNAME/PROJECT\n",
    "# # Display a single run\n",
    "# %wandb USERNAME/PROJECT/runs/RUN_ID\n",
    "# # Display a sweep\n",
    "# %wandb USERNAME/PROJECT/sweeps/SWEEP_ID\n",
    "# # Display a report\n",
    "# %wandb USERNAME/PROJECT/reports/REPORT_ID\n",
    "# # Specify the height of embedded iframe\n",
    "# %wandb USERNAME/PROJECT -h 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_stats():\n",
    "    def normalize_to_neg_one_to_one(img):\n",
    "        return img * 2 - 1  \n",
    "    data_tr = datasets.FashionMNIST(\n",
    "        root=c.data_dir, \n",
    "        download=True, \n",
    "        train=True,\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(28),\n",
    "            transforms.CenterCrop(28),\n",
    "        ]),\n",
    "        target_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    "    ])\n",
    "    )\n",
    "    imgs = torch.stack([img_t for img_t, _ in data_tr], dim=3)\n",
    "    mean = imgs.view(1,-1).mean(dim=1) \n",
    "    std = imgs.view(1,-1).std(dim=1)\n",
    "    print('Data mean: ', mean, 'Data std: ', std)\n",
    "    return mean, std\n",
    "mean, std = get_data_stats()\n",
    "loader_tr, loader_test = get_fashion_loader(c.data.b_size, c.data_dir, mean=mean, std=std)  # is not an iterator or list\n",
    "img, l = next(iter(loader_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Image.fromarray(np.uint8((img[0, 0]*std+mean).cpu().numpy()*255))\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('1', size=(cols*w, rows*h))\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "\n",
    "    return grid\n",
    "std = float(std.cpu().numpy())\n",
    "mean = float(mean.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = []\n",
    "\n",
    "for ep in range(c.n_epoch):\n",
    "    for step, (b, target) in enumerate(loader_tr):\n",
    "        b = jnp.squeeze(jnp.expand_dims(jnp.array(b.numpy()), axis=(0,-1)), axis=2) # p, B, H, W, C\n",
    "        rng, *train_step_rng = jax.random.split(rng, num=jax.local_device_count() + 1)\n",
    "        train_step_rng = jnp.array(train_step_rng)\n",
    "        \n",
    "        state, metrics = p_train_step(train_step_rng, state, b)\n",
    "\n",
    "        if step <= c.ema.update_after_step:\n",
    "            state = p_copy_params_to_ema(state)\n",
    "\n",
    "        elif step % c.ema.update_every == 0:\n",
    "            ema_decay = ema_decay_fn(step)\n",
    "            state =  p_apply_ema(state, ema_decay)\n",
    "\n",
    "        if step % c.log_metric_step == 0:\n",
    "\n",
    "            train_metrics.append(metrics)\n",
    "            train_metrics = common_utils.get_metrics(train_metrics)\n",
    "            \n",
    "            summary = {\n",
    "                f'train/{k}': v\n",
    "                for k, v in jax.tree_map(lambda x: x.mean(), train_metrics).items()\n",
    "            }\n",
    "\n",
    "            train_metrics = []\n",
    "            b = ((np.array(b)*std+mean)*255).astype(np.uint8)\n",
    "            imgs = [wandb.Image(Image.fromarray(b[0, i].reshape(28, 28))) for i in range(9)]\n",
    "            \n",
    "            wandb.log({\n",
    "                    \"train/step\": step, \n",
    "                    'train/sample': imgs,\n",
    "                    **summary\n",
    "            })\n",
    " \n",
    "    print('Epoch: ', ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.fromarray(b[0, 0].reshape(28, 28, 1))\n",
    "b[0, 0].max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('dex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b4edb4a58a0461d84e636e9142615dc364f099c3851533546c18fbe9e367308"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
