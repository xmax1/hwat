{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 09:55:38.612011: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:497] The NVIDIA driver's CUDA version is 11.4 which is older than the ptxas CUDA version (11.6.55). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "from jax.config import config\n",
    "config.update('jax_disable_jit', True)\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "import wandb\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import random as rnd\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.training import common_utils\n",
    "from flax import linen as nn, jax_utils\n",
    "import optax\n",
    "\n",
    "from pyfig import Pyfig\n",
    "from hwat import logabssumdet, create_masks\n",
    "\n",
    "def wpr(d:dict):\n",
    "    for k,v in d.items():\n",
    "        typ = type(v) \n",
    "        has_shape = hasattr(v, 'shape')\n",
    "        shape = v.shape if has_shape else None\n",
    "        dtype = v.dtype if hasattr(v, 'dtype') else None\n",
    "        mean = jnp.mean(v) if has_shape else v\n",
    "        std = jnp.std(v) if has_shape else None\n",
    "        print(k, f'mean={mean} std={std} shape={shape} dtype={dtype} type={typ} ')\n",
    "\n",
    "# Method 1 completely refer to Pyfig:\n",
    "    # - Can't get module in the args \n",
    "# Must have shape debug print\n",
    "\n",
    "# 11am: \n",
    "# 1- Putting all variables into every Sub - done, it was a loopy mutable issue\n",
    "# 2- Stop printing mask - done, moved masks to Ferminet\n",
    "# 3- \n",
    "\n",
    "c = Pyfig(wandb_mode='disabled', debug=True) # online:on|disabled:off|offline:local, True: \n",
    "c.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "x_s_var mean=1.9469865560531616 std=0.7550688982009888 shape=(10, 1) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_p_var mean=2.524434804916382 std=1.3280136585235596 shape=(10, 10, 1) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_p_var mean=-0.6352688670158386 std=0.47677209973335266 shape=(10, 10, 8) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_s_var mean=0.09472795575857162 std=0.5215277075767517 shape=(10, 16) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_p_var mean=-0.28023380041122437 std=0.4364957809448242 shape=(10, 10, 8) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_s_var mean=0.23098309338092804 std=0.6535929441452026 shape=(10, 16) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_w mean=-0.004265791270881891 std=0.5131012201309204 shape=(10, 64) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_wu mean=-0.15545150637626648 std=0.505211353302002 shape=(5, 5) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_wd mean=-0.07841941714286804 std=0.3952345550060272 shape=(5, 5) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "orb_u mean=-0.7551324963569641 std=2.791072368621826 shape=(1, 5, 5) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "orb_d mean=-0.0950351282954216 std=0.5227019190788269 shape=(1, 5, 5) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_s_var mean=1.9469865560531616 std=0.7550688982009888 shape=(10, 1) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_p_var mean=2.524434804916382 std=1.3280136585235596 shape=(10, 10, 1) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_p_var mean=-0.6352688670158386 std=0.47677209973335266 shape=(10, 10, 8) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_s_var mean=0.09472795575857162 std=0.5215277075767517 shape=(10, 16) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_p_var mean=-0.28023380041122437 std=0.4364957809448242 shape=(10, 10, 8) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_s_var mean=0.23098309338092804 std=0.6535929441452026 shape=(10, 16) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_w mean=-0.004265791270881891 std=0.5131012201309204 shape=(10, 64) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_wu mean=-0.15545150637626648 std=0.505211353302002 shape=(5, 5) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_wd mean=-0.07841941714286804 std=0.3952345550060272 shape=(5, 5) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "orb_u mean=-0.7551324963569641 std=2.791072368621826 shape=(1, 5, 5) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "orb_d mean=-0.0950351282954216 std=0.5227019190788269 shape=(1, 5, 5) dtype=float32 type=<class 'jaxlib.xla_extension.DeviceArray'> \n",
      "x_s_var mean=[1.6703053]{b=0} std=[0.4437974]{b=0} shape=(10, 1) dtype=float32 type=<class 'jax.interpreters.pxla.MapTracer'> \n",
      "x_p_var mean=[2.1549191]{b=0} std=[1.0897869]{b=0} shape=(10, 10, 1) dtype=float32 type=<class 'jax.interpreters.pxla.MapTracer'> \n",
      "x_p_var mean=[-0.39195046]{b=0} std=[0.7536899]{b=0} shape=(10, 10, 8) dtype=float32 type=<class 'jax.interpreters.pxla.MapTracer'> \n",
      "x_s_var mean=[-0.13299786]{b=0} std=[0.6838795]{b=0} shape=(10, 16) dtype=float32 type=<class 'jax.interpreters.pxla.MapTracer'> \n",
      "x_p_var mean=[-0.10697971]{b=0} std=[0.94211894]{b=0} shape=(10, 10, 8) dtype=float32 type=<class 'jax.interpreters.pxla.MapTracer'> \n",
      "x_s_var mean=[-0.0934475]{b=0} std=[1.1110154]{b=0} shape=(10, 16) dtype=float32 type=<class 'jax.interpreters.pxla.MapTracer'> \n",
      "x_w mean=[0.11202528]{b=0} std=[0.65376216]{b=0} shape=(10, 64) dtype=float32 type=<class 'jax.interpreters.pxla.MapTracer'> \n",
      "x_wu mean=[0.08771168]{b=0} std=[0.64905995]{b=0} shape=(5, 5) dtype=float32 type=<class 'jax.interpreters.pxla.MapTracer'> \n",
      "x_wd mean=[0.24882787]{b=0} std=[0.4773823]{b=0} shape=(5, 5) dtype=float32 type=<class 'jax.interpreters.pxla.MapTracer'> \n",
      "orb_u mean=[0.30781275]{b=0} std=[1.9340042]{b=0} shape=(1, 5, 5) dtype=float32 type=<class 'jax.interpreters.pxla.MapTracer'> \n",
      "orb_d mean=[0.39438242]{b=0} std=[1.0664365]{b=0} shape=(1, 5, 5) dtype=float32 type=<class 'jax.interpreters.pxla.MapTracer'> \n"
     ]
    }
   ],
   "source": [
    "class FermiNet(nn.Module):\n",
    "    n_e: int = None\n",
    "    n_u: int = None\n",
    "    n_d: int = None\n",
    "    compute_s_emb: Callable = None\n",
    "    compute_p_emb: Callable = None\n",
    "    compute_s_perm: Callable = None\n",
    "    n_det: int = None\n",
    "    n_fb: int = None\n",
    "    n_fb_out: int = None\n",
    "    n_pv: int = None\n",
    "    n_sv: int = None\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(_i, x):\n",
    "\n",
    "        p_mask_u, p_mask_d = create_masks(_i.n_e, _i.n_u)\n",
    "        \n",
    "        xu, xd = jnp.split(x, [_i.n_u,], axis=0)\n",
    "        x_s_var = _i.compute_s_emb(x)\n",
    "        x_p_var = _i.compute_p_emb(x)\n",
    "        wpr(dict(x_s_var=x_s_var, x_p_var=x_p_var))\n",
    "\n",
    "        x_s_res = x_p_res = 0.\n",
    "        for _ in range(_i.n_fb):\n",
    "            x_p_var = x_p_res = nn.tanh(nn.Dense(_i.n_pv)(x_p_var)) + x_p_res\n",
    "            x_s_var = _i.compute_s_perm(x_s_var, x_p_var, p_mask_u, p_mask_d)\n",
    "            x_s_var = x_s_res = nn.tanh(nn.Dense(_i.n_sv)(x_s_var)) + x_s_res\n",
    "            wpr(dict(x_p_var=x_p_var, x_s_var=x_s_var))\n",
    "\n",
    "        x_w = nn.tanh(nn.Dense(_i.n_fb_out)(x_s_var))\n",
    "        x_wu, x_wd = jnp.split(x_w, [_i.n_u,], axis=0)\n",
    "        x_wu = nn.tanh(nn.Dense(_i.n_det*_i.n_u)(x_wu))\n",
    "        x_wd = nn.tanh(nn.Dense(_i.n_det*_i.n_d)(x_wd))\n",
    "        wpr(dict(x_w=x_w, x_wu=x_wu, x_wd=x_wd))\n",
    "        \n",
    "        orb_u = jnp.stack((x_wu * jnp.exp(-nn.Dense(_i.n_u*_i.n_det)(-xu))).split(_i.n_det, axis=-1)) # (e, f(e)) (e, (f(e))*n_det)\n",
    "        orb_d = jnp.stack((x_wd * jnp.exp(-nn.Dense(_i.n_d*_i.n_det)(-xd))).split(_i.n_det, axis=-1))\n",
    "        wpr(dict(orb_u=orb_u, orb_d=orb_d))\n",
    "\n",
    "        log_psi, sgn = logabssumdet(orb_u, orb_d)\n",
    "        return log_psi\n",
    "\n",
    "model = c.pass_arg(FermiNet)\n",
    "\n",
    "rng = rnd.PRNGKey(1)\n",
    "x = rnd.normal(rng, (c.data.n_e, 3))\n",
    "params = model.init(rng, x)\n",
    "model.apply(params, x)\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "@partial(jax.pmap, axis_name='b')\n",
    "def create_train_state(rng):\n",
    "  model = c.pass_arg(FermiNet)\n",
    "  x = rnd.normal(rng, (c.data.n_e, 3))\n",
    "  params = model.init(rng, x)['params']\n",
    "  tx = optax.sgd(c.opt.lr)\n",
    "  return TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "rng = rnd.split(rng, len(jax.devices()))\n",
    "state = create_train_state(rng)\n",
    "state = jax_utils.replicate(state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# jax.tree_map(lambda x: x.shape, params) # Check the parameters\n",
    "\n",
    "def create_train_state(rng, c: Pyfig):\n",
    "  model = CNN()\n",
    "  params = model.init(c.rng_init, jnp.ones([1, 28, 28, 1]))['params']\n",
    "  tx = optax.sgd(c.lr)\n",
    "  return TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "state = create_train_state(c)\n",
    "state = jax_utils.replicate(state)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, state, b):\n",
    "  \n",
    "  b_energy = compute_energy(state)\n",
    "  \n",
    "  def loss_fn(p):\n",
    "    model_out = state.apply_fn({'params':p}, b)\n",
    "    log_psi, sgn = model_out\n",
    "    return log_psi, sgn\n",
    "  \n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  (log_psi, sgn), grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  \n",
    "  variables = {\n",
    "    'E batch'           : b_energy,\n",
    "    r'$\\log\\psi$ batch' : log_psi,\n",
    "    r'sgn$(\\cdot)$'     : sgn\n",
    "  }\n",
    "  \n",
    "  metrics = compute_metric(b_energy, log_psi, sgn)\n",
    "  return state, metrics\n",
    "\n",
    "train_step = jax.pmap(train_step, axis_name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_params_to_ema(state):\n",
    "   state = state.replace(params_ema = state.params)\n",
    "   return state\n",
    "\n",
    "def apply_ema_decay(state, ema_decay):\n",
    "    params_ema = jax.tree_map(lambda p_ema, p: p_ema * ema_decay + p * (1. - ema_decay), state.params_ema, state.params)\n",
    "    state = state.replace(params_ema = params_ema)\n",
    "    return state\n",
    "\n",
    "p_apply_ema = jax.pmap(apply_ema_decay, in_axes=(0, None), axis_name='batch')\n",
    "p_copy_params_to_ema = jax.pmap(copy_params_to_ema, axis_name='batch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "def to_wandb_config(d, parent_key: str = '', sep: str ='.'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(to_wandb_config(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            if isinstance(v, Path):\n",
    "                v = str(v)\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "wandb.init(\n",
    "    job_type=c.wandb.job_type,\n",
    "    entity=c.wandb.entity,\n",
    "    project=c.project,\n",
    "    config=to_wandb_config(c.dict),\n",
    "    settings=wandb.Settings(start_method='fork'),  # idk why this is an issue\n",
    "    dir=c.exp_path,\n",
    ")\n",
    "\n",
    "wandb.define_metric(\"*\", step_metric=\"train/step\")\n",
    "\n",
    "# # Display a project workspace\n",
    "# %wandb USERNAME/PROJECT\n",
    "# # Display a single run\n",
    "# %wandb USERNAME/PROJECT/runs/RUN_ID\n",
    "# # Display a sweep\n",
    "# %wandb USERNAME/PROJECT/sweeps/SWEEP_ID\n",
    "# # Display a report\n",
    "# %wandb USERNAME/PROJECT/reports/REPORT_ID\n",
    "# # Specify the height of embedded iframe\n",
    "# %wandb USERNAME/PROJECT -h 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_stats():\n",
    "    def normalize_to_neg_one_to_one(img):\n",
    "        return img * 2 - 1  \n",
    "    data_tr = datasets.FashionMNIST(\n",
    "        root=c.data_dir, \n",
    "        download=True, \n",
    "        train=True,\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(28),\n",
    "            transforms.CenterCrop(28),\n",
    "        ]),\n",
    "        target_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    "    ])\n",
    "    )\n",
    "    imgs = torch.stack([img_t for img_t, _ in data_tr], dim=3)\n",
    "    mean = imgs.view(1,-1).mean(dim=1) \n",
    "    std = imgs.view(1,-1).std(dim=1)\n",
    "    print('Data mean: ', mean, 'Data std: ', std)\n",
    "    return mean, std\n",
    "mean, std = get_data_stats()\n",
    "loader_tr, loader_test = get_fashion_loader(c.data.b_size, c.data_dir, mean=mean, std=std)  # is not an iterator or list\n",
    "img, l = next(iter(loader_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Image.fromarray(np.uint8((img[0, 0]*std+mean).cpu().numpy()*255))\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('1', size=(cols*w, rows*h))\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "\n",
    "    return grid\n",
    "std = float(std.cpu().numpy())\n",
    "mean = float(mean.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = []\n",
    "\n",
    "for ep in range(c.n_epoch):\n",
    "    for step, (b, target) in enumerate(loader_tr):\n",
    "        b = jnp.squeeze(jnp.expand_dims(jnp.array(b.numpy()), axis=(0,-1)), axis=2) # p, B, H, W, C\n",
    "        rng, *train_step_rng = jax.random.split(rng, num=jax.local_device_count() + 1)\n",
    "        train_step_rng = jnp.array(train_step_rng)\n",
    "        \n",
    "        state, metrics = p_train_step(train_step_rng, state, b)\n",
    "\n",
    "        if step <= c.ema.update_after_step:\n",
    "            state = p_copy_params_to_ema(state)\n",
    "\n",
    "        elif step % c.ema.update_every == 0:\n",
    "            ema_decay = ema_decay_fn(step)\n",
    "            state =  p_apply_ema(state, ema_decay)\n",
    "\n",
    "        if step % c.log_metric_step == 0:\n",
    "\n",
    "            train_metrics.append(metrics)\n",
    "            train_metrics = common_utils.get_metrics(train_metrics)\n",
    "            \n",
    "            summary = {\n",
    "                f'train/{k}': v\n",
    "                for k, v in jax.tree_map(lambda x: x.mean(), train_metrics).items()\n",
    "            }\n",
    "\n",
    "            train_metrics = []\n",
    "            b = ((np.array(b)*std+mean)*255).astype(np.uint8)\n",
    "            imgs = [wandb.Image(Image.fromarray(b[0, i].reshape(28, 28))) for i in range(9)]\n",
    "            \n",
    "            wandb.log({\n",
    "                    \"train/step\": step, \n",
    "                    'train/sample': imgs,\n",
    "                    **summary\n",
    "            })\n",
    " \n",
    "    print('Epoch: ', ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.fromarray(b[0, 0].reshape(28, 28, 1))\n",
    "b[0, 0].max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('dex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b4edb4a58a0461d84e636e9142615dc364f099c3851533546c18fbe9e367308"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
