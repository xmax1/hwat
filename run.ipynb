{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path exists, leaving alone\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "def mkdir(path: Path) -> Path:\n",
    "    path = Path(path)\n",
    "    if path.suffix != '':\n",
    "        path = path.parent\n",
    "    if path.exists():\n",
    "        print('path exists, leaving alone')\n",
    "    else:\n",
    "        path.mkdir(parents=True)\n",
    "    return path\n",
    "\n",
    "this_dir = Path('').parent\n",
    "TMP = mkdir('./tmp/out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from PIL import Image\n",
    "\n",
    "from flax import jax_utils\n",
    "from flax.training import common_utils, train_state, dynamic_scale\n",
    "import optax\n",
    "\n",
    "def flatten(x):\n",
    "  return x.reshape(x.shape[0], -1)\n",
    "\n",
    "def trs(b):\n",
    "    return jnp.array(b.numpy())[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import sys\n",
    "import subprocess\n",
    "import wandb\n",
    "from time import sleep\n",
    "from functools import partial, reduce\n",
    "from itertools import product\n",
    "from simple_slurm import Slurm\n",
    "import random\n",
    "from typing import Any, Iterable\n",
    "import re\n",
    "from ast import literal_eval\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import random as rnd\n",
    "from typing import Any\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "from flax import linen as nn, jax_utils\n",
    "from jax.config import config\n",
    "config.update('jax_disable_jit', True)\n",
    "\n",
    "\n",
    "compute_rvec = lambda x0, x1: \\\n",
    "        jnp.expand_dims(x0, -2) - jnp.expand_dims(x1, -3)\n",
    "\n",
    "compute_r = lambda x0, x1, keepdims=True: \\\n",
    "        jnp.linalg.norm(compute_rvec(x0, x1), axis=-1, keepdims=keepdims)\n",
    "\n",
    "l1_norm_keep = lambda x: jnp.linalg.norm(x, axis=-1, keepdims=True)\n",
    "\n",
    "# into pyfig\n",
    "def compute_terms_in(x, *, a=None, terms=['x']):\n",
    "    z = []\n",
    "    if 'x' in terms:\n",
    "        z += [x]\n",
    "    if 'x_r' in terms:\n",
    "        z += [jnp.linalg.norm(x, axis=-1, keepdims=True)]\n",
    "    if 'xa' in terms:\n",
    "        z += [compute_rvec(x, a)]\n",
    "    if 'xa_rvec' in terms:\n",
    "        z += [compute_r(x, a)]\n",
    "    if 'xx' in terms:\n",
    "        z += [compute_r(x, x)]\n",
    "    if 'xx_rvec' in terms:\n",
    "        z += [compute_rvec(x, x)]\n",
    "    return jnp.concatenate(z, axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'af': 'tanh',\n",
      " 'commit_id': 'b78c668',\n",
      " 'data': {'b_size': 16,\n",
      "          'cache': False,\n",
      "          'channels': 1,\n",
      "          'dataset': 'fashion_mnist',\n",
      "          'image_size': 28},\n",
      " 'data_dir': PosixPath('/home/amawi/projects/data'),\n",
      " 'dtype': 'f32',\n",
      " 'entity': 'xmax1',\n",
      " 'env': 'dex',\n",
      " 'exp_id': 'N0X0xSI',\n",
      " 'exp_name': 'junk',\n",
      " 'exp_path': PosixPath('/home/amawi/projects/hwat/exp/junk/N0X0xSI'),\n",
      " 'git_branch': 'main',\n",
      " 'git_remote': 'origin',\n",
      " 'half_precision': True,\n",
      " 'iter_exp_dir': True,\n",
      " 'log_metric_step': 5,\n",
      " 'log_sample_step': 5,\n",
      " 'log_state_step': 10,\n",
      " 'model': {'dim': 64, 'dim_mults': (1, 2, 4)},\n",
      " 'n_device': 1,\n",
      " 'n_epoch': 20,\n",
      " 'n_layer': 3,\n",
      " 'n_step': 1000,\n",
      " 'opt': {'beta1': 0.9,\n",
      "         'beta2': 0.99,\n",
      "         'eps': 1e-08,\n",
      "         'loss': 'l1',\n",
      "         'lr': 0.001,\n",
      "         'optimizer': 'Adam'},\n",
      " 'project': 'hwat',\n",
      " 'project_cfg_dir': PosixPath('/home/amawi/projects/hwat/cfg'),\n",
      " 'project_exp_dir': PosixPath('/home/amawi/projects/hwat/exp'),\n",
      " 'project_path': PosixPath('/home/amawi/projects/hwat'),\n",
      " 'project_root': PosixPath('/home/amawi/projects'),\n",
      " 'run_path': PosixPath('/home/amawi/projects/hwat/run.py'),\n",
      " 'seed': 808017424,\n",
      " 'server': 'svol.fysik.dtu.dk',\n",
      " 'server_project_path': PosixPath('/home/amawi/projects/hwat'),\n",
      " 'slurm': {'cpus_per_task': 1,\n",
      "           'error': PosixPath('tmp/out/e-%j.err'),\n",
      "           'gres': 'gpu:RTX3090:1',\n",
      "           'job_name': 'junk',\n",
      "           'mail_type': 'FAIL',\n",
      "           'nodes': 1,\n",
      "           'ntasks': 8,\n",
      "           'output': PosixPath('tmp/out/o-%j.out'),\n",
      "           'partition': 'sm3090',\n",
      "           'sbatch': ' \\n'\n",
      "                     '            module purge \\n'\n",
      "                     '            source ~/.bashrc \\n'\n",
      "                     '            module load GCC \\n'\n",
      "                     '            module load CUDA/11.4.1 \\n'\n",
      "                     '            module load cuDNN/8.2.2.26-CUDA-11.4.1 \\n'\n",
      "                     '            conda activate dex \\n'\n",
      "                     '            export MKL_NUM_THREADS=1 \\n'\n",
      "                     '            export NUMEXPR_NUM_THREADS=1 \\n'\n",
      "                     '            export OMP_NUM_THREADS=1 \\n'\n",
      "                     '            export OPENBLAS_NUM_THREADS=1\\n'\n",
      "                     '            pwd\\n'\n",
      "                     '            nvidia-smi\\n'\n",
      "                     \"            mv_cmd = f'mv tmp/out/o-$SLURM_JOB_ID.out \"\n",
      "                     \"tmp/out/e-$SLURM_JOB_ID.err $out_dir' \\n\"\n",
      "                     '    ',\n",
      "           'time': '0-12:00:00'},\n",
      " 'submit': <bound method Pyfig.submit of <__main__.Pyfig object at 0x7f4d77797340>>,\n",
      " 'sweep': {'method': 'random',\n",
      "           'metrics': {'goal': 'minimize', 'name': 'validation_loss'},\n",
      "           'n_sweep': 10,\n",
      "           'name': 'sweep',\n",
      "           'parameters': {'batch_size': {'values': [16, 32, 64]},\n",
      "                          'epoch': {'values': [5, 10, 15]},\n",
      "                          'lr': {'max': 0.1, 'min': 0.0001}},\n",
      "           'run_cap': 10,\n",
      "           'sweep_id': ''},\n",
      " 'user': 'amawi',\n",
      " 'wandb': {'entity': 'xmax1', 'job_type': 'training'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_cartesian_product(*args):\n",
    "    \"\"\" Cartesian product is the ordered set of all combinations of n sets \"\"\"\n",
    "    return list(product(*args))\n",
    "\n",
    "def zip_in_n_chunks(arg: Iterable[Any], n: int) -> zip:   \n",
    "    return zip(*([iter(arg)]*n))\n",
    "\n",
    "def flat_list(lst_of_lst):\n",
    "    return [lst for sublst in lst_of_lst for lst in sublst]\n",
    "\n",
    "def gen_alphanum(n: int = 7, test=False):\n",
    "    from string import ascii_lowercase, ascii_uppercase\n",
    "    random.seed(test if test else None)\n",
    "    numbers = ''.join([str(i) for i in range(10)])\n",
    "    characters = ascii_uppercase + ascii_lowercase + numbers\n",
    "    name = ''.join([random.choice(characters) for _ in range(n)])\n",
    "    return name\n",
    "\n",
    "def add_to_Path(path: Path, string: str | Path):\n",
    "        return Path(str(path) + str(string))\n",
    "\n",
    "def iterate_folder(folder: Path, iter_exp_dir):\n",
    "    if iter_exp_dir and folder.exists():\n",
    "        for i in range(100):\n",
    "            _folder = add_to_Path(folder, f'-{i}')\n",
    "            if not re.search(_folder.name, f'-[0-9]*'):\n",
    "                folder = _folder\n",
    "                break\n",
    "        else:\n",
    "            folder = add_to_Path(folder, f'-0')\n",
    "    return folder\n",
    "    \n",
    "class Sub:\n",
    "    \n",
    "    def __init__(_i, parent=None):\n",
    "        _i.parent = parent\n",
    "        _i.__safe_init__()\n",
    "\n",
    "    def __safe_init__(_i):\n",
    "        cls_d = dict(_i.__class__.__dict__)\n",
    "        sub_cls = {k:v for k,v in cls_d.items() if isinstance(v, type)}\n",
    "        [cls_d.pop(k) for k in sub_cls.keys()]\n",
    "        for k,v in cls_d.items():\n",
    "            # print(k, type(v), callable(v))\n",
    "            if k.startswith('__') or k in Pyfig._ignore_attr:\n",
    "                continue # this was around because 'dicts are unhashable types'\n",
    "            if callable(v) or isinstance(v, property):\n",
    "                continue\n",
    "            setattr(_i, k, v)\n",
    "        [setattr(_i, k, v(parent=_i)) for k,v in sub_cls.items()]\n",
    "\n",
    "    @property\n",
    "    def dict(_i,):\n",
    "        d = cls_to_dict(_i, Pyfig._ignore_attr)\n",
    "        for k,v in d.items():\n",
    "            if issubclass(type(v), Sub):\n",
    "                d[k] = cls_to_dict(v, Pyfig._ignore_attr)\n",
    "        return d\n",
    "\n",
    "def cls_to_dict(cls, ignore:list)->dict:\n",
    "    d = {}\n",
    "    for k,v in cls.__dict__.items():\n",
    "        if k.startswith('_') or k in ignore:\n",
    "            continue\n",
    "        if callable(v):\n",
    "            continue\n",
    "        if issubclass(type(v), Sub):\n",
    "            d[k] = cls_to_dict(v, ignore)\n",
    "            continue\n",
    "        d[k] = getattr(cls,k)\n",
    "    return d\n",
    "\n",
    "def cmd_to_dict(cmd:str|list,ref:dict,_d={},delim:str=' --'):\n",
    "    \"\"\"\n",
    "    fmt: [--flag, arg, --true_flag, --flag, arg1]\n",
    "    # all flags double dash because of negative numbers duh \"\"\"\n",
    "    booleans = ['True', 'true', 't', 'False', 'false', 'f']\n",
    "    \n",
    "    cmd = ' '.join(cmd) if isinstance(cmd, list) else cmd\n",
    "    cmd = [x.lstrip().lstrip('--').rstrip() for x in cmd.split(delim)]\n",
    "    cmd = [x.split(' ', maxsplit=1) for x in cmd if ' ' in x]\n",
    "    [x.append('True') for x in cmd if len(x) == 1]\n",
    "    cmd = flat_list(cmd)\n",
    "    cmd = iter([x.strip() for x in cmd])\n",
    "\n",
    "    for k,v in zip(cmd, cmd):\n",
    "        if v in booleans: \n",
    "            v=booleans.index(v)<3  # 0-2 True 3-5 False\n",
    "        if k in ref:\n",
    "            _d[k] = type(ref[k])(v)\n",
    "        else:\n",
    "            try:\n",
    "                _d[k] = literal_eval(v)\n",
    "            except:\n",
    "                _d[k] = str(v)\n",
    "            print(f'Guessing type: {k} as {type(v)}')\n",
    "    return _d\n",
    "\n",
    "def update_cls_with_dict(cls: Any, d:dict):\n",
    "    cls_all = [v for v in cls.__dict__.values() if issubclass(type(v), Sub)]\n",
    "    cls_all.extend([cls])\n",
    "    n_remain = len(d)\n",
    "    for k,v in d.items():\n",
    "        for _cls_assign in cls_all:            \n",
    "            if not hasattr(_cls_assign, k):\n",
    "                continue\n",
    "            else:\n",
    "                if isinstance(cls.__class__.__dict__[k], property):\n",
    "                    print('Tried to assign property, consider your life choices')\n",
    "                    continue\n",
    "                v = type(cls.__dict__)(v)\n",
    "                setattr(_cls_assign, k, v)\n",
    "                n_remain -= 1\n",
    "    return n_remain\n",
    "\n",
    "def cls_to_dict(cls, ignore:list)->dict:\n",
    "    return {k: getattr(cls, k) for k in dir(cls) if not (k.startswith('_') or k in ignore)}\n",
    "\n",
    "def flat_dict(d:dict,items:list=[]):\n",
    "    for k,v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flat_dict(v, items=items).items())\n",
    "        else:\n",
    "            items.append((k, v))\n",
    "    return dict(items)  \n",
    "\n",
    "def dict_to_wandb(d:dict,parent_key:str='',sep:str ='.',items:list=[])->dict:\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, dict): \n",
    "            items.extend(dict_to_wandb(v,new_key,items=items).items())\n",
    "        else:\n",
    "            if isinstance(v, Path):  v=str(v)\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "def count_gpu() -> int: # output = run_cmd('echo $CUDA_VISIBLE_DEVICES', cwd='.')\n",
    "    return sum(c.isdigit() for c in os.environ.get('CUDA_VISIBLE_DEVICES'))\n",
    "\n",
    "def run_cmds(cmd:str|list,cwd:str|Path=None,input_req:str=None):\n",
    "    _out = []\n",
    "    for cmd_1 in (cmd if isinstance(cmd, list) else [cmd]): \n",
    "        cmd_1 = [c.strip() for c in cmd_1.split(' ')]\n",
    "        _out += [subprocess.run(\n",
    "            cmd_1,cwd=cwd,input=input_req, capture_output=True)]\n",
    "        sleep(0.1)\n",
    "    return _out\n",
    "\n",
    "def run_cmds_server(server:str,user:str,cmd:str|list,cwd=str|Path):\n",
    "    _out = []\n",
    "    client = paramiko.SSHClient()\n",
    "    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # if not known host\n",
    "    client.connect(hostname=server, username=user)\n",
    "    client.exec_command(f'cd {cwd}')\n",
    "    with client as _r:\n",
    "        for cmd_1 in (cmd if isinstance(cmd, list) else [cmd]):\n",
    "            _out += [_r.exec_command(f'{cmd_1}')] # in, out, err\n",
    "            sleep(0.1)\n",
    "    return _out\n",
    "\n",
    "class Pyfig(Sub):\n",
    "\n",
    "    seed:               int     = 808017424 # grr\n",
    "    project_root:       str     = Path().home()/'projects'\n",
    "\n",
    "    project:            str     = 'hwat'\n",
    "    project_path:       Path    = property(lambda _: _.project_root / _.project)\n",
    "    server_project_path:Path    = property(lambda _: _.project_path)\n",
    "    n_device:           int     = property(lambda _: count_gpu())\n",
    "\n",
    "    exp_name:           str     = 'junk'\n",
    "    run_path:           Path    = property(lambda _: _.project_path / 'run.py')\n",
    "    data_dir:           Path    = project_root / 'data'\n",
    "    \n",
    "    half_precision:     bool    = True\n",
    "    dtype:              str     = 'f32'\n",
    "    n_step:             int     = 1000\n",
    "\n",
    "    af:                 str     = 'tanh' # activation function\n",
    "    n_layer:            int     = 3\n",
    "    \n",
    "    class data(Sub):\n",
    "        dataset     = 'fashion_mnist'\n",
    "        b_size      = 16\n",
    "        cache       = False\n",
    "        image_size  = 28\n",
    "        channels    = 1\n",
    "\n",
    "    class model(Sub):\n",
    "        dim         = 64\n",
    "        dim_mults   = (1, 2, 4)\n",
    "\n",
    "    class opt(Sub):\n",
    "        optimizer   = 'Adam'\n",
    "        beta1       = 0.9\n",
    "        beta2       = 0.99\n",
    "        eps         = 1e-8\n",
    "        lr          = 0.001\n",
    "        loss        = 'l1'  # change this to loss table load? \n",
    "\n",
    "    class sweep(Sub):\n",
    "        method      = 'random'\n",
    "        name        = 'sweep'\n",
    "        metrics = dict(\n",
    "            goal    = 'minimize',\n",
    "            name    = 'validation_loss',\n",
    "        )\n",
    "        parameters = dict(\n",
    "            batch_size  = {'values' : [16, 32, 64]},\n",
    "            epoch       = {'values' : [5, 10, 15]},\n",
    "            lr          = {'max'    : 0.1, 'min': 0.0001},\n",
    "        )\n",
    "        n_sweep = run_cap = reduce(\n",
    "            lambda i,j:i*j,[len(v['values']) for k,v in parameters.items() if 'values' in v])+1\n",
    "        sweep_id = ''\n",
    "\n",
    "    class wandb(Sub):\n",
    "        job_type:       str     = 'training'\n",
    "        entity:         str     = 'xmax1'\n",
    "\n",
    "    log_sample_step:    int     = 5\n",
    "    log_metric_step:    int     = 5\n",
    "    log_state_step:     int     = 10         # wandb entity\n",
    "    n_epoch:            int     = 20\n",
    "\n",
    "    class slurm(Sub):\n",
    "        output          = TMP/'o-%j.out'\n",
    "        error           = TMP/'e-%j.err'\n",
    "        mail_type       = 'FAIL'\n",
    "        partition       ='sm3090'\n",
    "        nodes           = 1                # n_node\n",
    "        ntasks          = 8                # n_cpu\n",
    "        cpus_per_task   = 1     \n",
    "        time            = '0-12:00:00'     # D-HH:MM:SS\n",
    "        gres            = 'gpu:RTX3090:1'\n",
    "        job_name        = property(lambda _: _.parent.exp_name)  # this does not call the instance it is in\n",
    "        sbatch          = property(lambda _: f\"\"\" \n",
    "            module purge \n",
    "            source ~/.bashrc \n",
    "            module load GCC \n",
    "            module load CUDA/11.4.1 \n",
    "            module load cuDNN/8.2.2.26-CUDA-11.4.1 \n",
    "            conda activate {_.parent.env} \n",
    "            export MKL_NUM_THREADS=1 \n",
    "            export NUMEXPR_NUM_THREADS=1 \n",
    "            export OMP_NUM_THREADS=1 \n",
    "            export OPENBLAS_NUM_THREADS=1\n",
    "            pwd\n",
    "            nvidia-smi\n",
    "            mv_cmd = f'mv {TMP}/o-$SLURM_JOB_ID.out {TMP}/e-$SLURM_JOB_ID.err $out_dir' \n",
    "    \"\"\")\n",
    "\n",
    "    exp_id:             str     = gen_alphanum(n=7)\n",
    "    \n",
    "    iter_exp_dir:       bool    = True\n",
    "    project_exp_dir:    Path    = property(lambda _: _.project_path / 'exp')\n",
    "    project_cfg_dir:    Path    = property(lambda _: _.project_path / 'cfg')\n",
    "    exp_path:           Path    = property(lambda _: iterate_folder(_.project_exp_dir/_.exp_name,_.iter_exp_dir)/_.exp_id)\n",
    "\n",
    "    server:             str     = 'svol.fysik.dtu.dk'   # SERVER\n",
    "    user:               str     = 'amawi'     # SERVER\n",
    "    entity:             str     = 'xmax1'       # WANDB entity\n",
    "    git_remote:         str     = 'origin'      \n",
    "    git_branch:         str     = 'main'        \n",
    "    env:                str     = 'dex'            # CONDA ENV\n",
    "    commit_id:          str     = property(lambda _: _.get_commit_id())\n",
    "    \n",
    "    _sys_arg: list = sys.argv[1:]\n",
    "    _submit_state:     int     = -1\n",
    "    _ignore_attr = ['parent','protected','dict','cmd']\n",
    "\n",
    "    def __init__(_i,args:dict={},cap=40,wandb_mode='online',notebook=False):\n",
    "        super().__init__()\n",
    "        _i.__safe_init__()\n",
    "\n",
    "        update_cls_with_dict(_i,args)\n",
    "        if not notebook:\n",
    "            update_cls_with_dict(cmd_to_dict(sys.argv[1:],_i.dict))\n",
    "\n",
    "        wandb.init(\n",
    "            job_type    = _i.wandb.job_type,\n",
    "            entity      = _i.wandb.entity,\n",
    "            project     = _i.project,\n",
    "            dir         = _i.exp_path,\n",
    "            config      = dict_to_wandb(_i.dict),\n",
    "            mode        = wandb_mode,\n",
    "            settings=wandb.Settings(start_method='fork'), # idk y this is issue, don't change\n",
    "        )\n",
    "\n",
    "        if _i._submit_state > 0:\n",
    "            n_job_running = run_cmds([f'squeue -u {_i.user} -h -t pending,running -r | wc -l'])\n",
    "            if n_job_running > cap:\n",
    "                exit(f'There are {n_job_running} on the submit cap is {cap}')\n",
    "\n",
    "            _slurm = Slurm(**_i.slurm.dict)\n",
    "\n",
    "            n_run, _i._submit_state = _i._submit_state, 0            \n",
    "            for _ in range(n_run):\n",
    "                _slurm.sbatch(_i.slurm.sbatch \n",
    "                + f'out_dir={(mkdir(_i.exp_path/\"out\"))} {_i.cmd} | tee $out_dir/py.out date \"+%B %V %T.%3N\" ')\n",
    "\n",
    "    @property\n",
    "    def cmd(_i,):\n",
    "        d = flat_dict(_i.dict)\n",
    "        return ' '.join([f' --{k}  {str(v)} ' for k,v in d.items()])\n",
    "\n",
    "    @property\n",
    "    def commit_id(_i,)->str:\n",
    "        process = run_cmds(['git log --pretty=format:%h -n 1'], cwd=_i.project_path)[0]\n",
    "        return process.stdout.decode('utf-8') \n",
    "\n",
    "    def submit(_i, sweep=False, commit_msg=None, cap=40):\n",
    "        commit_msg = commit_msg or _i.exp_id\n",
    "        _i._submit_state *= -1\n",
    "        if _i._submit_state > 0:\n",
    "            if sweep:\n",
    "                _i.sweep_id = wandb.sweep(\n",
    "                    env     = f'conda activate {_i.env};',\n",
    "                    sweep   = _i.sweep.dict, \n",
    "                    program = _i.run_path,\n",
    "                    project = _i.project,\n",
    "                    name    = _i.exp_name,\n",
    "                    run_cap = _i.sweep.n_sweep\n",
    "                )\n",
    "                _i._submit_state *= _i.sweep.n_sweep\n",
    "            local_out = run_cmds(['git add .', f'git commit -m {commit_msg}', 'git push'], cwd=_i.project_path)\n",
    "            server_out = run_cmds_server(_i.server, _i.user, f'python -u {_i.run_path} ' +_i.cmd, cwd=_i.server_project_path)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "c = Pyfig(wandb_mode='disabled', notebook=True)\n",
    "pprint(c.dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(d:dict):\n",
    "    ...\n",
    "    metrics = lax.pmean(metrics, axis_name='b')\n",
    "\n",
    "    return metric\n",
    "\n",
    "\n",
    "def compute_energy():\n",
    "    \n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_s_perm(x, x_p, p_mask_u, p_mask_d, n_u):\n",
    "    n_e, _ = x_p.shape\n",
    "    n_u_ish, _ = x.shape\n",
    "    n_d = n_e - n_u_ish\n",
    "    n_u = n_e - n_d\n",
    "\n",
    "    xu, xd = jnp.split(x, [n_u, n_d], axis=0)\n",
    "    mean_xu = jnp.mean(xu, axis=0, keepdims=True)\n",
    "    mean_xd = jnp.mean(xd, axis=0, keepdims=True)\n",
    "\n",
    "    x_p = jnp.expand_dims(x_p, axis=0)\n",
    "    sum_p_u = (p_mask_u * x_p).sum((1, 2)) / float(n_u)\n",
    "    sum_p_d = (p_mask_d * x_p).sum((1, 2)) / float(n_d)\n",
    "\n",
    "    x = jnp.concatenate((x, mean_xu, mean_xd, sum_p_u, sum_p_d), axis=-1)\n",
    "    return jnp.split(x, [n_u, n_d], axis=0)\n",
    "\n",
    "def logabssumdet(orb_u, orb_d=None):\n",
    "    # Special case if there is only one electron in any channel\n",
    "    # We can avoid the log(0) issue by not going into the log domain\n",
    "    \n",
    "    xs = [orb_u, orb_d] if not orb_d is None else [orb_u]\n",
    "    \n",
    "    dets = [x.reshape(-1) for x in xs if x.shape[-1] == 1]\n",
    "    dets = reduce(lambda a,b: a*b, dets) if len(dets)>0 else 1\n",
    "\n",
    "    slogdets = [jnp.linalg.slogdet(x) for x in xs if x.shape[-1] > 1]\n",
    "    \n",
    "    if len(slogdets) > 0: # at least 2 electon in at least 1 orbital\n",
    "        sign_in, logdet = reduce(lambda a,b: (a[0]*b[0], a[1]+b[1]), slogdets)\n",
    "        maxlogdet = jnp.max(logdet)\n",
    "        det = sign_in * dets * jnp.exp(logdet-maxlogdet)\n",
    "    else:\n",
    "        maxlogdet = 0\n",
    "        det = dets\n",
    "\n",
    "    psi_ish = jnp.sum(det)\n",
    "    sgn_psi = jnp.sign(psi_ish)\n",
    "    log_psi = jnp.log(jnp.abs(psi_ish)) + maxlogdet\n",
    "    return log_psi, sgn_psi\n",
    "\n",
    "def create_masks(n_electrons, n_up):\n",
    "    ups = jnp.ones(n_electrons)\n",
    "    ups[n_up:] = 0.\n",
    "    downs = (ups-1.)*-1.\n",
    "\n",
    "    pairwise_up_mask = []\n",
    "    pairwise_down_mask = []\n",
    "    for electron in range(n_electrons):\n",
    "        mask_up = jnp.zeros((n_electrons, n_electrons))\n",
    "        mask_up[electron, :] = ups\n",
    "        pairwise_up_mask.append(mask_up)\n",
    "        # mask_up = mask_up[eye_mask].reshape(-1) # for when drop diagonal enforced\n",
    "        mask_down = jnp.zeros((n_electrons, n_electrons))\n",
    "        mask_down[electron, :] = downs\n",
    "        pairwise_down_mask.append(mask_down)\n",
    "\n",
    "    pairwise_up_mask = jnp.stack(pairwise_up_mask, axis=0)[..., None]\n",
    "    pairwise_down_mask = jnp.stack(pairwise_down_mask, axis=0)[..., None]\n",
    "    return pairwise_up_mask, pairwise_down_mask\n",
    "\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_e = 10\n",
    "x = jnp.ones((2, n_e, 3))\n",
    "xu, xd = x.split(2, axis=1)\n",
    "\n",
    "def init_walker(xu, xd):\n",
    "    rng = rnd.PRNGKey(c.seed)\n",
    "    rng_u, rng_d = rnd.split(rng, 2)\n",
    "    return jnp.concatenate([rnd.normal(rng_u, xu.shape), rnd.normal(rng_d, xd.shape)], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "from jax import pmap\n",
    "\n",
    "class SampleState():\n",
    "    \"\"\"\n",
    "    @struct.dataclass\n",
    "    struct.PyTreeNode means jax transformations *do not affect it* eg pmap\n",
    "    fn.apply << apply vs fn() << apply_fn\"\"\"\n",
    "    \n",
    "    def __init__(_i, step=0, move_std=0.02, corr_len=20):\n",
    "        _i.acc_target = 0.5\n",
    "        _i.corr_len = corr_len//2\n",
    "        _i.move_std = move_std\n",
    "        _i.step = step\n",
    "    \n",
    "    def __call__(_i, x, state:TrainState):\n",
    "        _i.rng, rng_0, rng_1, rng_move = rnd.split(_i.rng, 4)\n",
    "\n",
    "        x, acc_0 = sample(rng_0, x, state, _i.corr_len, _i.move_std)\n",
    "        move_std_1 = jnp.clip(_i.move_std + 0.001*rnd.normal(rng_move))\n",
    "        x, acc_1 = sample(rng_1, x, state, _i.corr_len, move_std_1)\n",
    "\n",
    "        mask = jnp.array((_i.acc_target-acc_0)**2 < (_i.acc_target-acc_1)**2, dtype=jnp.float32)\n",
    "        not_mask = ((mask-1.)*-1.)\n",
    "        _i.move_std = mask*_i.move_std + not_mask*move_std_1\n",
    "        return x\n",
    "\n",
    "def sample(rng, x, state:TrainState, corr_len, move_std):\n",
    "\n",
    "    def move(x, rng, move_std):\n",
    "        x = x + rnd.normal(rng, x.shape)*move_std\n",
    "        return x\n",
    "\n",
    "    to_prob = lambda log_psi: jnp.exp(log_psi)**2\n",
    "    \n",
    "    p = to_prob(state(x))\n",
    "    \n",
    "    acc = 0.0\n",
    "    for _ in range(corr_len//2):\n",
    "        rng, rng_move, rng_alpha = rnd.split(rng)\n",
    "        \n",
    "        x_1 = move(x, rng_move, move_std)\n",
    "        p_1 = to_prob(state(x_1))\n",
    "\n",
    "        p_mask = (p_1 / p) > rnd.uniform(rng_alpha, p_1.shape)\n",
    "        p = jnp.where(p_mask, p_1, p)\n",
    "        p_mask = jnp.expand_dims(p, axis=(-1, -2))\n",
    "        x = jnp.where(p_mask, x_1, x)\n",
    "\n",
    "        acc += jnp.mean(p_mask)\n",
    "\n",
    "    return x, acc\n",
    "\n",
    "class FermiNet(nn.Module):\n",
    "  \n",
    "  n_sv: int\n",
    "  n_pv: int\n",
    "  n_fb: int\n",
    "  n_fb_out: int\n",
    "  n_det: int\n",
    "\n",
    "  af_fb: Callable\n",
    "  af_fb_out: Callable\n",
    "  af_pseudo: Callable\n",
    "\n",
    "  compute_s_emb: Callable\n",
    "  compute_p_emb: Callable\n",
    "  compute_s_var: Callable\n",
    "  compute_p_var: Callable\n",
    "  compute_s_emb: Callable\n",
    "  compute_p_emb: Callable\n",
    "  \n",
    "  n_e: int\n",
    "  n_u: int\n",
    "  n_d: int = n_e-n_u\n",
    "  p_mask_u, p_mask_d = create_masks(n_e, n_u)\n",
    "  compute_s: Callable = partial(compute_s, p_mask_u=p_mask_u, p_mask_d=p_mask_d, n_u=n_u)\n",
    "\n",
    "  pbc:          bool = False\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(_i, x):\n",
    "    \n",
    "    n_e, n_u, n_d, n_det = _i.n_e, _i.n_u, _i.n_d, _i.n_det\n",
    "\n",
    "    x_s_var = _i.compute_s_emb(x)\n",
    "    x_p_var = _i.compute_p_emb(x)\n",
    "\n",
    "    x_s_res = x_p_res = 0.\n",
    "    for _ in range(_i.n_fb):\n",
    "        x_p_var = x_p_res = _i.af_fb(nn.Dense(_i.n_pv)(x_p_var)) + x_p_res\n",
    "        \n",
    "        x_s_var = _i.compute_s(x_s_var, x_p_var)\n",
    "        x_s_var = x_s_res = _i.af_fb(nn.Dense(_i.n_sv)(x_s_var)) + x_s_res\n",
    "\n",
    "    x_w = jnp.concatentate([x_s_var, x_p_var], axis=-1)\n",
    "    x_w = _i.af_fb_out(nn.Dense(_i.n_fb_out)(x_w))\n",
    "    x_w = _i.af_pseudo(nn.Dense(n_det*n_e)(x_w))\n",
    "    x_wu, x_wd = jnp.split(x_w, [n_u, n_d], axis=0)\n",
    "    \n",
    "    orb_u = jnp.split(x_wu * jnp.exp(-nn.Dense(n_u*n_det)), n_det, axis=1)\n",
    "    orb_d = jnp.split(x_wd * jnp.exp(-nn.Dense(n_d*n_det)), n_det, axis=1)\n",
    "\n",
    "    log_psi, sgn = logabssumdet(orb_u, orb_d)\n",
    "\n",
    "    return log_psi\n",
    "\n",
    "\n",
    "x = init_walker(xu, xd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# jax.tree_map(lambda x: x.shape, params) # Check the parameters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_train_state(c: Pyfig):\n",
    "  model = CNN()\n",
    "  params = model.init(c.rng_init, jnp.ones([1, 28, 28, 1]))['params']\n",
    "  tx = optax.sgd(c.lr)\n",
    "  return TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "state = create_train_state(c)\n",
    "state = jax_utils.replicate(state)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, state, b):\n",
    "  \n",
    "  b_energy = compute_energy(state)\n",
    "  \n",
    "  def loss_fn(p):\n",
    "    model_out = state.apply_fn({'params':p}, b)\n",
    "    log_psi, sgn = model_out\n",
    "    return log_psi, sgn\n",
    "  \n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  (log_psi, sgn), grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  \n",
    "  variables = {\n",
    "    'E batch'           : b_energy,\n",
    "    r'$\\log\\psi$ batch' : log_psi,\n",
    "    r'sgn$(\\cdot)$'     : sgn\n",
    "  }\n",
    "  \n",
    "  metrics = compute_metric(b_energy, log_psi, sgn)\n",
    "  return state, metrics\n",
    "\n",
    "train_step = jax.pmap(train_step, axis_name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior_mean_variance(img, t, x0, v, ddpm):\n",
    "\n",
    "    beta = ddpm_param.betas[t,None,None,None] # only needed when t > 0\n",
    "    alpha = ddpm_param.alphas[t,None,None,None]\n",
    "    alpha_bar = ddpm_param.alphas_bar[t,None,None,None]\n",
    "    alpha_bar_last = ddpm_param.alphas_bar[t-1,None,None,None]\n",
    "    sqrt_alpha_bar_last = ddpm_param.sqrt_alphas_bar[t-1,None,None,None]\n",
    "\n",
    "    coef_x0 = beta * sqrt_alpha_bar_last / (1. - alpha_bar)\n",
    "    coef_xt = (1. - alpha_bar_last) * jnp.sqrt(alpha) / ( 1- alpha_bar)        \n",
    "    posterior_mean = coef_x0 * x0 + coef_xt * img\n",
    "        \n",
    "    posterior_variance = beta * (1 - alpha_bar_last) / (1. - alpha_bar)\n",
    "    posterior_log_variance = jnp.log(jnp.clip(posterior_variance, a_min = 1e-20))\n",
    "\n",
    "    return posterior_mean, posterior_log_variance\n",
    "\n",
    "\n",
    "def ddpm_sample_step(state, rng, x, t, x0_last, ddpm_param: ddpm_param):\n",
    " \n",
    "    batched_t = jnp.ones((x.shape[0],), dtype=jnp.int32) * t\n",
    "    \n",
    "    if c.ddpm.self_condition:\n",
    "        x0, v = model_predict(state, x, x0_last, batched_t, ddpm_param, use_ema=True) \n",
    "    else:\n",
    "        x0, v = model_predict(state, x, None, batched_t, ddpm_param, use_ema=True)\n",
    "    \n",
    "    x0 = jnp.clip(x0,-1.,1.) # make sure x0 between [-1,1]\n",
    "    posterior_mean, posterior_log_variance = get_posterior_mean_variance(x, t, x0, v, ddpm_param)\n",
    "    x = posterior_mean + jnp.exp(0.5 *  posterior_log_variance) * jax.random.normal(rng, x.shape) \n",
    "    return x, x0\n",
    "\n",
    "sample_step = functools.partial(\n",
    "    ddpm_sample_step, \n",
    "    ddpm_param=ddpm_param, \n",
    "    self_condition=c.ddpm.self_condition, \n",
    "    is_pred_x0=c.ddpm.is_pred_x0\n",
    ")\n",
    "\n",
    "p_sample_step = jax.pmap(sample_step, axis_name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_params_to_ema(state):\n",
    "   state = state.replace(params_ema = state.params)\n",
    "   return state\n",
    "\n",
    "def apply_ema_decay(state, ema_decay):\n",
    "    params_ema = jax.tree_map(lambda p_ema, p: p_ema * ema_decay + p * (1. - ema_decay), state.params_ema, state.params)\n",
    "    state = state.replace(params_ema = params_ema)\n",
    "    return state\n",
    "\n",
    "p_apply_ema = jax.pmap(apply_ema_decay, in_axes=(0, None), axis_name='batch')\n",
    "p_copy_params_to_ema = jax.pmap(copy_params_to_ema, axis_name='batch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "def to_wandb_config(d, parent_key: str = '', sep: str ='.'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(to_wandb_config(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            if isinstance(v, Path):\n",
    "                v = str(v)\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "wandb.init(\n",
    "    job_type=c.wandb.job_type,\n",
    "    entity=c.wandb.entity,\n",
    "    project=c.project,\n",
    "    config=to_wandb_config(c.dict),\n",
    "    settings=wandb.Settings(start_method='fork'),  # idk why this is an issue\n",
    "    dir=c.exp_path,\n",
    ")\n",
    "\n",
    "wandb.define_metric(\"*\", step_metric=\"train/step\")\n",
    "\n",
    "# # Display a project workspace\n",
    "# %wandb USERNAME/PROJECT\n",
    "# # Display a single run\n",
    "# %wandb USERNAME/PROJECT/runs/RUN_ID\n",
    "# # Display a sweep\n",
    "# %wandb USERNAME/PROJECT/sweeps/SWEEP_ID\n",
    "# # Display a report\n",
    "# %wandb USERNAME/PROJECT/reports/REPORT_ID\n",
    "# # Specify the height of embedded iframe\n",
    "# %wandb USERNAME/PROJECT -h 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_stats():\n",
    "    def normalize_to_neg_one_to_one(img):\n",
    "        return img * 2 - 1  \n",
    "    data_tr = datasets.FashionMNIST(\n",
    "        root=c.data_dir, \n",
    "        download=True, \n",
    "        train=True,\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(28),\n",
    "            transforms.CenterCrop(28),\n",
    "        ]),\n",
    "        target_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    "    ])\n",
    "    )\n",
    "    imgs = torch.stack([img_t for img_t, _ in data_tr], dim=3)\n",
    "    mean = imgs.view(1,-1).mean(dim=1) \n",
    "    std = imgs.view(1,-1).std(dim=1)\n",
    "    print('Data mean: ', mean, 'Data std: ', std)\n",
    "    return mean, std\n",
    "mean, std = get_data_stats()\n",
    "loader_tr, loader_test = get_fashion_loader(c.data.b_size, c.data_dir, mean=mean, std=std)  # is not an iterator or list\n",
    "img, l = next(iter(loader_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Image.fromarray(np.uint8((img[0, 0]*std+mean).cpu().numpy()*255))\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('1', size=(cols*w, rows*h))\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "\n",
    "    return grid\n",
    "std = float(std.cpu().numpy())\n",
    "mean = float(mean.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = []\n",
    "\n",
    "for ep in range(c.n_epoch):\n",
    "    for step, (b, target) in enumerate(loader_tr):\n",
    "        b = jnp.squeeze(jnp.expand_dims(jnp.array(b.numpy()), axis=(0,-1)), axis=2) # p, B, H, W, C\n",
    "        rng, *train_step_rng = jax.random.split(rng, num=jax.local_device_count() + 1)\n",
    "        train_step_rng = jnp.array(train_step_rng)\n",
    "        \n",
    "        state, metrics = p_train_step(train_step_rng, state, b)\n",
    "\n",
    "        if step <= c.ema.update_after_step:\n",
    "            state = p_copy_params_to_ema(state)\n",
    "\n",
    "        elif step % c.ema.update_every == 0:\n",
    "            ema_decay = ema_decay_fn(step)\n",
    "            state =  p_apply_ema(state, ema_decay)\n",
    "\n",
    "        if step % c.log_metric_step == 0:\n",
    "\n",
    "            train_metrics.append(metrics)\n",
    "            train_metrics = common_utils.get_metrics(train_metrics)\n",
    "            \n",
    "            summary = {\n",
    "                f'train/{k}': v\n",
    "                for k, v in jax.tree_map(lambda x: x.mean(), train_metrics).items()\n",
    "            }\n",
    "\n",
    "            train_metrics = []\n",
    "            b = ((np.array(b)*std+mean)*255).astype(np.uint8)\n",
    "            imgs = [wandb.Image(Image.fromarray(b[0, i].reshape(28, 28))) for i in range(9)]\n",
    "            \n",
    "            wandb.log({\n",
    "                    \"train/step\": step, \n",
    "                    'train/sample': imgs,\n",
    "                    **summary\n",
    "            })\n",
    " \n",
    "    print('Epoch: ', ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.fromarray(b[0, 0].reshape(28, 28, 1))\n",
    "b[0, 0].max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('dex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b4edb4a58a0461d84e636e9142615dc364f099c3851533546c18fbe9e367308"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
