{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init sub classes\n",
      "updating configuration\n",
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: 193e83a stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n",
      "running script\n",
      "setting exp_path\n",
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: 193e83a stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxmax1\u001b[0m (\u001b[33mhwat\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>dump/exp-10/Z3dsJdN/wandb/run-20221221_192119-1dnxy161</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hwat/hwat/runs/1dnxy161\" target=\"_blank\">eternal-eon-147</a></strong> to <a href=\"https://wandb.ai/hwat/hwat\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ 1 GPUs available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' copy lines and run in analysis while the exp is live '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Distribution ‚ú® ‚ùá Demo üí™ ### \n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "### fancy logging variables, philosophically reminding us of the goal ###\n",
    "fancy = dict(\n",
    "\t\tpe\t\t= r'$V(X)',    \t\t\t\t\n",
    "\t\tke\t\t= r'$\\nabla^2',    \t\t\n",
    "\t\te\t\t= r'$E',\t\t\t\t\t\t\n",
    "\t\tlog_psi\t= r'$\\log\\psi', \t\t\t\n",
    "\t\tdeltar\t= r'$\\delta_\\mathrm{r}',\t\n",
    "\t\tx\t\t= r'$r_\\mathrm{e}',\n",
    ")\n",
    "\n",
    "### pyfig ###\n",
    "from pyfig import Pyfig\n",
    "\n",
    "# arg = {\n",
    "# \t'a_z':[4,], \n",
    "# \t'n_b': 256, \n",
    "# \t'n_sv': 16, \n",
    "# \t'n_pv': 16, \n",
    "# \t'n_corr': 20, \n",
    "# \t'n_step': 100000, \n",
    "# \t'log_metric_step': 1, \n",
    "# \t'exp_name':'demo',\n",
    "# }\n",
    "\n",
    "c = Pyfig(wb_mode='online', submit=False, run_sweep=False, notebook=True)\n",
    "\n",
    "n_device = c.n_device\n",
    "print(f'ü§ñ {n_device} GPUs available')\n",
    "\n",
    "\n",
    "# from pprint import pprint\n",
    "# pprint(c.d)\n",
    "\n",
    "\"\"\" live plotting in another notebook \"\"\"\n",
    "\"\"\" copy lines and run in analysis while the exp is live \"\"\"\n",
    "# api = wandb.Api()\n",
    "# run = api.run(\"<run-here>\")\n",
    "# c = run.config\n",
    "# h = run.history()\n",
    "# s = run.summary\n",
    "\n",
    "\n",
    "# SOLVE THE CONUNDRUM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:  True n_dev:  1 device <torch.cuda.device object at 0x7f58fc417ee0> name:  NVIDIA TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1234)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)   # ‚ùó Ensure works when default not set AND can go float32 or 64\n",
    "cuda = torch.cuda.is_available()\n",
    "n_device = torch.cuda.device_count()\n",
    "current_device = torch.cuda.current_device()\n",
    "device = torch.cuda.device(0)\n",
    "device_name = torch.cuda.get_device_name(0)\n",
    "print('cuda: ', cuda, 'n_dev: ', n_device, 'device', device, 'name: ', device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r:  torch.Size([1, 256, 4, 3]) torch.float64\n",
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: 193e83a stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n",
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: 193e83a stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n",
      "anti-symmetry:  -14.735963386332884 -14.735963386332884 -1.0 1.0\n",
      "anti-symmetry:  -14.735963386332884 -14.735963386332884 -1.0 1.0\n",
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: 193e83a stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n",
      "{'TMP': PosixPath('dump/tmp'),\n",
      " 'cap': 3,\n",
      " 'commit_id': '193e83a',\n",
      " 'data': {'a': tensor([[0., 0., 0.]], device='cuda:0'),\n",
      "          'a_z': tensor([4.], device='cuda:0'),\n",
      "          'acc_target': 0.5,\n",
      "          'charge': 0,\n",
      "          'n_b': 256,\n",
      "          'n_corr': 20,\n",
      "          'n_d': 2,\n",
      "          'n_e': 4,\n",
      "          'n_equil': 10000,\n",
      "          'n_u': 2,\n",
      "          'spin': 0},\n",
      " 'debug': False,\n",
      " 'dtype': 'float32',\n",
      " 'dump': PosixPath('dump'),\n",
      " 'env': 'lumi',\n",
      " 'exp_name': '',\n",
      " 'exp_path': PosixPath('dump/exp-10/Z3dsJdN'),\n",
      " 'git_branch': 'main',\n",
      " 'git_remote': 'origin',\n",
      " 'hostname': 'oceanus ',\n",
      " 'log_metric_step': 10,\n",
      " 'log_state_step': 10,\n",
      " 'model': {'n_det': 1,\n",
      "           'n_fb': 3,\n",
      "           'n_fbv': 128,\n",
      "           'n_pv': 16,\n",
      "           'n_sv': 32,\n",
      "           'terms_p_emb': ['rr', 'rr_len'],\n",
      "           'terms_s_emb': ['ra', 'ra_len'],\n",
      "           'with_sign': False},\n",
      " 'n_device': 1,\n",
      " 'n_step': 10000,\n",
      " 'project': 'hwat',\n",
      " 'project_dir': PosixPath('/home/amawi/projects/hwat'),\n",
      " 'run_dir': PosixPath('projects/hwat'),\n",
      " 'run_name': 'run.py',\n",
      " 'run_sweep': False,\n",
      " 'sbatch': 'module purge\\n'\n",
      "           'source ~/.bashrc\\n'\n",
      "           'module load GCC\\n'\n",
      "           'module load CUDA/11.4.1\\n'\n",
      "           'module load cuDNN/8.2.2.26-CUDA-11.4.1\\n'\n",
      "           'conda activate lumi',\n",
      " 'seed': 808017424,\n",
      " 'server': 'svol.fysik.dtu.dk',\n",
      " 'server_project_dir': PosixPath('projects/hwat'),\n",
      " 'slurm': {'cpus_per_task': 1,\n",
      "           'error': PosixPath('dump/exp-10/Z3dsJdN/e-%j.err'),\n",
      "           'gres': 'gpu:RTX3090:1',\n",
      "           'job_name': '',\n",
      "           'mail_type': 'FAIL',\n",
      "           'nodes': 1,\n",
      "           'ntasks': 8,\n",
      "           'output': PosixPath('dump/exp-10/Z3dsJdN/o-%j.out'),\n",
      "           'partition': 'sm3090',\n",
      "           'time': '0-12:00:00'},\n",
      " 'submit': False,\n",
      " 'sweep': {'method': 'grid',\n",
      "           'name': 'demo',\n",
      "           'parameters': {'n_b': {'values': [16, 32, 64]}}},\n",
      " 'sweep_id': '',\n",
      " 'sweep_path_id': '',\n",
      " 'user': 'amawi',\n",
      " 'wandb_c': {'entity': 'hwat',\n",
      "             'job_type': 'training',\n",
      "             'name': '',\n",
      "             'program': PosixPath('projects/hwat/run.py')},\n",
      " 'wb_mode': 'online'}\n"
     ]
    }
   ],
   "source": [
    "### model ###\n",
    "from functools import partial\n",
    "from hwat_func import Ansatz_fb\n",
    "from torch import nn\n",
    "\n",
    "import pprint\n",
    "\n",
    "from hwat_func import init_r, get_center_points\n",
    "x = torch.randn(1)\n",
    "c._convert(device, dtype=x.dtype)\n",
    "n_e = c.data.n_e\n",
    "center_points = get_center_points(c.data.n_e, c.data.a)\n",
    "r = init_r(n_device, c.data.n_b, c.data.n_e, center_points, std=0.1).to(device)\n",
    "dtype = r.dtype\n",
    "\n",
    "print('r: ', r.shape, r.dtype)\n",
    "r = r[0] # single batch, single gpu, ‚ùó how to multi gpu\n",
    "model = c.partial(Ansatz_fb).to(device)\n",
    "\n",
    "model_check = c.partial(Ansatz_fb, with_sign=True).to(device)\n",
    "r_swap = r[0, [1,0]+[i for i in range(2, c.data.n_e)]]\n",
    "lp0, s0 = model_check(r[0])\n",
    "lp1, s1 = model_check(r_swap)\n",
    "print('anti-symmetry: ', lp0.item(), lp0.item(), s0.item(), s1.item())\n",
    "r_swap = r[0, [i for i in range(0, c.data.n_e-2)]+[n_e-1,n_e-2]]\n",
    "lp1, s1 = model_check(r_swap)\n",
    "print('anti-symmetry: ', lp0.item(), lp0.item(), s0.item(), s1.item())\n",
    "\n",
    "pprint.pprint(c.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 4, 3]) torch.Size([1, 3]) tensor([[0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([256, 4, 3]) jvp\n",
      "torch.Size([4, 3]) torch.Size([1, 3])\n",
      "torch.Size([4, 3]) torch.Size([1, 3])\n",
      "{'e': torch.Size([256]),\n",
      " 'grads': 20,\n",
      " 'ke': torch.Size([256]),\n",
      " 'params': 20,\n",
      " 'pe': torch.Size([256]),\n",
      " 'r': torch.Size([256, 4, 3])}\n",
      "exp/actual | \n",
      "\tcps    : (4, 3)/torch.Size([4, 3])\n",
      "\tr      : (1, 256, 4, 3)/torch.Size([256, 4, 3])\n",
      "\tdeltar : (1,)/torch.Size([1])\n",
      "\n",
      "Go see: https://wandb.ai/hwat/hwat/runs/1dnxy161\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "vmap(FunctionalModule(\n  (stateless_model): Ansatz_fb(\n    (Vs): ModuleList(\n      (0): Linear(in_features=20, out_features=32, bias=True)\n      (1): Linear(in_features=128, out_features=32, bias=True)\n      (2): Linear(in_features=128, out_features=32, bias=True)\n    )\n    (Ws): ModuleList(\n      (0): Linear(in_features=4, out_features=16, bias=True)\n      (1): Linear(in_features=16, out_features=16, bias=True)\n      (2): Linear(in_features=16, out_features=16, bias=True)\n    )\n    (V_half_u): Linear(in_features=32, out_features=16, bias=True)\n    (V_half_d): Linear(in_features=32, out_features=16, bias=True)\n    (wu): Linear(in_features=16, out_features=2, bias=True)\n    (wd): Linear(in_features=16, out_features=2, bias=True)\n  )\n), in_dims=(None, 0), ...)(<inputs>): in_dims is not compatible with the structure of `inputs`. in_dims has structure TreeSpec(tuple, None, [*, *]) but inputs has structure TreeSpec(tuple, None, [*]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 84\u001b[0m\n\u001b[1;32m     81\u001b[0m model_gv \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m _params: model_v(_params, r)\n\u001b[1;32m     82\u001b[0m model_rv \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m _r: model_v(params, _r)\n\u001b[0;32m---> 84\u001b[0m r, acc, deltar \u001b[39m=\u001b[39m sample_b(model_v, r, deltar, n_corr\u001b[39m=\u001b[39;49mc\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mn_corr)  \u001b[39m# ‚ùóneeds testing  ‚úÖ\u001b[39;00m\n\u001b[1;32m     85\u001b[0m r \u001b[39m=\u001b[39m keep_around_points(r, center_points, l\u001b[39m=\u001b[39m\u001b[39m2.\u001b[39m) \u001b[39mif\u001b[39;00m step \u001b[39m<\u001b[39m \u001b[39m1000\u001b[39m \u001b[39melse\u001b[39;00m r\n\u001b[1;32m     86\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/projects/hwat/hwat_func.py:239\u001b[0m, in \u001b[0;36msample_b\u001b[0;34m(model, r_0, deltar_0, n_corr)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39mfor\u001b[39;00m deltar \u001b[39min\u001b[39;00m [deltar_0, deltar_1]:\n\u001b[1;32m    237\u001b[0m \t\u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m torch\u001b[39m.\u001b[39marange(n_corr):\n\u001b[0;32m--> 239\u001b[0m \t\tp_0 \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mexp(model(r_0))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)  \t\t\t\u001b[39m# ‚ùócan make more efficient with where modelment at end\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \t\t\u001b[39m# print(deltar.shape, r_0.shape)\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \t\tr_1 \u001b[39m=\u001b[39m r_0 \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mrandn_like(r_0, device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mdtype)\u001b[39m*\u001b[39mdeltar\n",
      "File \u001b[0;32m~/.conda/envs/lumi/lib/python3.9/site-packages/functorch/_src/vmap.py:361\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    360\u001b[0m     _check_out_dims_is_int_or_int_pytree(out_dims, func)\n\u001b[0;32m--> 361\u001b[0m     batch_size, flat_in_dims, flat_args, args_spec \u001b[39m=\u001b[39m _process_batched_inputs(in_dims, args, func)\n\u001b[1;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m _flat_vmap(\n\u001b[1;32m    363\u001b[0m         func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    364\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/lumi/lib/python3.9/site-packages/functorch/_src/vmap.py:88\u001b[0m, in \u001b[0;36m_process_batched_inputs\u001b[0;34m(in_dims, args, func)\u001b[0m\n\u001b[1;32m     86\u001b[0m flat_in_dims \u001b[39m=\u001b[39m _broadcast_to_and_flatten(in_dims, args_spec)\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m flat_in_dims \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvmap(\u001b[39m\u001b[39m{\u001b[39;00m_get_name(func)\u001b[39m}\u001b[39;00m\u001b[39m, in_dims=\u001b[39m\u001b[39m{\u001b[39;00min_dims\u001b[39m}\u001b[39;00m\u001b[39m, ...)(<inputs>): \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     90\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39min_dims is not compatible with the structure of `inputs`. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     91\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39min_dims has structure \u001b[39m\u001b[39m{\u001b[39;00mtree_flatten(in_dims)[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m but inputs \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhas structure \u001b[39m\u001b[39m{\u001b[39;00margs_spec\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[39mfor\u001b[39;00m i, (arg, in_dim) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(flat_args, flat_in_dims)):\n\u001b[1;32m     95\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(in_dim, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m in_dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: vmap(FunctionalModule(\n  (stateless_model): Ansatz_fb(\n    (Vs): ModuleList(\n      (0): Linear(in_features=20, out_features=32, bias=True)\n      (1): Linear(in_features=128, out_features=32, bias=True)\n      (2): Linear(in_features=128, out_features=32, bias=True)\n    )\n    (Ws): ModuleList(\n      (0): Linear(in_features=4, out_features=16, bias=True)\n      (1): Linear(in_features=16, out_features=16, bias=True)\n      (2): Linear(in_features=16, out_features=16, bias=True)\n    )\n    (V_half_u): Linear(in_features=32, out_features=16, bias=True)\n    (V_half_d): Linear(in_features=32, out_features=16, bias=True)\n    (wu): Linear(in_features=16, out_features=2, bias=True)\n    (wd): Linear(in_features=16, out_features=2, bias=True)\n  )\n), in_dims=(None, 0), ...)(<inputs>): in_dims is not compatible with the structure of `inputs`. in_dims has structure TreeSpec(tuple, None, [*, *]) but inputs has structure TreeSpec(tuple, None, [*])."
     ]
    }
   ],
   "source": [
    "### train step ###\n",
    "from hwat_func import compute_ke_b, compute_pe_b\n",
    "from functorch import vmap, make_functional, grad\n",
    "\n",
    "def train_step(model, r):\n",
    "\n",
    "\tprint(r.shape, model.a.shape, model.a)\n",
    "\tmodel_fn, params = make_functional(model)\n",
    "\tmodel_v = vmap(model_fn, in_dims=(None, 0))\n",
    "\n",
    "\tmodel_gv = lambda _params: model_v(_params, r)\n",
    "\tmodel_rv = lambda _r: model_v(params, _r)\n",
    " \n",
    "\t# with torch.no_grad():\n",
    "\tke = compute_ke_b(model_rv, r)\n",
    "\tpe = compute_pe_b(r, c.data.a, c.data.a_z)\n",
    "\t# print(pe.shape, ke.shape)\n",
    "\te = -0.5*ke + pe \n",
    "\te_mean_dist = torch.mean(torch.abs(torch.median(e) - e))\n",
    "\t# print(e.shape, e_mean_dist.shape)\n",
    "\te_clip = torch.clip(e, min=e-5*e_mean_dist, max=e+5*e_mean_dist)\n",
    "\n",
    "\tloss = lambda _params: ((e_clip - e_clip.mean())*model_gv(_params)).mean()\n",
    "\tgrad_fn = grad(loss)\t\n",
    "\tgrads = grad_fn(params)\n",
    "\n",
    "\tfor p, g in zip(model.parameters(), grads):    # ‚ùó do we want functional or other design?! \n",
    "\t\tp.grad = g.clone()\n",
    "  \n",
    "\t# loss.backward()\n",
    "\tgrads = [p.detach().cpu().numpy() for p in model.parameters()]\n",
    "\tparams = [p.detach().cpu().numpy() for p in model.parameters()]  \n",
    "\n",
    "\tv_tr = dict(\n",
    "\t\tparams=params, \n",
    "  \t\tgrads=grads,\n",
    "\t\te=e, pe=pe, ke=ke,\n",
    "\t\tr=r\n",
    "\t)\n",
    "\treturn v_tr\n",
    "\n",
    "r = r.to(device)\n",
    "center_points = center_points.to(device)\n",
    "\n",
    "v_tr = train_step(model, r)\n",
    "pprint.pprint({k:v.shape if isinstance(v, torch.Tensor) else len(v) for k,v in v_tr.items()})\n",
    "\n",
    "### init variables ###\n",
    "deltar = torch.tensor([0.02]).to(device)\n",
    "\n",
    "print(f\"\"\"exp/actual | \n",
    "\tcps    : {(c.data.n_e,3)}/{center_points.shape}\n",
    "\tr      : {(c.n_device, c.data.n_b, c.data.n_e, 3)}/{r.shape}\n",
    "\tdeltar : {(1,)}/{deltar.shape}\n",
    "\"\"\")\n",
    "\n",
    "### init functions ### \n",
    "from hwat_func import sample_b\n",
    "\n",
    "### train ###\n",
    "import wandb\n",
    "from hwat_func import keep_around_points\n",
    "from utils import compute_metrix\n",
    "from time import time\n",
    "t0 = time()\n",
    "\n",
    "### add in optimiser\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "c.log_metric_step = 1\n",
    "### fix sampler\n",
    "### fix train step \n",
    "### metrix conversion\n",
    "# print(c.wandb_c.)\n",
    "wandb.define_metric(\"*\", step_metric=\"tr/step\")\n",
    "print('Go see:', c._run.url)\n",
    "for step in range(1, c.n_step+1):\n",
    "\t\n",
    "\tmodel_fn, params = make_functional(model)\n",
    "\tmodel_v = vmap(model_fn, in_dims=(None, 0))\n",
    "\tmodel_rv = lambda _r: model_v(params, _r)\n",
    " \n",
    "\tr, acc, deltar = sample_b(model_rv, r, deltar, n_corr=c.data.n_corr)  # ‚ùóneeds testing  ‚úÖ\n",
    "\tr = keep_around_points(r, center_points, l=2.) if step < 1000 else r\n",
    "\topt.zero_grad()\n",
    "\tv_tr = train_step(model, r)\n",
    "\topt.step()\n",
    "\t\n",
    "\tif not (step % c.log_metric_step):\n",
    "\t\tmetrix = compute_metrix(v_tr)  # ‚ùó needs converting to torch, ie tree maps ‚úÖ\n",
    "\t\twandb.log({'tr/step':step, **metrix})\n",
    "\t\n",
    "\tdiff = time()-t0\n",
    "\tif diff > 100: \n",
    "\t\tt0 = time()\n",
    "\t\tpprint.pprint({k:f'{v.mean().item():.4f}' for k,v in v_tr.items() if isinstance(v, torch.Tensor)} | {'step': step})\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ```{toggle} env vars and jax debug config notes\n",
    "# ‚ùáÔ∏è Magic & debug not currently used\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %env CUDA_VISIBLE_DEVICES='3'\n",
    "# %env \"WANDB_NOTEBOOK_NAME\" \"run.ipynb\" # ‚ùïsame as notebook\n",
    "\n",
    "# from jax.config import config\n",
    "# config.update('jax_disable_jit', True)\n",
    "# ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b38d4d910256a52431c1e658e4ec4972e8b118bd5f76052a504536bbce1f208"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
