{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init sub classes\n",
      "updating configuration\n",
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: 1e03f0c stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n",
      "running script\n",
      "setting exp_path\n",
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: 1e03f0c stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxmax1\u001b[0m (\u001b[33mhwat\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>dump/exp-7/dO2w2nd/wandb/run-20221221_182420-10e2x2zg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hwat/hwat/runs/10e2x2zg\" target=\"_blank\">snowy-violet-144</a></strong> to <a href=\"https://wandb.ai/hwat/hwat\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ 1 GPUs available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' copy lines and run in analysis while the exp is live '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Distribution ‚ú® ‚ùá Demo üí™ ### \n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "### fancy logging variables, philosophically reminding us of the goal ###\n",
    "fancy = dict(\n",
    "\t\tpe\t\t= r'$V(X)',    \t\t\t\t\n",
    "\t\tke\t\t= r'$\\nabla^2',    \t\t\n",
    "\t\te\t\t= r'$E',\t\t\t\t\t\t\n",
    "\t\tlog_psi\t= r'$\\log\\psi', \t\t\t\n",
    "\t\tdeltar\t= r'$\\delta_\\mathrm{r}',\t\n",
    "\t\tx\t\t= r'$r_\\mathrm{e}',\n",
    ")\n",
    "\n",
    "### pyfig ###\n",
    "from pyfig import Pyfig\n",
    "\n",
    "# arg = {\n",
    "# \t'a_z':[4,], \n",
    "# \t'n_b': 256, \n",
    "# \t'n_sv': 16, \n",
    "# \t'n_pv': 16, \n",
    "# \t'n_corr': 20, \n",
    "# \t'n_step': 100000, \n",
    "# \t'log_metric_step': 1, \n",
    "# \t'exp_name':'demo',\n",
    "# }\n",
    "\n",
    "\n",
    "c = Pyfig(wb_mode='online', submit=False, run_sweep=False, notebook=True)\n",
    "\n",
    "n_device = c.n_device\n",
    "print(f'ü§ñ {n_device} GPUs available')\n",
    "\n",
    "\n",
    "# from pprint import pprint\n",
    "# pprint(c.d)\n",
    "\n",
    "\"\"\" live plotting in another notebook \"\"\"\n",
    "\"\"\" copy lines and run in analysis while the exp is live \"\"\"\n",
    "# api = wandb.Api()\n",
    "# run = api.run(\"<run-here>\")\n",
    "# c = run.config\n",
    "# h = run.history()\n",
    "# s = run.summary\n",
    "\n",
    "\n",
    "# SOLVE THE CONUNDRUM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:  True n_dev:  1 device <torch.cuda.device object at 0x7f6974e8f5b0> name:  NVIDIA TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1234)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)   # ‚ùó Ensure works when default not set AND can go float32 or 64\n",
    "cuda = torch.cuda.is_available()\n",
    "n_device = torch.cuda.device_count()\n",
    "current_device = torch.cuda.current_device()\n",
    "device = torch.cuda.device(0)\n",
    "device_name = torch.cuda.get_device_name(0)\n",
    "print('cuda: ', cuda, 'n_dev: ', n_device, 'device', device, 'name: ', device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r:  torch.Size([1, 256, 4, 3]) torch.float64\n",
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: 1e03f0c stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mr: \u001b[39m\u001b[39m'\u001b[39m, r\u001b[39m.\u001b[39mshape, r\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m     17\u001b[0m r \u001b[39m=\u001b[39m r[\u001b[39m0\u001b[39m] \u001b[39m# single batch, single gpu, ‚ùó how to multi gpu\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m model \u001b[39m=\u001b[39m c\u001b[39m.\u001b[39;49mpartial(Ansatz_fb)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m model_check \u001b[39m=\u001b[39m c\u001b[39m.\u001b[39mpartial(Ansatz_fb, with_sign\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m r_swap \u001b[39m=\u001b[39m r[\u001b[39m0\u001b[39m, [\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m]\u001b[39m+\u001b[39m[i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m, c\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mn_e)]]\n",
      "File \u001b[0;32m~/projects/hwat/pyfig.py:243\u001b[0m, in \u001b[0;36mPyfig.partial\u001b[0;34m(ii, f, d, get_d, print_d, **kw)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mif\u001b[39;00m print_d:\n\u001b[1;32m    242\u001b[0m     pprint\u001b[39m.\u001b[39mpprint(d)\n\u001b[0;32m--> 243\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49md)\n",
      "File \u001b[0;32m~/projects/hwat/hwat.py:26\u001b[0m, in \u001b[0;36mAnsatz_fb.__init__\u001b[0;34m(self, n_e, n_u, n_d, n_det, n_fb, n_pv, n_sv, a, with_sign)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39massert\u001b[39;00m (\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn1) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_fb\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mand\u001b[39;00m (\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn2) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_fb)\n\u001b[1;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mVs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([nn\u001b[39m.\u001b[39mLinear(\u001b[39m3\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn1[i]\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn2[i], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn1[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_fb)])\n\u001b[0;32m---> 26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn2[i], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn2[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_fb)])\n\u001b[1;32m     28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mV_half_u \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_sv, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_sv \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mV_half_d \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_sv, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_sv \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/projects/hwat/hwat.py:26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39massert\u001b[39;00m (\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn1) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_fb\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mand\u001b[39;00m (\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn2) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_fb)\n\u001b[1;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mVs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([nn\u001b[39m.\u001b[39mLinear(\u001b[39m3\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn1[i]\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn2[i], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn1[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_fb)])\n\u001b[0;32m---> 26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn2[i], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn2[i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_fb)])\n\u001b[1;32m     28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mV_half_u \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_sv, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_sv \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mV_half_d \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_sv, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_sv \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "### model ###\n",
    "from functools import partial\n",
    "from hwat import Ansatz_fb\n",
    "from torch import nn\n",
    "\n",
    "import pprint\n",
    "\n",
    "from hwat import init_r, get_center_points\n",
    "x = torch.randn(1)\n",
    "c._convert(device, dtype=x.dtype)\n",
    "n_e = c.data.n_e\n",
    "center_points = get_center_points(c.data.n_e, c.data.a)\n",
    "r = init_r(n_device, c.data.n_b, c.data.n_e, center_points, std=0.1).to(device)\n",
    "dtype = r.dtype\n",
    "\n",
    "print('r: ', r.shape, r.dtype)\n",
    "r = r[0] # single batch, single gpu, ‚ùó how to multi gpu\n",
    "model = c.partial(Ansatz_fb).to(device)\n",
    "\n",
    "model_check = c.partial(Ansatz_fb, with_sign=True).to(device)\n",
    "r_swap = r[0, [1,0]+[i for i in range(2, c.data.n_e)]]\n",
    "lp0, s0 = model_check(r[0])\n",
    "lp1, s1 = model_check(r_swap)\n",
    "print('anti-symmetry: ', lp0.item(), lp0.item(), s0.item(), s1.item())\n",
    "r_swap = r[0, [i for i in range(0, c.data.n_e-2)]+[n_e-1,n_e-2]]\n",
    "lp1, s1 = model_check(r_swap)\n",
    "print('anti-symmetry: ', lp0.item(), lp0.item(), s0.item(), s1.item())\n",
    "\n",
    "pprint.pprint(c.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': torch.Size([256]),\n",
      " 'grads': 18,\n",
      " 'ke': torch.Size([256]),\n",
      " 'params': 18,\n",
      " 'pe': torch.Size([256]),\n",
      " 'r': torch.Size([256, 4, 3])}\n",
      "exp/actual | \n",
      "\tcps    : (4, 3)/torch.Size([4, 3])\n",
      "\tr      : (1, 256, 4, 3)/torch.Size([256, 4, 3])\n",
      "\tdeltar : (1,)/torch.Size([1])\n",
      "\n",
      "Go see: https://wandb.ai/hwat/hwat/runs/2qztkgat\n",
      "{'e': '-8.7969', 'ke': '-7.6951', 'pe': '-12.6444', 'r': '-0.0185'}\n",
      "{'e': '-8.2510', 'ke': '-8.9282', 'pe': '-12.7151', 'r': '-0.0201'}\n",
      "{'e': '-13.1621', 'ke': '-5.3839', 'pe': '-15.8541', 'r': '0.0301'}\n",
      "{'e': '-18.9316', 'ke': '9.9038', 'pe': '-13.9797', 'r': '-0.0116'}\n",
      "{'e': '-3.5923', 'ke': '-16.1036', 'pe': '-11.6441', 'r': '0.0276'}\n",
      "{'e': '-6.4631', 'ke': '-9.5289', 'pe': '-11.2276', 'r': '0.0079'}\n",
      "{'e': '-4.4273', 'ke': '-14.1766', 'pe': '-11.5156', 'r': '0.0239'}\n",
      "{'e': '-4.8130', 'ke': '-13.6131', 'pe': '-11.6195', 'r': '0.0156'}\n",
      "{'e': '-3.9907', 'ke': '-15.1778', 'pe': '-11.5796', 'r': '0.0302'}\n",
      "{'e': '-3.8649', 'ke': '-13.4845', 'pe': '-10.6072', 'r': '-0.0087'}\n"
     ]
    }
   ],
   "source": [
    "### train step ###\n",
    "from hwat import compute_ke_b, compute_pe_b\n",
    "from functorch import vmap, make_functional, grad\n",
    "\n",
    "def train_step(model, r):\n",
    "\n",
    "\tmodel_fn, params = make_functional(model)\n",
    "\tmodel_fn = vmap(model_fn, in_dims=(None, 0))\n",
    "\tmodel_rv = lambda _r: model_fn(params, _r)\n",
    "\tmodel_gv = lambda _params: model_fn(_params, r)\n",
    "\n",
    "\t# with torch.no_grad():\n",
    "\tke = compute_ke_b(model_rv, r)\n",
    "\tpe = compute_pe_b(r, c.data.a, c.data.a_z)\n",
    "\t# print(pe.shape, ke.shape)\n",
    "\te = -0.5*ke + pe \n",
    "\te_mean_dist = torch.mean(torch.abs(torch.median(e) - e))\n",
    "\t# print(e.shape, e_mean_dist.shape)\n",
    "\te_clip = torch.clip(e, min=e-5*e_mean_dist, max=e+5*e_mean_dist)\n",
    "\n",
    "\tloss = lambda _params: ((e_clip - e_clip.mean())*model_gv(_params)).mean()\n",
    "\tgrad_fn = grad(loss)\t\n",
    "\tgrads = grad_fn(params)\n",
    "\n",
    "\tfor p, g in zip(model.parameters(), grads):    # ‚ùó do we want functional or other design?! \n",
    "\t\tp.grad = g.clone()\n",
    "  \n",
    "\t# loss.backward()\n",
    "\tgrads = [p.detach().cpu().numpy() for p in model.parameters()]\n",
    "\tparams = [p.detach().cpu().numpy() for p in model.parameters()]  \n",
    "\n",
    "\tv_tr = dict(\n",
    "\t\tparams=params, grads=grads,\n",
    "\t\te=e, pe=pe, ke=ke,\n",
    "\t\tr=r\n",
    "\t)\n",
    "\treturn v_tr\n",
    "\n",
    "r = r.to(device)\n",
    "center_points = center_points.to(device)\n",
    "\n",
    "v_tr = train_step(model, r)\n",
    "pprint.pprint({k:v.shape if isinstance(v, torch.Tensor) else len(v) for k,v in v_tr.items()})\n",
    "\n",
    "\n",
    "### init variables ###\n",
    "deltar = torch.tensor([0.02]).to(device)\n",
    "\n",
    "print(f\"\"\"exp/actual | \n",
    "\tcps    : {(c.data.n_e,3)}/{center_points.shape}\n",
    "\tr      : {(c.n_device, c.data.n_b, c.data.n_e, 3)}/{r.shape}\n",
    "\tdeltar : {(1,)}/{deltar.shape}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "### init functions ### \n",
    "from hwat import sample_b\n",
    "\n",
    "### train ###\n",
    "import wandb\n",
    "from hwat import keep_around_points\n",
    "from utils import compute_metrix\n",
    "from time import time\n",
    "t0 = time()\n",
    "\n",
    "### add in optimiser\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "c.log_metric_step = 1\n",
    "### fix sampler\n",
    "### fix train step \n",
    "### metrix conversion\n",
    "# print(c.wandb_c.)\n",
    "wandb.define_metric(\"*\", step_metric=\"tr/step\")\n",
    "print('Go see:', c._run.url)\n",
    "for step in range(1, c.n_step+1):\n",
    "\t\n",
    "\tmodel_fn, params = make_functional(model)\n",
    "\tmodel_fn = vmap(model_fn, in_dims=(None, 0))\n",
    "\tmodel_rv = lambda _r: model_fn(params, _r)\n",
    " \n",
    "\tr, acc, deltar = sample_b(model_rv, r, deltar, n_corr=c.data.n_corr)  # ‚ùóneeds testing  ‚úÖ\n",
    "\tr = keep_around_points(r, center_points, l=2.) if step < 1000 else r\n",
    "\t\n",
    "\topt.zero_grad()\n",
    "\tv_tr = train_step(model, r)\n",
    "\topt.step()\n",
    "\t\n",
    "\t\n",
    "\tif not (step % c.log_metric_step):\n",
    "\t\tv_tr |= dict(acc=0.0, deltar=deltar)\n",
    "\t\tmetrix = compute_metrix(v_tr)  # ‚ùó needs converting to torch, ie tree maps ‚úÖ\n",
    "\t\twandb.log({'tr/step':step, **metrix})\n",
    "\t\n",
    "\tdiff = time()-t0\n",
    "\tif diff > 100: \n",
    "\t\tt0 = time()\n",
    "\t\tpprint.pprint({k:f'{v.mean().item():.4f}' for k,v in v_tr.items() if isinstance(v, torch.Tensor)} | {'step': step})\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ```{toggle} env vars and jax debug config notes\n",
    "# ‚ùáÔ∏è Magic & debug not currently used\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %env CUDA_VISIBLE_DEVICES='3'\n",
    "# %env \"WANDB_NOTEBOOK_NAME\" \"run.ipynb\" # ‚ùïsame as notebook\n",
    "\n",
    "# from jax.config import config\n",
    "# config.update('jax_disable_jit', True)\n",
    "# ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b38d4d910256a52431c1e658e4ec4972e8b118bd5f76052a504536bbce1f208"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
