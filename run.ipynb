{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init sub classes\n",
      "updating configuration\n",
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: 1e03f0c stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n",
      "running script\n",
      "setting exp_path\n",
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: 1e03f0c stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxmax1\u001b[0m (\u001b[33mhwat\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>dump/exp-6/poKx0Qm/wandb/run-20221221_180611-2qztkgat</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hwat/hwat/runs/2qztkgat\" target=\"_blank\">soft-paper-143</a></strong> to <a href=\"https://wandb.ai/hwat/hwat\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ 1 GPUs available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' copy lines and run in analysis while the exp is live '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Distribution ‚ú® ‚ùá Demo üí™ ### \n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "### fancy logging variables, philosophically reminding us of the goal ###\n",
    "fancy = dict(\n",
    "\t\tpe\t\t= r'$V(X)',    \t\t\t\t\n",
    "\t\tke\t\t= r'$\\nabla^2',    \t\t\n",
    "\t\te\t\t= r'$E',\t\t\t\t\t\t\n",
    "\t\tlog_psi\t= r'$\\log\\psi', \t\t\t\n",
    "\t\tdeltar\t= r'$\\delta_\\mathrm{r}',\t\n",
    "\t\tx\t\t= r'$r_\\mathrm{e}',\n",
    ")\n",
    "\n",
    "### pyfig ###\n",
    "from pyfig import Pyfig\n",
    "\n",
    "# arg = {\n",
    "# \t'a_z':[4,], \n",
    "# \t'n_b': 256, \n",
    "# \t'n_sv': 16, \n",
    "# \t'n_pv': 16, \n",
    "# \t'n_corr': 20, \n",
    "# \t'n_step': 100000, \n",
    "# \t'log_metric_step': 1, \n",
    "# \t'exp_name':'demo',\n",
    "# }\n",
    "\n",
    "\n",
    "c = Pyfig(wb_mode='online', submit=False, run_sweep=False, notebook=True)\n",
    "\n",
    "n_device = c.n_device\n",
    "print(f'ü§ñ {n_device} GPUs available')\n",
    "\n",
    "\n",
    "# from pprint import pprint\n",
    "# pprint(c.d)\n",
    "\n",
    "\"\"\" live plotting in another notebook \"\"\"\n",
    "\"\"\" copy lines and run in analysis while the exp is live \"\"\"\n",
    "# api = wandb.Api()\n",
    "# run = api.run(\"<run-here>\")\n",
    "# c = run.config\n",
    "# h = run.history()\n",
    "# s = run.summary\n",
    "\n",
    "\n",
    "# SOLVE THE CONUNDRUM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:  True n_dev:  1 device <torch.cuda.device object at 0x7fce9aca0a30> name:  NVIDIA TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1234)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)   # ‚ùó Ensure works when default not set AND can go float32 or 64\n",
    "cuda = torch.cuda.is_available()\n",
    "n_device = torch.cuda.device_count()\n",
    "current_device = torch.cuda.current_device()\n",
    "device = torch.cuda.device(0)\n",
    "device_name = torch.cuda.get_device_name(0)\n",
    "print('cuda: ', cuda, 'n_dev: ', n_device, 'device', device, 'name: ', device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r:  torch.Size([1, 256, 4, 3]) torch.float64\n",
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: 1e03f0c stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n",
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: 1e03f0c stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n",
      "anti-symmetry:  -12.842213024358934 -12.842213024358934 1.0 -1.0\n",
      "anti-symmetry:  -12.842213024358934 -12.842213024358934 1.0 -1.0\n",
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: 1e03f0c stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n",
      "{'TMP': PosixPath('dump/tmp'),\n",
      " 'cap': 3,\n",
      " 'commit_id': '1e03f0c',\n",
      " 'data': {'a': tensor([[0., 0., 0.]], device='cuda:0'),\n",
      "          'a_z': tensor([4.], device='cuda:0'),\n",
      "          'acc_target': 0.5,\n",
      "          'charge': 0,\n",
      "          'n_b': 256,\n",
      "          'n_corr': 20,\n",
      "          'n_d': 2,\n",
      "          'n_e': 4,\n",
      "          'n_equil': 10000,\n",
      "          'n_u': 2,\n",
      "          'spin': 0},\n",
      " 'debug': False,\n",
      " 'dtype': 'float32',\n",
      " 'dump': PosixPath('dump'),\n",
      " 'env': 'lumi',\n",
      " 'exp_name': '',\n",
      " 'exp_path': PosixPath('dump/exp-6/poKx0Qm'),\n",
      " 'git_branch': 'main',\n",
      " 'git_remote': 'origin',\n",
      " 'hostname': 'oceanus ',\n",
      " 'log_metric_step': 10,\n",
      " 'log_state_step': 10,\n",
      " 'model': {'n_det': 1,\n",
      "           'n_fb': 3,\n",
      "           'n_fbv': 128,\n",
      "           'n_pv': 16,\n",
      "           'n_sv': 32,\n",
      "           'terms_p_emb': ['rr', 'rr_len'],\n",
      "           'terms_s_emb': ['ra', 'ra_len'],\n",
      "           'with_sign': False},\n",
      " 'n_device': 1,\n",
      " 'n_step': 10000,\n",
      " 'project': 'hwat',\n",
      " 'project_dir': PosixPath('/home/amawi/projects/hwat'),\n",
      " 'run_dir': PosixPath('projects/hwat'),\n",
      " 'run_name': 'run.py',\n",
      " 'run_sweep': False,\n",
      " 'sbatch': 'module purge\\n'\n",
      "           'source ~/.bashrc\\n'\n",
      "           'module load GCC\\n'\n",
      "           'module load CUDA/11.4.1\\n'\n",
      "           'module load cuDNN/8.2.2.26-CUDA-11.4.1\\n'\n",
      "           'conda activate lumi',\n",
      " 'seed': 808017424,\n",
      " 'server': 'svol.fysik.dtu.dk',\n",
      " 'server_project_dir': PosixPath('projects/hwat'),\n",
      " 'slurm': {'cpus_per_task': 1,\n",
      "           'error': PosixPath('dump/exp-6/poKx0Qm/e-%j.err'),\n",
      "           'gres': 'gpu:RTX3090:1',\n",
      "           'job_name': '',\n",
      "           'mail_type': 'FAIL',\n",
      "           'nodes': 1,\n",
      "           'ntasks': 8,\n",
      "           'output': PosixPath('dump/exp-6/poKx0Qm/o-%j.out'),\n",
      "           'partition': 'sm3090',\n",
      "           'time': '0-12:00:00'},\n",
      " 'submit': False,\n",
      " 'sweep': {'method': 'grid',\n",
      "           'name': 'demo',\n",
      "           'parameters': {'n_b': {'values': [16, 32, 64]}}},\n",
      " 'sweep_id': '',\n",
      " 'sweep_path_id': '',\n",
      " 'user': 'amawi',\n",
      " 'wandb_c': {'entity': 'hwat',\n",
      "             'job_type': 'training',\n",
      "             'name': '',\n",
      "             'program': PosixPath('projects/hwat/run.py')},\n",
      " 'wb_mode': 'online'}\n"
     ]
    }
   ],
   "source": [
    "### model ###\n",
    "from functools import partial\n",
    "from hwat import Ansatz_fb\n",
    "from torch import nn\n",
    "\n",
    "import pprint\n",
    "\n",
    "from hwat import init_r, get_center_points\n",
    "x = torch.randn(1)\n",
    "c._convert(device, dtype=x.dtype)\n",
    "n_e = c.data.n_e\n",
    "center_points = get_center_points(c.data.n_e, c.data.a)\n",
    "r = init_r(n_device, c.data.n_b, c.data.n_e, center_points, std=0.1).to(device)\n",
    "dtype = r.dtype\n",
    "\n",
    "print('r: ', r.shape, r.dtype)\n",
    "r = r[0] # single batch, single gpu, ‚ùó how to multi gpu\n",
    "model = c.partial(Ansatz_fb).to(device)\n",
    "\n",
    "model_check = c.partial(Ansatz_fb, with_sign=True).to(device)\n",
    "r_swap = r[0, [1,0]+[i for i in range(2, c.data.n_e)]]\n",
    "lp0, s0 = model_check(r[0])\n",
    "lp1, s1 = model_check(r_swap)\n",
    "print('anti-symmetry: ', lp0.item(), lp0.item(), s0.item(), s1.item())\n",
    "r_swap = r[0, [i for i in range(0, c.data.n_e-2)]+[n_e-1,n_e-2]]\n",
    "lp1, s1 = model_check(r_swap)\n",
    "print('anti-symmetry: ', lp0.item(), lp0.item(), s0.item(), s1.item())\n",
    "\n",
    "pprint.pprint(c.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': torch.Size([256]),\n",
      " 'grads': 18,\n",
      " 'ke': torch.Size([256]),\n",
      " 'params': 18,\n",
      " 'pe': torch.Size([256]),\n",
      " 'r': torch.Size([256, 4, 3])}\n",
      "exp/actual | \n",
      "\tcps    : (4, 3)/torch.Size([4, 3])\n",
      "\tr      : (1, 256, 4, 3)/torch.Size([256, 4, 3])\n",
      "\tdeltar : (1,)/torch.Size([1])\n",
      "\n",
      "Go see: https://wandb.ai/hwat/hwat/runs/2qztkgat\n",
      "{'e': '-8.7969', 'ke': '-7.6951', 'pe': '-12.6444', 'r': '-0.0185'}\n",
      "{'e': '-8.2510', 'ke': '-8.9282', 'pe': '-12.7151', 'r': '-0.0201'}\n",
      "{'e': '-13.1621', 'ke': '-5.3839', 'pe': '-15.8541', 'r': '0.0301'}\n"
     ]
    }
   ],
   "source": [
    "### train step ###\n",
    "from hwat import compute_ke_b, compute_pe_b\n",
    "from functorch import vmap, make_functional, grad\n",
    "\n",
    "def train_step(model, r):\n",
    "\n",
    "\tmodel_fn, params = make_functional(model)\n",
    "\tmodel_fn = vmap(model_fn, in_dims=(None, 0))\n",
    "\tmodel_rv = lambda _r: model_fn(params, _r)\n",
    "\tmodel_gv = lambda _params: model_fn(_params, r)\n",
    "\n",
    "\t# with torch.no_grad():\n",
    "\tke = compute_ke_b(model_rv, r)\n",
    "\tpe = compute_pe_b(r, c.data.a, c.data.a_z)\n",
    "\t# print(pe.shape, ke.shape)\n",
    "\te = -0.5*ke + pe \n",
    "\te_mean_dist = torch.mean(torch.abs(torch.median(e) - e))\n",
    "\t# print(e.shape, e_mean_dist.shape)\n",
    "\te_clip = torch.clip(e, min=e-5*e_mean_dist, max=e+5*e_mean_dist)\n",
    "\n",
    "\tloss = lambda _params: ((e_clip - e_clip.mean())*model_gv(_params)).mean()\n",
    "\tgrad_fn = grad(loss)\t\n",
    "\tgrads = grad_fn(params)\n",
    "\n",
    "\tfor p, g in zip(model.parameters(), grads):    # ‚ùó do we want functional or other design?! \n",
    "\t\tp.grad = g.clone()\n",
    "  \n",
    "\t# loss.backward()\n",
    "\tgrads = [p.detach().cpu().numpy() for p in model.parameters()]\n",
    "\tparams = [p.detach().cpu().numpy() for p in model.parameters()]  \n",
    "\n",
    "\tv_tr = dict(\n",
    "\t\tparams=params, grads=grads,\n",
    "\t\te=e, pe=pe, ke=ke,\n",
    "\t\tr=r\n",
    "\t)\n",
    "\treturn v_tr\n",
    "\n",
    "r = r.to(device)\n",
    "center_points = center_points.to(device)\n",
    "\n",
    "v_tr = train_step(model, r)\n",
    "pprint.pprint({k:v.shape if isinstance(v, torch.Tensor) else len(v) for k,v in v_tr.items()})\n",
    "\n",
    "\n",
    "### init variables ###\n",
    "deltar = torch.tensor([0.02]).to(device)\n",
    "\n",
    "print(f\"\"\"exp/actual | \n",
    "\tcps    : {(c.data.n_e,3)}/{center_points.shape}\n",
    "\tr      : {(c.n_device, c.data.n_b, c.data.n_e, 3)}/{r.shape}\n",
    "\tdeltar : {(1,)}/{deltar.shape}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "### init functions ### \n",
    "from hwat import sample_b\n",
    "\n",
    "### train ###\n",
    "import wandb\n",
    "from hwat import keep_around_points\n",
    "from utils import compute_metrix\n",
    "from time import time\n",
    "t0 = time()\n",
    "\n",
    "### add in optimiser\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "c.log_metric_step = 1\n",
    "### fix sampler\n",
    "### fix train step \n",
    "### metrix conversion\n",
    "# print(c.wandb_c.)\n",
    "wandb.define_metric(\"*\", step_metric=\"tr/step\")\n",
    "print('Go see:', c._run.url)\n",
    "for step in range(1, c.n_step+1):\n",
    "\t\n",
    "\tmodel_fn, params = make_functional(model)\n",
    "\tmodel_fn = vmap(model_fn, in_dims=(None, 0))\n",
    "\tmodel_rv = lambda _r: model_fn(params, _r)\n",
    " \n",
    "\tr, acc, deltar = sample_b(model_rv, r, deltar, n_corr=c.data.n_corr)  # ‚ùóneeds testing  ‚úÖ\n",
    "\tr = keep_around_points(r, center_points, l=2.) if step < 1000 else r\n",
    "\t\n",
    "\topt.zero_grad()\n",
    "\tv_tr = train_step(model, r)\n",
    "\topt.step()\n",
    "\t\n",
    "\t\n",
    "\tif not (step % c.log_metric_step):\n",
    "\t\tv_tr |= dict(acc=0.0, deltar=deltar)\n",
    "\t\tmetrix = compute_metrix(v_tr)  # ‚ùó needs converting to torch, ie tree maps ‚úÖ\n",
    "\t\twandb.log({'tr/step':step, **metrix})\n",
    "\t\n",
    "\tdiff = time()-t0\n",
    "\tif diff > 100: \n",
    "\t\tt0 = time()\n",
    "\t\tpprint.pprint({k:f'{v.mean().item():.4f}' for k,v in v_tr.items() if isinstance(v, torch.Tensor)} | {'step': step})\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ```{toggle} env vars and jax debug config notes\n",
    "# ‚ùáÔ∏è Magic & debug not currently used\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %env CUDA_VISIBLE_DEVICES='3'\n",
    "# %env \"WANDB_NOTEBOOK_NAME\" \"run.ipynb\" # ‚ùïsame as notebook\n",
    "\n",
    "# from jax.config import config\n",
    "# config.update('jax_disable_jit', True)\n",
    "# ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b38d4d910256a52431c1e658e4ec4972e8b118bd5f76052a504536bbce1f208"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
