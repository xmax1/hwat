{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 11:54:27.891218: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:497] The NVIDIA driver's CUDA version is 11.4 which is older than the ptxas CUDA version (11.6.55). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "from jax.config import config\n",
    "config.update('jax_disable_jit', True)\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "import wandb\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import random as rnd\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.training import common_utils\n",
    "from flax import linen as nn, jax_utils\n",
    "import optax\n",
    "\n",
    "from pyfig import Pyfig\n",
    "from hwat import logabssumdet, create_masks\n",
    "\n",
    "def wpr(d:dict):\n",
    "    for k,v in d.items():\n",
    "        typ = type(v) \n",
    "        has_shape = hasattr(v, 'shape')\n",
    "        shape = v.shape if has_shape else None\n",
    "        dtype = v.dtype if hasattr(v, 'dtype') else None\n",
    "        mean = jnp.mean(v) if has_shape else v\n",
    "        std = jnp.std(v) if has_shape else None\n",
    "        print(k, f'\\t mean={mean} \\t std={std} \\t shape={shape} \\t dtype={dtype}') # \\t type={typ}\n",
    "\n",
    "\n",
    "# Method 1 completely refer to Pyfig:\n",
    "    # - Can't get module in the args \n",
    "# Must have shape debug print\n",
    "\n",
    "# 11am: \n",
    "# 1- Putting all variables into every Sub - done, it was a loopy mutable issue\n",
    "# 2- Stop printing mask - done, moved masks to Ferminet\n",
    "# 3- \n",
    "\n",
    "c = Pyfig(wandb_mode='disabled', debug=True) # online:on|disabled:off|offline:local, True: \n",
    "# c.d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FermiNet(nn.Module):\n",
    "\t\t\tn_e: int = None\n",
    "\t\t\tn_u: int = None\n",
    "\t\t\tn_d: int = None\n",
    "\t\t\tcompute_s_emb: Callable = None\n",
    "\t\t\tcompute_p_emb: Callable = None\n",
    "\t\t\tcompute_s_perm: Callable = None\n",
    "\t\t\tn_det: int = None\n",
    "\t\t\tn_fb: int = None\n",
    "\t\t\tn_fb_out: int = None\n",
    "\t\t\tn_pv: int = None\n",
    "\t\t\tn_sv: int = None\n",
    "\n",
    "\t\t\t@nn.compact\n",
    "\t\t\tdef __call__(_i, x):\n",
    "\n",
    "\t\t\t\t\t\t\tp_mask_u, p_mask_d = create_masks(_i.n_e, _i.n_u)\n",
    "\n",
    "\t\t\t\t\t\t\txu, xd = jnp.split(x, [_i.n_u,], axis=0)\n",
    "\t\t\t\t\t\t\tx_s_var = _i.compute_s_emb(x)\n",
    "\t\t\t\t\t\t\tx_p_var = _i.compute_p_emb(x)\n",
    "\t\t\t\t\t\t\twpr(dict(x_s_var=x_s_var, x_p_var=x_p_var))\n",
    "\n",
    "\t\t\t\t\t\t\tx_s_res = x_p_res = 0.\n",
    "\t\t\t\t\t\t\tfor _ in range(_i.n_fb):\n",
    "\t\t\t\t\t\t\t\t\t\t\tx_p_var = x_p_res = nn.tanh(nn.Dense(_i.n_pv)(x_p_var)) + x_p_res\n",
    "\t\t\t\t\t\t\t\t\t\t\tx_s_var = _i.compute_s_perm(x_s_var, x_p_var, p_mask_u, p_mask_d)\n",
    "\t\t\t\t\t\t\t\t\t\t\tx_s_var = x_s_res = nn.tanh(nn.Dense(_i.n_sv)(x_s_var)) + x_s_res\n",
    "\t\t\t\t\t\t\t\t\t\t\twpr(dict(x_p_var=x_p_var, x_s_var=x_s_var))\n",
    "\n",
    "\t\t\t\t\t\t\tx_w = nn.tanh(nn.Dense(_i.n_fb_out)(x_s_var))\n",
    "\t\t\t\t\t\t\tx_wu, x_wd = jnp.split(x_w, [_i.n_u,], axis=0)\n",
    "\t\t\t\t\t\t\tx_wu = nn.tanh(nn.Dense(_i.n_det*_i.n_u)(x_wu))\n",
    "\t\t\t\t\t\t\tx_wd = nn.tanh(nn.Dense(_i.n_det*_i.n_d)(x_wd))\n",
    "\t\t\t\t\t\t\twpr(dict(x_w=x_w, x_wu=x_wu, x_wd=x_wd))\n",
    "\n",
    "\t\t\t\t\t\t\torb_u = jnp.stack((x_wu * jnp.exp(-nn.Dense(_i.n_u*_i.n_det)(-xu))).split(_i.n_det, axis=-1)) # (e, f(e)) (e, (f(e))*n_det)\n",
    "\t\t\t\t\t\t\torb_d = jnp.stack((x_wd * jnp.exp(-nn.Dense(_i.n_d*_i.n_det)(-xd))).split(_i.n_det, axis=-1))\n",
    "\t\t\t\t\t\t\twpr(dict(orb_u=orb_u, orb_d=orb_d))\n",
    "\n",
    "\t\t\t\t\t\t\tlog_psi, sgn = logabssumdet(orb_u, orb_d)\n",
    "\t\t\t\t\t\t\treturn log_psi\n",
    "\n",
    "model = c.pass_arg(FermiNet)\n",
    "\n",
    "rng = rnd.PRNGKey(1)\n",
    "x = rnd.normal(rng, (c.data.n_e, 3))\n",
    "params = model.init(rng, x)\n",
    "model.apply(params, x)\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "@partial(jax.pmap, axis_name='b')\n",
    "def create_train_state(rng):\n",
    "\tmodel = c.pass_arg(FermiNet)\n",
    "\tx = rnd.normal(rng, (c.data.n_e, 3))\n",
    "\tparams = model.init(rng, x)['params']\n",
    "\ttx = optax.sgd(c.opt.lr)\n",
    "\treturn TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "rng = rnd.split(rng, len(jax.devices()))\n",
    "state = create_train_state(rng)\n",
    "state = jax_utils.replicate(state)\n",
    "\n",
    "# train step framework get\n",
    "# train step framework copy\n",
    "# test energy function\n",
    "# kinetic energy\n",
    "# potential energy\n",
    "# include atoms\n",
    "# include sampler\n",
    "# write metric\n",
    "# run loop\n",
    "from hwat import SampleState\n",
    "sample = SampleState()\n",
    "\n",
    "@partial(jax.pmap, axis_name='x')\n",
    "def train_step(state, x):\n",
    "\n",
    "\tx = sample(x, state)\n",
    "\n",
    "\tke = c.compute_ke()\n",
    "\tpe = c.compute_pe()\n",
    "\te = ke+pe\n",
    "\n",
    "\tdef loss_fn(p):\n",
    "\t\tout = model.apply({'params': p}, x) \n",
    "\t\treturn jnp.mean(out*e)\n",
    "\n",
    "\tgrad_fn = jax.value_and_grad(loss_fn, has_aux=False)  # has_aux for more than one out\n",
    "\tout, grads = grad_fn(state.params)\n",
    "\tlog_psi = out\n",
    "\n",
    "\tv_b = { # scalars\n",
    "\t\tr'V(X)'    \t\t\t\t\t: pe,\n",
    "\t\tr'$\\nabla^2'    \t\t\t: ke,\n",
    "\t\t'E'\t\t\t\t\t\t\t: e,\n",
    "\t\tr'$\\log\\psi$' \t\t\t\t: log_psi,\n",
    "\t\tr'\\delta_\\mathrm{r}'\t\t: sample.move_std,\n",
    "\t}\n",
    "\n",
    "\treturn state, grads, x, v_b\n",
    "\n",
    "@jax.pmap\n",
    "def update_model(state, grads):\n",
    "\treturn state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wandb.define_metric(\"*\", step_metric=\"train/step\")\n",
    "\n",
    "# # Display a project workspace\n",
    "# %wandb USERNAME/PROJECT\n",
    "# # Display a single run\n",
    "# %wandb USERNAME/PROJECT/runs/RUN_ID\n",
    "# # Display a sweep\n",
    "# %wandb USERNAME/PROJECT/sweeps/SWEEP_ID\n",
    "# # Display a report\n",
    "# %wandb USERNAME/PROJECT/reports/REPORT_ID\n",
    "# # Specify the height of embedded iframe\n",
    "# %wandb USERNAME/PROJECT -h 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = []\n",
    "\n",
    "for step in range(c.n_step):\n",
    "        \n",
    "    state, grads, x, data = train_step(state, x)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    if step % c.log_metric_step == 0:\n",
    "        \n",
    "        # r'sgn$(\\cdot)$'     : sgn\n",
    "        metric = compute_metric(metric)\n",
    "        \n",
    "        summary = {\n",
    "            f'train/{k}': v\n",
    "            for k, v in jax.tree_map(lambda x: x.mean(), train_metrics).items()\n",
    "        }\n",
    "\n",
    "        wandb.log({\n",
    "                \"train/step\": step, \n",
    "                **summary\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMA Decay extension\n",
    "if step <= c.ema.update_after_step:\n",
    "            state = p_copy_params_to_ema(state)\n",
    "\n",
    "        elif step % c.ema.update_every == 0:\n",
    "            ema_decay = ema_decay_fn(step)\n",
    "            state =  p_apply_ema(state, ema_decay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('dex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b4edb4a58a0461d84e636e9142615dc364f099c3851533546c18fbe9e367308"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
