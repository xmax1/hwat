{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 11:54:27.891218: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:497] The NVIDIA driver's CUDA version is 11.4 which is older than the ptxas CUDA version (11.6.55). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "from jax.config import config\n",
    "config.update('jax_disable_jit', True)\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "import wandb\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import random as rnd\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.training import common_utils\n",
    "from flax import linen as nn, jax_utils\n",
    "import optax\n",
    "\n",
    "from pyfig import Pyfig\n",
    "from hwat import logabssumdet, create_masks\n",
    "\n",
    "def wpr(d:dict):\n",
    "    for k,v in d.items():\n",
    "        typ = type(v) \n",
    "        has_shape = hasattr(v, 'shape')\n",
    "        shape = v.shape if has_shape else None\n",
    "        dtype = v.dtype if hasattr(v, 'dtype') else None\n",
    "        mean = jnp.mean(v) if has_shape else v\n",
    "        std = jnp.std(v) if has_shape else None\n",
    "        print(k, f'\\t mean={mean} \\t std={std} \\t shape={shape} \\t dtype={dtype}') # \\t type={typ}\n",
    "\n",
    "# Method 1 completely refer to Pyfig:\n",
    "    # - Can't get module in the args \n",
    "# Must have shape debug print\n",
    "\n",
    "# 11am: \n",
    "# 1- Putting all variables into every Sub - done, it was a loopy mutable issue\n",
    "# 2- Stop printing mask - done, moved masks to Ferminet\n",
    "# 3- \n",
    "\n",
    "c = Pyfig(wandb_mode='disabled', debug=True) # online:on|disabled:off|offline:local, True: \n",
    "# c.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_s_var \t mean=1.9469865560531616 \t std=0.7550688982009888 \t shape=(10, 1) \t dtype=float32\n",
      "x_p_var \t mean=2.524434804916382 \t std=1.3280136585235596 \t shape=(10, 10, 1) \t dtype=float32\n",
      "x_p_var \t mean=-0.6352688670158386 \t std=0.47677209973335266 \t shape=(10, 10, 8) \t dtype=float32\n",
      "x_s_var \t mean=0.09472795575857162 \t std=0.5215277075767517 \t shape=(10, 16) \t dtype=float32\n",
      "x_p_var \t mean=-0.28023380041122437 \t std=0.4364957809448242 \t shape=(10, 10, 8) \t dtype=float32\n",
      "x_s_var \t mean=0.23098309338092804 \t std=0.6535929441452026 \t shape=(10, 16) \t dtype=float32\n",
      "x_w \t mean=-0.004265791270881891 \t std=0.5131012201309204 \t shape=(10, 64) \t dtype=float32\n",
      "x_wu \t mean=-0.15545150637626648 \t std=0.505211353302002 \t shape=(5, 5) \t dtype=float32\n",
      "x_wd \t mean=-0.07841941714286804 \t std=0.3952345550060272 \t shape=(5, 5) \t dtype=float32\n",
      "orb_u \t mean=-0.7551324963569641 \t std=2.791072368621826 \t shape=(1, 5, 5) \t dtype=float32\n",
      "orb_d \t mean=-0.0950351282954216 \t std=0.5227019190788269 \t shape=(1, 5, 5) \t dtype=float32\n",
      "x_s_var \t mean=1.9469865560531616 \t std=0.7550688982009888 \t shape=(10, 1) \t dtype=float32\n",
      "x_p_var \t mean=2.524434804916382 \t std=1.3280136585235596 \t shape=(10, 10, 1) \t dtype=float32\n",
      "x_p_var \t mean=-0.6352688670158386 \t std=0.47677209973335266 \t shape=(10, 10, 8) \t dtype=float32\n",
      "x_s_var \t mean=0.09472795575857162 \t std=0.5215277075767517 \t shape=(10, 16) \t dtype=float32\n",
      "x_p_var \t mean=-0.28023380041122437 \t std=0.4364957809448242 \t shape=(10, 10, 8) \t dtype=float32\n",
      "x_s_var \t mean=0.23098309338092804 \t std=0.6535929441452026 \t shape=(10, 16) \t dtype=float32\n",
      "x_w \t mean=-0.004265791270881891 \t std=0.5131012201309204 \t shape=(10, 64) \t dtype=float32\n",
      "x_wu \t mean=-0.15545150637626648 \t std=0.505211353302002 \t shape=(5, 5) \t dtype=float32\n",
      "x_wd \t mean=-0.07841941714286804 \t std=0.3952345550060272 \t shape=(5, 5) \t dtype=float32\n",
      "orb_u \t mean=-0.7551324963569641 \t std=2.791072368621826 \t shape=(1, 5, 5) \t dtype=float32\n",
      "orb_d \t mean=-0.0950351282954216 \t std=0.5227019190788269 \t shape=(1, 5, 5) \t dtype=float32\n",
      "x_s_var \t mean=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t std=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t shape=(10, 1) \t dtype=float32\n",
      "x_p_var \t mean=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t std=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t shape=(10, 10, 1) \t dtype=float32\n",
      "x_p_var \t mean=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t std=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t shape=(10, 10, 8) \t dtype=float32\n",
      "x_s_var \t mean=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t std=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t shape=(10, 16) \t dtype=float32\n",
      "x_p_var \t mean=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t std=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t shape=(10, 10, 8) \t dtype=float32\n",
      "x_s_var \t mean=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t std=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t shape=(10, 16) \t dtype=float32\n",
      "x_w \t mean=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t std=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t shape=(10, 64) \t dtype=float32\n",
      "x_wu \t mean=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t std=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t shape=(5, 5) \t dtype=float32\n",
      "x_wd \t mean=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t std=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t shape=(5, 5) \t dtype=float32\n",
      "orb_u \t mean=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t std=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t shape=(1, 5, 5) \t dtype=float32\n",
      "orb_d \t mean=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t std=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> \t shape=(1, 5, 5) \t dtype=float32\n"
     ]
    }
   ],
   "source": [
    "class FermiNet(nn.Module):\n",
    "    n_e: int = None\n",
    "    n_u: int = None\n",
    "    n_d: int = None\n",
    "    compute_s_emb: Callable = None\n",
    "    compute_p_emb: Callable = None\n",
    "    compute_s_perm: Callable = None\n",
    "    n_det: int = None\n",
    "    n_fb: int = None\n",
    "    n_fb_out: int = None\n",
    "    n_pv: int = None\n",
    "    n_sv: int = None\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(_i, x):\n",
    "\n",
    "        p_mask_u, p_mask_d = create_masks(_i.n_e, _i.n_u)\n",
    "        \n",
    "        xu, xd = jnp.split(x, [_i.n_u,], axis=0)\n",
    "        x_s_var = _i.compute_s_emb(x)\n",
    "        x_p_var = _i.compute_p_emb(x)\n",
    "        wpr(dict(x_s_var=x_s_var, x_p_var=x_p_var))\n",
    "\n",
    "        x_s_res = x_p_res = 0.\n",
    "        for _ in range(_i.n_fb):\n",
    "            x_p_var = x_p_res = nn.tanh(nn.Dense(_i.n_pv)(x_p_var)) + x_p_res\n",
    "            x_s_var = _i.compute_s_perm(x_s_var, x_p_var, p_mask_u, p_mask_d)\n",
    "            x_s_var = x_s_res = nn.tanh(nn.Dense(_i.n_sv)(x_s_var)) + x_s_res\n",
    "            wpr(dict(x_p_var=x_p_var, x_s_var=x_s_var))\n",
    "\n",
    "        x_w = nn.tanh(nn.Dense(_i.n_fb_out)(x_s_var))\n",
    "        x_wu, x_wd = jnp.split(x_w, [_i.n_u,], axis=0)\n",
    "        x_wu = nn.tanh(nn.Dense(_i.n_det*_i.n_u)(x_wu))\n",
    "        x_wd = nn.tanh(nn.Dense(_i.n_det*_i.n_d)(x_wd))\n",
    "        wpr(dict(x_w=x_w, x_wu=x_wu, x_wd=x_wd))\n",
    "        \n",
    "        orb_u = jnp.stack((x_wu * jnp.exp(-nn.Dense(_i.n_u*_i.n_det)(-xu))).split(_i.n_det, axis=-1)) # (e, f(e)) (e, (f(e))*n_det)\n",
    "        orb_d = jnp.stack((x_wd * jnp.exp(-nn.Dense(_i.n_d*_i.n_det)(-xd))).split(_i.n_det, axis=-1))\n",
    "        wpr(dict(orb_u=orb_u, orb_d=orb_d))\n",
    "\n",
    "        log_psi, sgn = logabssumdet(orb_u, orb_d)\n",
    "        return log_psi\n",
    "\n",
    "model = c.pass_arg(FermiNet)\n",
    "\n",
    "rng = rnd.PRNGKey(1)\n",
    "x = rnd.normal(rng, (c.data.n_e, 3))\n",
    "params = model.init(rng, x)\n",
    "model.apply(params, x)\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "@partial(jax.pmap, axis_name='b')\n",
    "def create_train_state(rng):\n",
    "  model = c.pass_arg(FermiNet)\n",
    "  x = rnd.normal(rng, (c.data.n_e, 3))\n",
    "  params = model.init(rng, x)['params']\n",
    "  tx = optax.sgd(c.opt.lr)\n",
    "  return TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "rng = rnd.split(rng, len(jax.devices()))\n",
    "state = create_train_state(rng)\n",
    "state = jax_utils.replicate(state)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, state, b):\n",
    "  \n",
    "  b_energy = compute_energy(state)\n",
    "  \n",
    "  def loss_fn(p):\n",
    "    model_out = state.apply_fn({'params':p}, b)\n",
    "    log_psi, sgn = model_out\n",
    "    return log_psi, sgn\n",
    "  \n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  (log_psi, sgn), grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  \n",
    "  variables = {\n",
    "    'E batch'           : b_energy,\n",
    "    r'$\\log\\psi$ batch' : log_psi,\n",
    "    r'sgn$(\\cdot)$'     : sgn\n",
    "  }\n",
    "  \n",
    "  metrics = compute_metric(b_energy, log_psi, sgn)\n",
    "  return state, metrics\n",
    "\n",
    "train_step = jax.pmap(train_step, axis_name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wandb.define_metric(\"*\", step_metric=\"train/step\")\n",
    "\n",
    "# # Display a project workspace\n",
    "# %wandb USERNAME/PROJECT\n",
    "# # Display a single run\n",
    "# %wandb USERNAME/PROJECT/runs/RUN_ID\n",
    "# # Display a sweep\n",
    "# %wandb USERNAME/PROJECT/sweeps/SWEEP_ID\n",
    "# # Display a report\n",
    "# %wandb USERNAME/PROJECT/reports/REPORT_ID\n",
    "# # Specify the height of embedded iframe\n",
    "# %wandb USERNAME/PROJECT -h 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_stats():\n",
    "    def normalize_to_neg_one_to_one(img):\n",
    "        return img * 2 - 1  \n",
    "    data_tr = datasets.FashionMNIST(\n",
    "        root=c.data_dir, \n",
    "        download=True, \n",
    "        train=True,\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(28),\n",
    "            transforms.CenterCrop(28),\n",
    "        ]),\n",
    "        target_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    "    ])\n",
    "    )\n",
    "    imgs = torch.stack([img_t for img_t, _ in data_tr], dim=3)\n",
    "    mean = imgs.view(1,-1).mean(dim=1) \n",
    "    std = imgs.view(1,-1).std(dim=1)\n",
    "    print('Data mean: ', mean, 'Data std: ', std)\n",
    "    return mean, std\n",
    "mean, std = get_data_stats()\n",
    "loader_tr, loader_test = get_fashion_loader(c.data.b_size, c.data_dir, mean=mean, std=std)  # is not an iterator or list\n",
    "img, l = next(iter(loader_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Image.fromarray(np.uint8((img[0, 0]*std+mean).cpu().numpy()*255))\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('1', size=(cols*w, rows*h))\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "\n",
    "    return grid\n",
    "std = float(std.cpu().numpy())\n",
    "mean = float(mean.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = []\n",
    "\n",
    "for ep in range(c.n_epoch):\n",
    "    for step, (b, target) in enumerate(loader_tr):\n",
    "        b = jnp.squeeze(jnp.expand_dims(jnp.array(b.numpy()), axis=(0,-1)), axis=2) # p, B, H, W, C\n",
    "        rng, *train_step_rng = jax.random.split(rng, num=jax.local_device_count() + 1)\n",
    "        train_step_rng = jnp.array(train_step_rng)\n",
    "        \n",
    "        state, metrics = p_train_step(train_step_rng, state, b)\n",
    "\n",
    "        if step <= c.ema.update_after_step:\n",
    "            state = p_copy_params_to_ema(state)\n",
    "\n",
    "        elif step % c.ema.update_every == 0:\n",
    "            ema_decay = ema_decay_fn(step)\n",
    "            state =  p_apply_ema(state, ema_decay)\n",
    "\n",
    "        if step % c.log_metric_step == 0:\n",
    "\n",
    "            train_metrics.append(metrics)\n",
    "            train_metrics = common_utils.get_metrics(train_metrics)\n",
    "            \n",
    "            summary = {\n",
    "                f'train/{k}': v\n",
    "                for k, v in jax.tree_map(lambda x: x.mean(), train_metrics).items()\n",
    "            }\n",
    "\n",
    "            train_metrics = []\n",
    "            b = ((np.array(b)*std+mean)*255).astype(np.uint8)\n",
    "            imgs = [wandb.Image(Image.fromarray(b[0, i].reshape(28, 28))) for i in range(9)]\n",
    "            \n",
    "            wandb.log({\n",
    "                    \"train/step\": step, \n",
    "                    'train/sample': imgs,\n",
    "                    **summary\n",
    "            })\n",
    " \n",
    "    print('Epoch: ', ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.fromarray(b[0, 0].reshape(28, 28, 1))\n",
    "b[0, 0].max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('dex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b4edb4a58a0461d84e636e9142615dc364f099c3851533546c18fbe9e367308"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
