{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import reduce \n",
    "from torch import nn\n",
    "from functorch import vmap\n",
    "torch.set_default_dtype(torch.float64)\n",
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \n",
    "\n",
    "\n",
    "def logabssumdet(xs):\n",
    "\t\t\n",
    "\t\tdets = [x.reshape(-1) for x in xs if x.shape[-1] == 1]\t\t\t\t\t\t# in case n_u or n_d=1, no need to compute determinant\n",
    "\t\tdets = reduce(lambda a,b: a*b, dets) if len(dets)>0 else 1.\t\t\t\t\t# take product of these cases\n",
    "\t\tmaxlogdet = 0.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# initialised for sumlogexp trick (for stability)\n",
    "\t\tdet = dets\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# if both cases satisfy n_u or n_d=1, this is the determinant\n",
    "\t\t\n",
    "\t\tslogdets = [torch.linalg.slogdet(x) for x in xs if x.shape[-1]>1] \t\t\t# otherwise take slogdet\n",
    "\t\tif len(slogdets)>0: \n",
    "\t\t\tsign_in, logdet = reduce(lambda a,b: (a[0]*b[0], a[1]+b[1]), slogdets)  # take product of n_u or n_d!=1 cases\n",
    "\t\t\tmaxlogdet = torch.max(logdet)\t\t\t\t\t\t\t\t\t\t\t# adjusted for new inputs\n",
    "\t\t\tdet = sign_in * dets * torch.exp(logdet-maxlogdet)\t\t\t\t\t\t# product of all these things is determinant\n",
    "\t\t\n",
    "\t\tpsi_ish = torch.sum(det)\n",
    "\t\tsgn_psi = torch.sign(psi_ish)\n",
    "\t\tlog_psi = torch.log(torch.abs(psi_ish)) + maxlogdet\n",
    "\t\treturn log_psi, sgn_psi\n",
    "\n",
    "\n",
    "class FermiNetTorch(nn.Module):\n",
    "\tdef __init__(self, *, n_e=None, n_u=None, n_d=None, n_det=None, n_fb=None, n_pv=None, n_sv=None, a=None, with_sign=False, **kw):\n",
    "\t\tsuper(FermiNetTorch, self).__init__()\n",
    "\t\tself.n_e = n_e                  # number of electrons\n",
    "\t\tself.n_u = n_u                  # number of up electrons\n",
    "\t\tself.n_d = n_d                  # number of down electrons\n",
    "\t\tself.n_det = n_det              # number of determinants\n",
    "\t\tself.n_fb = n_fb                # number of feedforward blocks\n",
    "\t\tself.n_pv = n_pv                # latent dimension for 2-electron\n",
    "\t\tself.n_sv = n_sv                # latent dimension for 1-electron\n",
    "\t\tself.a = torch.tensor(a)                      # nuclei positions\n",
    "\t\tself.with_sign = with_sign      # return sign of wavefunction\n",
    "\n",
    "\t\tself.n1 = [4*self.a.shape[0]] + [self.n_sv]*self.n_fb\n",
    "\t\tself.n2 = [4] + [self.n_pv]*(self.n_fb - 1)\n",
    "\t\tassert (len(self.n1) == self.n_fb+1) and (len(self.n2) == self.n_fb)\n",
    "\t\n",
    "\t\tself.Vs = nn.ModuleList([nn.Linear(3*self.n1[i]+2*self.n2[i], self.n1[i+1]) for i in range(self.n_fb)])\n",
    "\t\tself.Ws = nn.ModuleList([nn.Linear(self.n2[i], self.n2[i+1]) for i in range(self.n_fb-1)])\n",
    "\n",
    "\t\tself.V_half_u = nn.Linear(self.n_sv, self.n_sv // 2)\n",
    "\t\tself.V_half_d = nn.Linear(self.n_sv, self.n_sv // 2)\n",
    "\n",
    "\t\tself.wu = nn.Linear(self.n_sv // 2, self.n_u)\n",
    "\t\tself.wd = nn.Linear(self.n_sv // 2, self.n_d)\n",
    "\n",
    "\t\t# TODO: Multideterminant. If n_det > 1 we should map to n_det*n_u (and n_det*n_d) instead,\n",
    "\t\t#  and then split these outputs in chunks of n_u (n_d)\n",
    "\t\t# TODO: implement layers for sigma and pi\n",
    "\n",
    "\tdef forward(self, r: torch.Tensor):\n",
    "\t\t\"\"\"\n",
    "\t\tBatch dimension is not yet supported.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tif len(r.shape) == 1:\n",
    "\t\t\tr = r.reshape(self.n_e, 3) # (n_e, 3)\n",
    "\n",
    "\t\tdtype=r.dtype\n",
    "\t\tprint(r.shape, r.dtype)\n",
    "\n",
    "\t\teye = torch.eye(self.n_e, device=r.device, dtype=dtype).unsqueeze(-1)\n",
    "\n",
    "\t\tra = r[:, None, :] - self.a[None, :, :] # (n_e, n_a, 3)\n",
    "\t\tra_len = torch.norm(ra, dim=-1, keepdim=True) # (n_e, n_a, 1)\n",
    "\n",
    "\t\trr = r[None, :, :] - r[:, None, :] # (n_e, n_e, 1)\n",
    "\t\trr_len = torch.norm(rr + eye, dim=-1, keepdim=True) * (torch.ones((self.n_e, self.n_e, 1), dtype=dtype)-eye) # (n_e, n_e, 1) \n",
    "\t\t# TODO: Just remove '+eye' from above, it's unnecessary\n",
    "\n",
    "\t\ts_v = torch.cat([ra, ra_len], dim=-1).reshape(self.n_e, -1) # (n_e, n_a*4)\n",
    "\t\tp_v = torch.cat([rr, rr_len], dim=-1) # (n_e, n_e, 4)\n",
    "\n",
    "\t\tfor l, (V, W) in enumerate(zip(self.Vs, self.Ws)):\n",
    "\t\t\tsfb_v = [torch.tile(_v.mean(dim=0)[None, :], (self.n_e, 1)) for _v in torch.split(s_v, 2, dim=0)]\n",
    "\t\t\tpfb_v = [_v.mean(dim=0) for _v in torch.split(p_v, self.n_u, dim=0)]\n",
    "\t\t\t\n",
    "\t\t\ts_v = torch.cat(sfb_v+pfb_v+[s_v,], dim=-1) # s_v = torch.cat((s_v, sfb_v[0], sfb_v[1], pfb_v[0], pfb_v[0]), dim=-1)\n",
    "\t\t\ts_v = torch.tanh(V(s_v)) + (s_v if (s_v.shape[-1]==self.n_sv) else 0.)\n",
    "\t\t\t\n",
    "\t\t\tif not (l == (self.n_fb-1)):\n",
    "\t\t\t\tp_v = torch.tanh(W(p_v)) + (p_v if (p_v.shape[-1]==self.n_pv) else 0.)\n",
    "\t\t\n",
    "\t\ts_u, s_d = torch.split(s_v, self.n_u, dim=0)\n",
    "\n",
    "\t\ts_u = torch.tanh(self.V_half_u(s_u)) # spin dependent size reduction\n",
    "\t\ts_d = torch.tanh(self.V_half_d(s_d))\n",
    "\n",
    "\t\ts_wu = self.wu(s_u) # map to phi orbitals\n",
    "\t\ts_wd = self.wd(s_d)\n",
    "\n",
    "\t\tassert s_wd.shape == (self.n_d, self.n_d)\n",
    "\n",
    "\t\tra_u, ra_d = torch.split(ra, self.n_u, dim=0)\n",
    "\n",
    "\t\t# TODO: implement sigma = nn.Linear() before this\n",
    "\t\texp_u = torch.norm(ra_u, dim=-1, keepdim=True)\n",
    "\t\texp_d = torch.norm(ra_d, dim=-1, keepdim=True)\n",
    "\n",
    "\t\tassert exp_d.shape == (self.n_d, self.a.shape[0], 1)\n",
    "\n",
    "\t\t# TODO: implement pi = nn.Linear() before this\n",
    "\t\torb_u = (s_wu * (torch.exp(-exp_u).sum(axis=1)))[None, :, :]\n",
    "\t\torb_d = (s_wd * (torch.exp(-exp_d).sum(axis=1)))[None, :, :]\n",
    "\n",
    "\t\tassert orb_u.shape == (1, self.n_u, self.n_u)\n",
    "\n",
    "\t\tlog_psi, sgn = logabssumdet([orb_u, orb_d])\n",
    "\n",
    "\t\tif self.with_sign:\n",
    "\t\t\treturn log_psi, sgn\n",
    "\t\telse:\n",
    "\t\t\treturn log_psi.squeeze()\n",
    "\n",
    "\n",
    "from pyfig import Pyfig\n",
    "from utils import flat_any\n",
    "from functorch import make_functional\n",
    "from torch.autograd import gradgradcheck\n",
    "from functools import partial\n",
    "\n",
    "c = Pyfig(wb_mode='disabled', submit=False, run_sweep=False)\n",
    "d = flat_any(c.d)\n",
    "\n",
    "r = torch.randn((c.data.n_b, c.data.n_e, 3))\n",
    "model_b = FermiNetTorch(**d)\n",
    "log_psi_b = model_b(r[0])\n",
    "\n",
    "print(r[0], log_psi_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, params = make_functional(model_b)\n",
    "model = vmap(partial(model, params))\n",
    "log_psi = model(r)\n",
    "\n",
    "n_jvp = c.data.n_e * 3\n",
    "laplacian = torch.zeros(r.shape[0]) #array to store values of laplacian\n",
    "\n",
    "for i, xi in enumerate(r):\n",
    "\thess = torch.autograd.functional.hessian(model, xi.unsqueeze(0), create_graph=True)\n",
    "\tlaplacian[i] = torch.diagonal(hess.view(n_jvp, n_jvp), offset=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import jacrev\n",
    "\n",
    "def kinetic_functorch(params, r):\n",
    "  calc_jacobian = jacrev(logabs, argnums=1) #do once, and re-use?\n",
    "  calc_hessian = jacrev(jacrev(logabs, argnums=1), argnums=1)\n",
    "  return -0.5*torch.sum(calc_hessian(params, r).squeeze(-3).squeeze(-1).diagonal(0,-2,-1) + calc_jacobian(params, r).squeeze(-1).pow(2), dim=-1)\n",
    "\n",
    "#per-sample gradients for local energy w.r.t params via FuncTorch\n",
    "elocal_grad_ft = vmap(grad(kinetic_functorch, argnums=0), in_dims=(None, 0))(params, x)\n",
    "elocal_grad_ft = [p.clone().mean(dim=0) for p in elocal_grad_ft]\n",
    "\n",
    "\n",
    "def kinetic_pytorch(xs: Tensor) -> Tensor:\n",
    "  \"\"\"Method to calculate the local kinetic energy values of a netork function, f, for samples, x.\n",
    "  The values calculated here are 1/f d2f/dx2 which is equivalent to d2log(|f|)/dx2 + (dlog(|f|)/dx)^2\n",
    "  within the log-domain (rather than the linear-domain).\n",
    "  :param xs: The input positions of the many-body particles\n",
    "  :type xs: class: `torch.Tensor`\n",
    "  \"\"\"\n",
    "  xis = [xi.requires_grad_() for xi in xs.flatten(start_dim=1).t()]\n",
    "  xs_flat = torch.stack(xis, dim=1)\n",
    "\n",
    "  _, ys = net(xs_flat.view_as(xs))\n",
    "  #print(\"pytorch logabs: \",ys)\n",
    "  ones = torch.ones_like(ys)\n",
    "\n",
    "  #df_dx calculation\n",
    "  (dy_dxs, ) = torch.autograd.grad(ys, xs_flat, ones, retain_graph=True, create_graph=True)\n",
    "\n",
    "\n",
    "  #d2f_dx2 calculation\n",
    "  lay_ys = sum(torch.autograd.grad(dy_dxi, xi, ones, retain_graph=True, create_graph=False)[0] \\\n",
    "                for xi, dy_dxi in zip(xis, (dy_dxs[..., i] for i in range(len(xis))))\n",
    "  )\n",
    "  \n",
    "  ek_local_per_walker = -0.5 * (lay_ys + dy_dxs.pow(2).sum(-1)) #move const out of loop?\n",
    "  return ek_local_per_walker\n",
    "\n",
    "#PyTorch gradients via reverse-mode AD\n",
    "net.zero_grad()\n",
    "kin_pt = kinetic_pytorch(x)\n",
    "loss = torch.mean(kin_pt)\n",
    "loss.backward()\n",
    "elocal_grad_pt = [param.grad for param in net.parameters()]\n",
    "\n",
    "energy_ft = kinetic_functorch(params, x)\n",
    "energy_pt = kinetic_pytorch(x)\n",
    "\n",
    "print(energy_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/functorch/stable/ux_limitations.html#vmap-limitations\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.autograd.gradgradcheck.html#torch.autograd.gradgradcheck\n",
    "\n",
    "dpsidr2 = gradgradcheck(model, r)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/how-to-calculate-laplacian-for-multiple-batches-in-parallel/104888\n",
    "\n",
    "y = model(x) #where model is an R^N to R^1 function\n",
    "\n",
    "laplacian = torch.zeros(x.shape[0]) #array to store values of laplacian\n",
    "\n",
    "for i, xi in enumerate(x):\n",
    "    hess = torch.autograd.functional.hessian(model, xi.unsqueeze(0), create_graph=True)\n",
    "    laplacian[i] = torch.diagonal(hess.view(N, N) offset=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.autograd.functional.hvp.html\n",
    "# Hessian vector product vs grad * jvp "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hwat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70216fde95fcbf718e02bad93c0be246941620cadd5cc950a99175687f39a779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
