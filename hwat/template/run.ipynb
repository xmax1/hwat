{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "def mkdir(path: Path) -> Path:\n",
    "    path = Path(path)\n",
    "    if path.suffix != '':\n",
    "        path = path.parent\n",
    "    if path.exists():\n",
    "        print('path exists, leaving alone')\n",
    "    else:\n",
    "        path.mkdir(parents=True)\n",
    "    return path\n",
    "\n",
    "this_dir = Path('').parent\n",
    "TMP = mkdir('./tmp/out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from PIL import Image\n",
    "\n",
    "from flax import jax_utils\n",
    "from flax.training import common_utils, train_state, dynamic_scale\n",
    "import optax\n",
    "\n",
    "def flatten(x):\n",
    "  return x.reshape(x.shape[0], -1)\n",
    "\n",
    "def trs(b):\n",
    "    return jnp.array(b.numpy())[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import sys\n",
    "import subprocess\n",
    "import wandb\n",
    "from time import sleep\n",
    "from functools import partial, reduce\n",
    "from itertools import product\n",
    "from simple_slurm import Slurm\n",
    "import random\n",
    "from typing import Any, Iterable\n",
    "import re\n",
    "from ast import literal_eval\n",
    "\n",
    "def get_cartesian_product(*args):\n",
    "    \"\"\" Cartesian product is the ordered set of all combinations of n sets \"\"\"\n",
    "    return list(product(*args))\n",
    "\n",
    "def zip_in_n_chunks(arg: Iterable[Any], n: int) -> zip:   \n",
    "    return zip(*([iter(arg)]*n))\n",
    "\n",
    "def flat_list(lst_of_lst):\n",
    "    return [lst for sublst in lst_of_lst for lst in sublst]\n",
    "\n",
    "def gen_alphanum(n: int = 7, test=False):\n",
    "    from string import ascii_lowercase, ascii_uppercase\n",
    "    random.seed(test if test else None)\n",
    "    numbers = ''.join([str(i) for i in range(10)])\n",
    "    characters = ascii_uppercase + ascii_lowercase + numbers\n",
    "    name = ''.join([random.choice(characters) for _ in range(n)])\n",
    "    return name\n",
    "\n",
    "def add_to_Path(path: Path, string: str | Path):\n",
    "        return Path(str(path) + str(string))\n",
    "\n",
    "def iterate_folder(folder: Path, iter_exp_dir):\n",
    "    if iter_exp_dir and folder.exists():\n",
    "        for i in range(100):\n",
    "            _folder = add_to_Path(folder, f'-{i}')\n",
    "            if not re.search(_folder.name, f'-[0-9]*'):\n",
    "                folder = _folder\n",
    "                break\n",
    "        else:\n",
    "            folder = add_to_Path(folder, f'-0')\n",
    "    return folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'can', 'max'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = set()\n",
    "x.update({'max'})\n",
    "x.update({'can'})\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout = run_cmds(['git log -l'], cwd=_i.project_dir)\n",
    "stdout.decode('utf-8').replace('\\n', ' ').split(' ')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [100], line 277\u001b[0m\n\u001b[1;32m    274\u001b[0m         stdout \u001b[39m=\u001b[39m run_cmds([\u001b[39m'\u001b[39m\u001b[39mgit log -l\u001b[39m\u001b[39m'\u001b[39m], cwd\u001b[39m=\u001b[39m_i\u001b[39m.\u001b[39mproject_dir)\n\u001b[1;32m    275\u001b[0m         \u001b[39mreturn\u001b[39;00m stdout\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 277\u001b[0m c \u001b[39m=\u001b[39m Pyfig()\n",
      "Cell \u001b[0;32mIn [100], line 221\u001b[0m, in \u001b[0;36mPyfig.__init__\u001b[0;34m(_i, args, cap)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(_i, args: \u001b[39mdict\u001b[39m\u001b[39m=\u001b[39m{}, cap\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    223\u001b[0m     [\u001b[39msetattr\u001b[39m(_i, k, v(_i)) \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m Pyfig\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, \u001b[39mtype\u001b[39m)]\n\u001b[1;32m    225\u001b[0m     wandb\u001b[39m.\u001b[39minit(\n\u001b[1;32m    226\u001b[0m         job_type\u001b[39m=\u001b[39m_i\u001b[39m.\u001b[39mwandb\u001b[39m.\u001b[39mjob_type,\n\u001b[1;32m    227\u001b[0m         entity\u001b[39m=\u001b[39m_i\u001b[39m.\u001b[39mwandb\u001b[39m.\u001b[39mentity,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[39mdir\u001b[39m\u001b[39m=\u001b[39m_i\u001b[39m.\u001b[39mexp_path,\n\u001b[1;32m    232\u001b[0m     )\n",
      "Cell \u001b[0;32mIn [100], line 8\u001b[0m, in \u001b[0;36mSub.__init__\u001b[0;34m(_i, parent)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m() \n\u001b[1;32m      6\u001b[0m _i\u001b[39m.\u001b[39mparent \u001b[39m=\u001b[39m parent\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m cls_to_dict(_i, Sub\u001b[39m.\u001b[39;49mignore_attr)\u001b[39m.\u001b[39mitems(): \u001b[39m# dictfromcls pulls values (property agnostic),\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     cls_v \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(_i\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, k)\n\u001b[1;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(cls_v, \u001b[39mproperty\u001b[39m): \u001b[39m# check the dict if property\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [100], line 68\u001b[0m, in \u001b[0;36mcls_to_dict\u001b[0;34m(cls, ignore)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcls_to_dict\u001b[39m(\u001b[39mcls\u001b[39m, ignore:\u001b[39mlist\u001b[39m)\u001b[39m-\u001b[39m\u001b[39m>\u001b[39m\u001b[39mdict\u001b[39m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: \u001b[39mgetattr\u001b[39m(\u001b[39mcls\u001b[39m, k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mdir\u001b[39m(\u001b[39mcls\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (k\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m k \u001b[39min\u001b[39;00m ignore)}\n",
      "Cell \u001b[0;32mIn [100], line 68\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcls_to_dict\u001b[39m(\u001b[39mcls\u001b[39m, ignore:\u001b[39mlist\u001b[39m)\u001b[39m-\u001b[39m\u001b[39m>\u001b[39m\u001b[39mdict\u001b[39m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: \u001b[39mgetattr\u001b[39;49m(\u001b[39mcls\u001b[39;49m, k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mdir\u001b[39m(\u001b[39mcls\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (k\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m k \u001b[39min\u001b[39;00m ignore)}\n",
      "Cell \u001b[0;32mIn [100], line 275\u001b[0m, in \u001b[0;36mPyfig.commit_id\u001b[0;34m(_i)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    273\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcommit_id\u001b[39m(_i,)\u001b[39m-\u001b[39m\u001b[39m>\u001b[39m\u001b[39mstr\u001b[39m:\n\u001b[1;32m    274\u001b[0m     stdout \u001b[39m=\u001b[39m run_cmds([\u001b[39m'\u001b[39m\u001b[39mgit log -l\u001b[39m\u001b[39m'\u001b[39m], cwd\u001b[39m=\u001b[39m_i\u001b[39m.\u001b[39mproject_dir)\n\u001b[0;32m--> 275\u001b[0m     \u001b[39mreturn\u001b[39;00m stdout\u001b[39m.\u001b[39;49mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "class Sub:\n",
    "    ignore_attr = ['parent','protected','dict','cmd']\n",
    "    \n",
    "    def __init__(_i, parent):\n",
    "        super().__init__() \n",
    "        _i.parent = parent\n",
    "        \n",
    "        for k,v in cls_to_dict(_i, Sub.ignore_attr).items(): # dictfromcls pulls values (property agnostic),\n",
    "            cls_v = getattr(_i.__class__, k)\n",
    "            if not isinstance(cls_v, property): # check the dict if property\n",
    "                setattr(_i, k, v)  # initialising subclasses\n",
    "\n",
    "    @property\n",
    "    def dict(_i,):\n",
    "        d = cls_to_dict(_i, Sub.ignore_attr)\n",
    "        for k,v in d.items():\n",
    "            if issubclass(type(v), Sub):\n",
    "                d[k] = cls_to_dict(v, Sub.ignore_attr)\n",
    "        return d\n",
    "\n",
    "def cls_to_dict(cls, ignore:list)->dict:\n",
    "    return {k: getattr(cls, k) for k in dir(cls) if not (k.startswith('_') or k in ignore)}\n",
    "\n",
    "def cmd_to_dict(cmd:str|list,ref:dict,_d={},delim:str=' --'):\n",
    "    \"\"\"\n",
    "    fmt: [--flag, arg, --true_flag, --flag, arg1]\n",
    "    # all flags double dash because of negative numbers duh \"\"\"\n",
    "    booleans = ['True', 'true', 't', 'False', 'false', 'f']\n",
    "    \n",
    "    cmd = ' '.join(cmd) if isinstance(cmd, list) else cmd\n",
    "    cmd = [x.lstrip().lstrip('--').rstrip() for x in cmd.split(delim)]\n",
    "    cmd = [x.split(' ', maxsplit=1) for x in cmd if ' ' in x]\n",
    "    [x.append('True') for x in cmd if len(x) == 1]\n",
    "    cmd = flat_list(cmd)\n",
    "    cmd = iter([x.strip() for x in cmd])\n",
    "\n",
    "    for k,v in zip(cmd, cmd):\n",
    "        if v in booleans: \n",
    "            v=booleans.index(v)<3  # 0-2 True 3-5 False\n",
    "        if k in ref:\n",
    "            _d[k] = type(ref[k])(v)\n",
    "        else:\n",
    "            try:\n",
    "                _d[k] = literal_eval(v)\n",
    "            except:\n",
    "                _d[k] = str(v)\n",
    "            print(f'Guessing type: {k} as {type(v)}')\n",
    "    return _d\n",
    "\n",
    "def update_cls_with_dict(cls: Any, d:dict):\n",
    "    cls_all = [Sub for Sub in cls.__dict__.values() if issubclass(type(v), Sub)]\n",
    "    cls_all.extend(cls)\n",
    "    n_remain = len(d)\n",
    "    for k,v in d.items():\n",
    "        for _cls_assign in cls_all:            \n",
    "            if not hasattr(_cls_assign, k):\n",
    "                continue\n",
    "            else:\n",
    "                if isinstance(cls.__class__.__dict__[k], property):\n",
    "                    print('Tried to assign property, consider your life choices')\n",
    "                    continue\n",
    "                v = type(cls.__dict__)(v)\n",
    "                setattr(_cls_assign, k, v)\n",
    "                n_remain -= 1\n",
    "    return n_remain\n",
    "\n",
    "def cls_to_dict(cls, ignore:list)->dict:\n",
    "    return {k: getattr(cls, k) for k in dir(cls) if not (k.startswith('_') or k in ignore)}\n",
    "\n",
    "def flat_dict(d:dict,items:list=[]):\n",
    "    for k,v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flat_dict(v).items(),items=items)\n",
    "        else:\n",
    "            items.append((k, v))\n",
    "    return dict(items)  \n",
    "\n",
    "def dict_to_wandb(d:dict,parent_key:str='',sep:str ='.',items:list=[])->dict:\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, dict): \n",
    "            items.extend(dict_to_wandb(v,new_key,items=items).items())\n",
    "        else:\n",
    "            if isinstance(v, Path):  v=str(v)\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "def count_gpu() -> int: # output = run_cmd('echo $CUDA_VISIBLE_DEVICES', cwd='.')\n",
    "    return sum(c.isdigit() for c in os.environ.get('CUDA_VISIBLE_DEVICES'))\n",
    "\n",
    "def run_cmds(cmd:str|list,cwd:str|Path=None,input_req:str=None,_out=[]):\n",
    "    for cmd_1 in (cmd if isinstance(cmd, list) else [cmd]): \n",
    "        _out += [subprocess.run(\n",
    "            [c.strip() for c in cmd_1.split(' ')],cwd=cwd, input=input_req, capture_output=True)]\n",
    "        sleep(0.1)\n",
    "    return _out\n",
    "\n",
    "def run_cmds_server(server:str,user:str,cmd:str|list,cwd=str|Path,_out=[]):\n",
    "    client = paramiko.SSHClient()\n",
    "    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # if not known host\n",
    "    client.connect(hostname=server, username=user)\n",
    "    client.exec_command(f'cd {cwd}')\n",
    "    with client as _r:\n",
    "        for cmd_1 in (cmd if isinstance(cmd, list) else [cmd]):\n",
    "            _out += [_r.exec_command(f'{cmd_1}')] # in, out, err\n",
    "            sleep(0.1)\n",
    "    return _out\n",
    "\n",
    "class Pyfig(Sub):\n",
    "\n",
    "    project:            str     = 'hwat'\n",
    "    project_dir:        Path    = Path().absolute()\n",
    "    n_device:           int     = property(lambda _i: _i.count_gpu())\n",
    "    \n",
    "    seed:               int     = 808017424         # grr\n",
    "\n",
    "    exp_name:           str     = 'junk'\n",
    "    run_path:           Path    = property(lambda _i: _i.project_dir / 'run.py')\n",
    "    data_dir:          Path     = Path().home() / 'data'\n",
    "    \n",
    "    half_precision = True\n",
    "    dtype:              str     = 'f32'\n",
    "    n_step:             int     = 1000\n",
    "\n",
    "    n_layer:            int     = 3\n",
    "    af:                 str     = 'tanh'    # activation function\n",
    "    \n",
    "    class data(Sub):\n",
    "        dataset = 'fashion_mnist'\n",
    "        b_size = 16\n",
    "        cache = False\n",
    "        image_size = 28\n",
    "        channels = 1\n",
    "\n",
    "    class model(Sub):\n",
    "        dim = 64\n",
    "        dim_mults = (1, 2, 4)\n",
    "\n",
    "    class opt(Sub):\n",
    "        optimizer = 'Adam'\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.99\n",
    "        eps = 1e-8\n",
    "        lr = 0.001\n",
    "        loss = 'l1'  # change this to loss table load? \n",
    "\n",
    "    class sweep(Sub):\n",
    "        method = 'random'\n",
    "        name = 'sweep'\n",
    "        metrics = dict(\n",
    "            goal = 'minimize',\n",
    "            name = 'validation_loss',\n",
    "        )\n",
    "        parameters = dict(\n",
    "            batch_size = {'values': [16, 32, 64]},\n",
    "            epoch = {'values': [5, 10, 15]},\n",
    "            lr = {'max': 0.1, 'min': 0.0001},\n",
    "        )\n",
    "        n_sweep = run_cap = reduce(\n",
    "            lambda i0,i1: i0*i1, [len(v['values']) for k,v in parameters.items() if 'values' in v])+1\n",
    "        sweep_id = ''\n",
    "\n",
    "    class wandb(Sub):\n",
    "        job_type:           str     = 'training'\n",
    "        entity:             str     = 'xmax1'\n",
    "\n",
    "    log_sample_step:    int     = 5\n",
    "    log_metric_step:    int     = 5\n",
    "    log_state_step:     int     = 10         # wandb entity\n",
    "    n_epoch:            int     = 20\n",
    "\n",
    "    class slurm(Sub):\n",
    "        output          = TMP/'o-%j.out'\n",
    "        error           = TMP/'e-%j.err'\n",
    "        mail_type       = 'FAIL'\n",
    "        partition       ='sm3090'\n",
    "        nodes           = 1                # n_node\n",
    "        ntasks          = 8                # n_cpu\n",
    "        cpus_per_task   = 1     \n",
    "        time            = '0-12:00:00'     # D-HH:MM:SS\n",
    "        gres            = 'gpu:RTX3090:1'\n",
    "        job_name        = property(lambda sub_i: sub_i.parent.exp_name)  # this does not call the instance it is in\n",
    "        sbatch          = property(lambda sub_i: f\"\"\" \n",
    "            module purge \n",
    "            source ~/.bashrc \n",
    "            module load GCC \n",
    "            module load CUDA/11.4.1 \n",
    "            module load cuDNN/8.2.2.26-CUDA-11.4.1 \n",
    "            conda activate {sub_i.parent.env} \n",
    "            export MKL_NUM_THREADS=1 \n",
    "            export NUMEXPR_NUM_THREADS=1 \n",
    "            export OMP_NUM_THREADS=1 \n",
    "            export OPENBLAS_NUM_THREADS=1\n",
    "            pwd\n",
    "            nvidia-smi\n",
    "            mv_cmd = f'mv {TMP}/o-$SLURM_JOB_ID.out {TMP}/e-$SLURM_JOB_ID.err $out_dir' \n",
    "        \"\"\")\n",
    "\n",
    "    exp_id:             str     = gen_alphanum(n=7)\n",
    "    \n",
    "    iter_exp_dir:       bool    = True\n",
    "    server_project_dir: Path    = property(lambda _i: _i.project_dir)\n",
    "    project_exp_dir:    Path    = property(lambda _i: _i.project_dir / 'exp')\n",
    "    project_cfg_dir:    Path    = property(lambda _i: _i.project_dir / 'cfg')\n",
    "    exp_path:           Path    = property(lambda _i: iterate_folder(_i.project_exp_dir / _i.exp_name, _i.iter_exp_dir) / _i.exp_id)\n",
    "\n",
    "    server:             str     = 'svol.fysik.dtu.dk'   # SERVER\n",
    "    user:               str     = 'amawi'     # SERVER\n",
    "    entity:             str     = 'xmax1'       # WANDB entity\n",
    "    git_remote:         str     = 'origin'      \n",
    "    git_branch:         str     = 'main'        \n",
    "    env:                str     = 'dex'            # CONDA ENV\n",
    "    commit_id:          str     = property(lambda _i: _i.get_commit_id())\n",
    "    \n",
    "    _cluster_state:     int     = -1\n",
    "\n",
    "    sys_arg: list = sys.argv[1:]\n",
    "\n",
    "    def __init__(_i, args: dict={}, cap=40) -> None:\n",
    "        \n",
    "        super().__init__(None)\n",
    "        \n",
    "        [setattr(_i, k, v(_i)) for k,v in Pyfig.__dict__.items() if isinstance(v, type)]\n",
    "\n",
    "        wandb.init(\n",
    "            job_type=_i.wandb.job_type,\n",
    "            entity=_i.wandb.entity,\n",
    "            project=_i.project,\n",
    "            config=dict_to_wandb(_i.dict),\n",
    "            settings=wandb.Settings(start_method='fork'),  # idk why this is an issue\n",
    "            dir=_i.exp_path,\n",
    "        )\n",
    "\n",
    "        update_cls_with_dict(args | cmd_to_dict(sys.argv[1:]))\n",
    "        \n",
    "        if _i._cluster_state > 0:\n",
    "            n_job_running = _i.run_cmds([f'squeue -u {_i.parent.user} -h -t pending,running -r | wc -l'])\n",
    "            if n_job_running > cap:\n",
    "                exit(f'There are {n_job_running} on the cluster cap is {cap}')\n",
    "            \n",
    "            _slurm = Slurm(**_i.slurm)\n",
    "\n",
    "            n_run, _i.clusterSubmit_state = _i.clusterSubmit_state, 0\n",
    "            exit()\n",
    "            for _ in range(_i._cluster_state):\n",
    "                _slurm.sbatch(_i.slurm.sbatch \n",
    "                + f'out_dir={(mkdir(_i.exp_path/\"out\"))} {_i.cmd} | tee $out_dir/py.out date \"+%B %V %T.%3N\" ')\n",
    "\n",
    "    @property\n",
    "    def cmd(_i,):\n",
    "        d = flat_dict(_i.dict)\n",
    "        return ' '.join([f' --{k}  {str(v)} ' for k,v in d.items()])\n",
    "\n",
    "    def clusterSubmit(_i, sweep=False, msg=None, cap=40):\n",
    "        msg = msg or _i.exp_id\n",
    "        _i._cluster_state *= -1 # init is -1\n",
    "        if _i._cluster_state > 0:\n",
    "            if sweep:\n",
    "                _i.sweep_id = wandb.sweep(\n",
    "                    env     = f'conda activate {_i.env};',\n",
    "                    sweep   = _i.sweep, \n",
    "                    program = _i.run_path,\n",
    "                    project = _i.project,\n",
    "                    name    = _i.exp_name,\n",
    "                    run_cap = _i.sweep.n_sweep\n",
    "                )\n",
    "                _i._cluster_state *= _i.sweep.n_sweep\n",
    "            \n",
    "            local_out = run_cmds(['git add .', f'git commit -m {msg}', 'git push'], cwd=_i.project_dir)\n",
    "            server_out = run_cmds_server(_i.server, _i.user, f'python -u {_i.run_path} ' +_i.cmd, cwd=_i.server_project_dir)\n",
    "\n",
    "    @property\n",
    "    def commit_id(_i,)->str:\n",
    "        stdout = run_cmds(['git log -l'], cwd=_i.project_dir)\n",
    "        return stdout.decode('utf-8').replace('\\n', ' ').split(' ')[1]\n",
    "            \n",
    "c = Pyfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git log -l\n",
      "['git', 'log', '-l']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CompletedProcess(args=['git', 'log', '-l'], returncode=128, stdout=b'', stderr=b\"fatal: Option 'l' requires a value\\n\")]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_cmds(cmd:str|list,cwd:str|Path=None,input_req:str=None,_out=[]):\n",
    "\n",
    "    for cmd_1 in (cmd if isinstance(cmd, list) else [cmd]): \n",
    "        print(cmd_1)\n",
    "        cmd_1 = [c.strip() for c in cmd_1.split(' ')]\n",
    "        print(cmd_1)\n",
    "        _out += [subprocess.run(\n",
    "            cmd_1,cwd=cwd, input=input_req, capture_output=True)]\n",
    "        sleep(0.1)\n",
    "    return _out\n",
    "\n",
    "stdout = run_cmds(['git log -l'], cwd='/home/amawi/projects/hwat')\n",
    "# stdout.decode('utf-8').replace('\\n', ' ').split(' ')[1]\n",
    "stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device: ', device)\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_fashion_loader(b_size, data_dir, mean=None, std=None, n_workers=4):\n",
    "    \n",
    "    data_tr = datasets.FashionMNIST(\n",
    "        root=data_dir, \n",
    "        download=True, \n",
    "        train=True,\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(28),\n",
    "            transforms.CenterCrop(28),\n",
    "            transforms.Normalize((mean), (std)),\n",
    "        ]),\n",
    "        target_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    loader_tr = DataLoader(\n",
    "        data_tr, \n",
    "        batch_size=b_size, \n",
    "        shuffle=True, \n",
    "        num_workers=n_workers,\n",
    "    )\n",
    "\n",
    "    data_test = datasets.FashionMNIST(\n",
    "        root=data_dir, \n",
    "        download=True, \n",
    "        train=False,\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(256),\n",
    "            transforms.Normalize((mean), (std)),\n",
    "        ]),\n",
    "        target_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    "    ])\n",
    "    )\n",
    "\n",
    "    loader_test = DataLoader(\n",
    "        data_test, \n",
    "        batch_size=b_size, \n",
    "        shuffle=True, \n",
    "        num_workers=n_workers,\n",
    "    )\n",
    "    return loader_tr, loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(c.seed)\n",
    "rng, subrng = jax.random.split(rng)\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    dynamic_scale: Optional[dynamic_scale_lib.DynamicScale] = None\n",
    "    params_ema: Any = None\n",
    "\n",
    "def create_train_state(rng, dynamic_scale=None):\n",
    "    \n",
    "    model = unet.Unet(\n",
    "        dim = c.model.dim, \n",
    "        out_dim =  c.data.channels,\n",
    "        dim_mults = c.model.dim_mults\n",
    "    )\n",
    "    input_dim = c.data.channels * 2 if c.ddpm.self_condition else c.data.channels\n",
    "    input_shape = (1, c.data.image_size, c.data.image_size, input_dim)\n",
    "\n",
    "    def initialized():\n",
    "        @jax.jit\n",
    "        def init(*args):\n",
    "            return model.init(*args)\n",
    "        variables = init(\n",
    "            {'params': rng}, \n",
    "            jnp.ones(input_shape, model.dtype), # x noisy image\n",
    "            jnp.ones(input_shape[:1], model.dtype) # t\n",
    "            )\n",
    "        return variables['params']\n",
    "    \n",
    "    tx = optax.adam(learning_rate=c.opt.lr , b1=c.opt.beta1, b2 = c.opt.beta2, eps=c.opt.eps)\n",
    "\n",
    "    params = initialized()\n",
    "    state = TrainState.create(\n",
    "        apply_fn=model.apply, \n",
    "        params=params,\n",
    "        params_ema=params,\n",
    "        tx=tx, \n",
    "        dynamic_scale=dynamic_scale\n",
    "    )\n",
    "\n",
    "    return state\n",
    "    \n",
    "state = create_train_state(rng)\n",
    "state = jax_utils.replicate(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ema_decay_schedule(update_after_step, inv_gamma, power, min_value, beta, **kw):\n",
    "\n",
    "    def ema_decay_schedule(step):\n",
    "        count = jnp.clip(step -  update_after_step - 1, a_min = 0.)\n",
    "        value = 1 - (1 + count /  inv_gamma) ** -  power \n",
    "        ema_rate = jnp.clip(value, a_min =  min_value, a_max =  beta)\n",
    "        return ema_rate\n",
    "\n",
    "    return ema_decay_schedule\n",
    "    \n",
    "ema_decay_fn = create_ema_decay_schedule(**c.ema.dict)\n",
    "\n",
    "def cosine_beta_schedule(timesteps):\n",
    "    \"\"\"Return cosine schedule \n",
    "    as proposed in https://arxiv.org/abs/2102.09672 \"\"\"\n",
    "    s=0.008\n",
    "    max_beta=0.999\n",
    "    ts = jnp.linspace(0, 1, timesteps + 1)\n",
    "    alphas_bar = jnp.cos((ts+s)/(1+s)*jnp.pi/2) ** 2\n",
    "    alphas_bar = alphas_bar/alphas_bar[0]\n",
    "    betas = 1 - (alphas_bar[1:] / alphas_bar[:-1])\n",
    "    return jnp.clip(betas, 0, max_beta)\n",
    "\n",
    "class ddpm_param:\n",
    "    timesteps = c.ddpm.timesteps\n",
    "    p2_loss_weight_gamma = c.ddpm.p2_loss_weight_gamma\n",
    "    p2_loss_weight_k = c.ddpm.p2_loss_weight_gamma\n",
    "\n",
    "    betas = cosine_beta_schedule(timesteps)\n",
    "\n",
    "    assert betas.shape == (timesteps,)\n",
    "    alphas = 1. - betas\n",
    "    alphas_bar = jnp.cumprod(alphas, axis=0)\n",
    "    sqrt_alphas_bar = jnp.sqrt(alphas_bar)\n",
    "    sqrt_1m_alphas_bar= jnp.sqrt(1. - alphas_bar)\n",
    "    \n",
    "    # calculate p2 reweighting\n",
    "    p2_loss_weight=  (p2_loss_weight_k + alphas_bar / (1 - alphas_bar)) ** -p2_loss_weight_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(logit, target):\n",
    "    return (logit - target)**2\n",
    "\n",
    "def l1_loss(logit, target): \n",
    "    return jnp.abs(logit - target)\n",
    "\n",
    "loss_table = dict(\n",
    "    l1_loss = l1_loss,\n",
    "    l2_loss = l2_loss\n",
    ")\n",
    "\n",
    "loss_fn = loss_table[c.opt.loss_fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_to_x0(noise, xt, batched_t, alphas_bar, sqrt_alphas_bar):\n",
    "    assert batched_t.shape[0] == xt.shape[0] == noise.shape[0] # make sure all has batch dimension\n",
    "    sqrt_alpha_bar = sqrt_alphas_bar[batched_t, None, None, None]\n",
    "    alpha_bar= alphas_bar[batched_t, None, None, None]\n",
    "    x0 = 1. / sqrt_alpha_bar * xt -  jnp.sqrt(1./alpha_bar-1) * noise\n",
    "    return x0\n",
    "\n",
    "def x0_to_noise(x0, xt, batched_t, alphas_bar, sqrt_alphas_bar):\n",
    "    assert batched_t.shape[0] == xt.shape[0] == x0.shape[0] # make sure all has batch dimension\n",
    "    sqrt_alpha_bar = sqrt_alphas_bar[batched_t, None, None, None]\n",
    "    alpha_bar= alphas_bar[batched_t, None, None, None]\n",
    "    noise = (1. / sqrt_alpha_bar * xt - x0) /jnp.sqrt(1./alpha_bar-1)\n",
    "    return noise\n",
    "\n",
    "def model_predict(state, x, x0, t, ddpm_param: ddpm_param, use_ema=True):\n",
    "    if use_ema:\n",
    "        variables = {'params': state.params_ema}\n",
    "    else:\n",
    "        variables = {'params': state.params}\n",
    "    \n",
    "    if c.ddpm.self_condition:\n",
    "        pred = state.apply_fn(variables, jnp.concatenate([x, x0],axis=-1), t)\n",
    "    else:\n",
    "        pred = state.apply_fn(variables, x, t)\n",
    "\n",
    "    if c.ddpm.is_pred_x0: # if the objective is is_pred_x0, pred == x0_pred\n",
    "        x0_pred = pred\n",
    "        noise_pred =  x0_to_noise(pred, x, t, ddpm_param.alphas_bar, ddpm_param.sqrt_alphas_bar)\n",
    "    else:\n",
    "        noise_pred = pred\n",
    "        x0_pred = noise_to_x0(pred, x, t, ddpm_param.alphas_bar, ddpm_param.sqrt_alphas_bar)\n",
    "    \n",
    "    return x0_pred, noise_pred\n",
    "\n",
    "def q_sample(x, t, noise, ddpm_param: ddpm_param):\n",
    "    sqrt_alpha_bar = ddpm_param.sqrt_alphas_bar[t, None, None, None]\n",
    "    sqrt_1m_alpha_bar = ddpm_param.sqrt_1m_alphas_bar[t,None,None,None]\n",
    "    x_t = sqrt_alpha_bar * x + sqrt_1m_alpha_bar * noise\n",
    "    return x_t\n",
    "\n",
    "def p_loss(rng, state: TrainState, b, loss_fn, ddpm_param: ddpm_param, pmap_axis='batch'):\n",
    "    rng, t_rng, noise_rng, condition_rng = jax.random.split(rng, 4)\n",
    "\n",
    "    assert b.dtype in [jnp.float32, jnp.float64]\n",
    "\n",
    "    B, H, W, C = b.shape\n",
    "    batched_t = jax.random.randint(t_rng, shape=(B,), dtype = jnp.int32, minval=0, maxval=len(ddpm_param.betas))\n",
    "    noise = jax.random.normal(noise_rng, b.shape)\n",
    "\n",
    "    target = b if c.ddpm.is_pred_x0 else noise\n",
    "    \n",
    "    x_t = q_sample(b, batched_t, noise, ddpm_param)\n",
    "\n",
    "    if c.ddpm.self_condition:\n",
    "        zeros = jnp.zeros_like(x_t)\n",
    "        \n",
    "        def estimate_x0(_): # self-conditioning \n",
    "            x0, _ = model_predict(state, x_t, zeros, batched_t, ddpm_param, use_ema=False)\n",
    "            return x0\n",
    "        \n",
    "        x0 = jax.lax.cond(\n",
    "            jax.random.uniform(condition_rng, shape=(1,))[0] < 0.5,\n",
    "            estimate_x0,\n",
    "            lambda _ :zeros,\n",
    "            None,\n",
    "        )\n",
    "        \n",
    "        x_t = jnp.concatenate([x_t, x0], axis=-1)\n",
    "\n",
    "    def compute_loss(params):\n",
    "        pred = state.apply_fn({'params':params}, x_t, batched_t)\n",
    "        loss = loss_fn(flatten(pred),flatten(target))\n",
    "        loss = jnp.mean(loss, axis=1)\n",
    "        assert loss.shape == (B,)\n",
    "        loss = loss * ddpm_param.p2_loss_weight[batched_t]\n",
    "        return loss.mean()\n",
    "    \n",
    "    dynamic_scale = state.dynamic_scale\n",
    "\n",
    "    if dynamic_scale: # dynamic loss takes care of averaging gradients across replicas\n",
    "        grad_fn = dynamic_scale.value_and_grad(compute_loss, axis_name=pmap_axis)\n",
    "        dynamic_scale, is_fin, loss, grads = grad_fn(state.params)\n",
    "    else: #  Re-use same axis_name as in the call to `pmap(...train_step,axis=...)` in the train function\n",
    "        grad_fn = jax.value_and_grad(compute_loss)\n",
    "        loss, grads = grad_fn(state.params)\n",
    "        grads = jax.lax.pmean(grads, axis_name=pmap_axis)\n",
    "    \n",
    "    loss = jax.lax.pmean(loss, axis_name=pmap_axis)\n",
    "    loss_ema = jax.lax.pmean(compute_loss(state.params_ema), axis_name=pmap_axis)\n",
    "\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'loss_ema': loss_ema,\n",
    "    }\n",
    "\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    if dynamic_scale: \n",
    "        # if is_fin == False the gradients contain Inf/NaNs and optimizer state and # params should be restored (= skip this step).\n",
    "        new_state=new_state.replace(\n",
    "            opt_state=jax.tree_map(\n",
    "                functools.partial(jnp.where, is_fin),\n",
    "                new_state.opt_state,\n",
    "                state.opt_state\n",
    "            ),\n",
    "            params=jax.tree_map(\n",
    "                functools.partial(jnp.where, is_fin),\n",
    "                new_state.params,\n",
    "                state.params\n",
    "            ),\n",
    "            dynamic_scale=dynamic_scale\n",
    "        )\n",
    "        metrics['scale'] = dynamic_scale.scale\n",
    "\n",
    "    return new_state, metrics\n",
    "\n",
    "train_step = functools.partial(\n",
    "    p_loss,\n",
    "    ddpm_param  = ddpm_param, \n",
    "    loss_fn     = loss_fn,\n",
    "    pmap_axis   = 'batch'\n",
    ")\n",
    "\n",
    "p_train_step = jax.pmap(train_step, axis_name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior_mean_variance(img, t, x0, v, ddpm):\n",
    "\n",
    "    beta = ddpm_param.betas[t,None,None,None] # only needed when t > 0\n",
    "    alpha = ddpm_param.alphas[t,None,None,None]\n",
    "    alpha_bar = ddpm_param.alphas_bar[t,None,None,None]\n",
    "    alpha_bar_last = ddpm_param.alphas_bar[t-1,None,None,None]\n",
    "    sqrt_alpha_bar_last = ddpm_param.sqrt_alphas_bar[t-1,None,None,None]\n",
    "\n",
    "    coef_x0 = beta * sqrt_alpha_bar_last / (1. - alpha_bar)\n",
    "    coef_xt = (1. - alpha_bar_last) * jnp.sqrt(alpha) / ( 1- alpha_bar)        \n",
    "    posterior_mean = coef_x0 * x0 + coef_xt * img\n",
    "        \n",
    "    posterior_variance = beta * (1 - alpha_bar_last) / (1. - alpha_bar)\n",
    "    posterior_log_variance = jnp.log(jnp.clip(posterior_variance, a_min = 1e-20))\n",
    "\n",
    "    return posterior_mean, posterior_log_variance\n",
    "\n",
    "\n",
    "def ddpm_sample_step(state, rng, x, t, x0_last, ddpm_param: ddpm_param):\n",
    " \n",
    "    batched_t = jnp.ones((x.shape[0],), dtype=jnp.int32) * t\n",
    "    \n",
    "    if c.ddpm.self_condition:\n",
    "        x0, v = model_predict(state, x, x0_last, batched_t, ddpm_param, use_ema=True) \n",
    "    else:\n",
    "        x0, v = model_predict(state, x, None, batched_t, ddpm_param, use_ema=True)\n",
    "    \n",
    "    x0 = jnp.clip(x0,-1.,1.) # make sure x0 between [-1,1]\n",
    "    posterior_mean, posterior_log_variance = get_posterior_mean_variance(x, t, x0, v, ddpm_param)\n",
    "    x = posterior_mean + jnp.exp(0.5 *  posterior_log_variance) * jax.random.normal(rng, x.shape) \n",
    "    return x, x0\n",
    "\n",
    "sample_step = functools.partial(\n",
    "    ddpm_sample_step, \n",
    "    ddpm_param=ddpm_param, \n",
    "    self_condition=c.ddpm.self_condition, \n",
    "    is_pred_x0=c.ddpm.is_pred_x0\n",
    ")\n",
    "\n",
    "p_sample_step = jax.pmap(sample_step, axis_name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_params_to_ema(state):\n",
    "   state = state.replace(params_ema = state.params)\n",
    "   return state\n",
    "\n",
    "def apply_ema_decay(state, ema_decay):\n",
    "    params_ema = jax.tree_map(lambda p_ema, p: p_ema * ema_decay + p * (1. - ema_decay), state.params_ema, state.params)\n",
    "    state = state.replace(params_ema = params_ema)\n",
    "    return state\n",
    "\n",
    "p_apply_ema = jax.pmap(apply_ema_decay, in_axes=(0, None), axis_name='batch')\n",
    "p_copy_params_to_ema = jax.pmap(copy_params_to_ema, axis_name='batch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "def to_wandb_config(d, parent_key: str = '', sep: str ='.'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(to_wandb_config(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            if isinstance(v, Path):\n",
    "                v = str(v)\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "wandb.init(\n",
    "    job_type=c.wandb.job_type,\n",
    "    entity=c.wandb.entity,\n",
    "    project=c.project,\n",
    "    config=to_wandb_config(c.dict),\n",
    "    settings=wandb.Settings(start_method='fork'),  # idk why this is an issue\n",
    "    dir=c.exp_path,\n",
    ")\n",
    "\n",
    "wandb.define_metric(\"*\", step_metric=\"train/step\")\n",
    "\n",
    "# # Display a project workspace\n",
    "# %wandb USERNAME/PROJECT\n",
    "# # Display a single run\n",
    "# %wandb USERNAME/PROJECT/runs/RUN_ID\n",
    "# # Display a sweep\n",
    "# %wandb USERNAME/PROJECT/sweeps/SWEEP_ID\n",
    "# # Display a report\n",
    "# %wandb USERNAME/PROJECT/reports/REPORT_ID\n",
    "# # Specify the height of embedded iframe\n",
    "# %wandb USERNAME/PROJECT -h 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_stats():\n",
    "    def normalize_to_neg_one_to_one(img):\n",
    "        return img * 2 - 1  \n",
    "    data_tr = datasets.FashionMNIST(\n",
    "        root=c.data_dir, \n",
    "        download=True, \n",
    "        train=True,\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(28),\n",
    "            transforms.CenterCrop(28),\n",
    "        ]),\n",
    "        target_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    "    ])\n",
    "    )\n",
    "    imgs = torch.stack([img_t for img_t, _ in data_tr], dim=3)\n",
    "    mean = imgs.view(1,-1).mean(dim=1) \n",
    "    std = imgs.view(1,-1).std(dim=1)\n",
    "    print('Data mean: ', mean, 'Data std: ', std)\n",
    "    return mean, std\n",
    "mean, std = get_data_stats()\n",
    "loader_tr, loader_test = get_fashion_loader(c.data.b_size, c.data_dir, mean=mean, std=std)  # is not an iterator or list\n",
    "img, l = next(iter(loader_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Image.fromarray(np.uint8((img[0, 0]*std+mean).cpu().numpy()*255))\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('1', size=(cols*w, rows*h))\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "\n",
    "    return grid\n",
    "std = float(std.cpu().numpy())\n",
    "mean = float(mean.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = []\n",
    "\n",
    "for ep in range(c.n_epoch):\n",
    "    for step, (b, target) in enumerate(loader_tr):\n",
    "        b = jnp.squeeze(jnp.expand_dims(jnp.array(b.numpy()), axis=(0,-1)), axis=2) # p, B, H, W, C\n",
    "        rng, *train_step_rng = jax.random.split(rng, num=jax.local_device_count() + 1)\n",
    "        train_step_rng = jnp.array(train_step_rng)\n",
    "        \n",
    "        state, metrics = p_train_step(train_step_rng, state, b)\n",
    "\n",
    "        if step <= c.ema.update_after_step:\n",
    "            state = p_copy_params_to_ema(state)\n",
    "\n",
    "        elif step % c.ema.update_every == 0:\n",
    "            ema_decay = ema_decay_fn(step)\n",
    "            state =  p_apply_ema(state, ema_decay)\n",
    "\n",
    "        if step % c.log_metric_step == 0:\n",
    "\n",
    "            train_metrics.append(metrics)\n",
    "            train_metrics = common_utils.get_metrics(train_metrics)\n",
    "            \n",
    "            summary = {\n",
    "                f'train/{k}': v\n",
    "                for k, v in jax.tree_map(lambda x: x.mean(), train_metrics).items()\n",
    "            }\n",
    "\n",
    "            train_metrics = []\n",
    "            b = ((np.array(b)*std+mean)*255).astype(np.uint8)\n",
    "            imgs = [wandb.Image(Image.fromarray(b[0, i].reshape(28, 28))) for i in range(9)]\n",
    "            \n",
    "            wandb.log({\n",
    "                    \"train/step\": step, \n",
    "                    'train/sample': imgs,\n",
    "                    **summary\n",
    "            })\n",
    " \n",
    "    print('Epoch: ', ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.fromarray(b[0, 0].reshape(28, 28, 1))\n",
    "b[0, 0].max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('dex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b4edb4a58a0461d84e636e9142615dc364f099c3851533546c18fbe9e367308"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
