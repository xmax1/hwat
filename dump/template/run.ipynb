{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "path exists, leaving alone\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "def mkdir(path: Path) -> Path:\n",
    "    path = Path(path)\n",
    "    if path.suffix != '':\n",
    "        path = path.parent\n",
    "    if path.exists():\n",
    "        print('path exists, leaving alone')\n",
    "    else:\n",
    "        path.mkdir(parents=True)\n",
    "    return path\n",
    "\n",
    "this_dir = Path('').parent\n",
    "TMP = mkdir('./tmp/out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from PIL import Image\n",
    "\n",
    "from flax import jax_utils\n",
    "from flax.training import common_utils, train_state, dynamic_scale\n",
    "import optax\n",
    "\n",
    "def flatten(x):\n",
    "  return x.reshape(x.shape[0], -1)\n",
    "\n",
    "def trs(b):\n",
    "    return jnp.array(b.numpy())[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import sys\n",
    "import subprocess\n",
    "import wandb\n",
    "from time import sleep\n",
    "from functools import partial, reduce\n",
    "from itertools import product\n",
    "from simple_slurm import Slurm\n",
    "import random\n",
    "from typing import Any, Iterable\n",
    "import re\n",
    "from ast import literal_eval\n",
    "\n",
    "def get_cartesian_product(*args):\n",
    "    \"\"\" Cartesian product is the ordered set of all combinations of n sets \"\"\"\n",
    "    return list(product(*args))\n",
    "\n",
    "def zip_in_n_chunks(arg: Iterable[Any], n: int) -> zip:   \n",
    "    return zip(*([iter(arg)]*n))\n",
    "\n",
    "def flat_list(lst_of_lst):\n",
    "    return [lst for sublst in lst_of_lst for lst in sublst]\n",
    "\n",
    "def gen_alphanum(n: int = 7, test=False):\n",
    "    from string import ascii_lowercase, ascii_uppercase\n",
    "    random.seed(test if test else None)\n",
    "    numbers = ''.join([str(i) for i in range(10)])\n",
    "    characters = ascii_uppercase + ascii_lowercase + numbers\n",
    "    name = ''.join([random.choice(characters) for _ in range(n)])\n",
    "    return name\n",
    "\n",
    "def add_to_Path(path: Path, string: str | Path):\n",
    "        return Path(str(path) + str(string))\n",
    "\n",
    "def iterate_folder(folder: Path, iter_exp_dir):\n",
    "    if iter_exp_dir and folder.exists():\n",
    "        for i in range(100):\n",
    "            _folder = add_to_Path(folder, f'-{i}')\n",
    "            if not re.search(_folder.name, f'-[0-9]*'):\n",
    "                folder = _folder\n",
    "                break\n",
    "        else:\n",
    "            folder = add_to_Path(folder, f'-0')\n",
    "    return folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'af': 'tanh',\n",
      " 'commit_id': 'b78c668',\n",
      " 'data': {'b_size': 16,\n",
      "          'cache': False,\n",
      "          'channels': 1,\n",
      "          'dataset': 'fashion_mnist',\n",
      "          'image_size': 28},\n",
      " 'data_dir': PosixPath('/home/amawi/projects/data'),\n",
      " 'dtype': 'f32',\n",
      " 'entity': 'xmax1',\n",
      " 'env': 'dex',\n",
      " 'exp_id': 'oWJHydk',\n",
      " 'exp_name': 'junk',\n",
      " 'exp_path': PosixPath('/home/amawi/projects/hwat/exp/junk/oWJHydk'),\n",
      " 'git_branch': 'main',\n",
      " 'git_remote': 'origin',\n",
      " 'half_precision': True,\n",
      " 'iter_exp_dir': True,\n",
      " 'log_metric_step': 5,\n",
      " 'log_sample_step': 5,\n",
      " 'log_state_step': 10,\n",
      " 'model': {'dim': 64, 'dim_mults': (1, 2, 4)},\n",
      " 'n_device': 1,\n",
      " 'n_epoch': 20,\n",
      " 'n_layer': 3,\n",
      " 'n_step': 1000,\n",
      " 'opt': {'beta1': 0.9,\n",
      "         'beta2': 0.99,\n",
      "         'eps': 1e-08,\n",
      "         'loss': 'l1',\n",
      "         'lr': 0.001,\n",
      "         'optimizer': 'Adam'},\n",
      " 'project': 'hwat',\n",
      " 'project_cfg_dir': PosixPath('/home/amawi/projects/hwat/cfg'),\n",
      " 'project_exp_dir': PosixPath('/home/amawi/projects/hwat/exp'),\n",
      " 'project_path': PosixPath('/home/amawi/projects/hwat'),\n",
      " 'project_root': PosixPath('/home/amawi/projects'),\n",
      " 'run_path': PosixPath('/home/amawi/projects/hwat/run.py'),\n",
      " 'seed': 808017424,\n",
      " 'server': 'svol.fysik.dtu.dk',\n",
      " 'server_project_path': PosixPath('/home/amawi/projects/hwat'),\n",
      " 'slurm': {'cpus_per_task': 1,\n",
      "           'error': PosixPath('tmp/out/e-%j.err'),\n",
      "           'gres': 'gpu:RTX3090:1',\n",
      "           'job_name': 'junk',\n",
      "           'mail_type': 'FAIL',\n",
      "           'nodes': 1,\n",
      "           'ntasks': 8,\n",
      "           'output': PosixPath('tmp/out/o-%j.out'),\n",
      "           'partition': 'sm3090',\n",
      "           'sbatch': ' \\n'\n",
      "                     '            module purge \\n'\n",
      "                     '            source ~/.bashrc \\n'\n",
      "                     '            module load GCC \\n'\n",
      "                     '            module load CUDA/11.4.1 \\n'\n",
      "                     '            module load cuDNN/8.2.2.26-CUDA-11.4.1 \\n'\n",
      "                     '            conda activate dex \\n'\n",
      "                     '            export MKL_NUM_THREADS=1 \\n'\n",
      "                     '            export NUMEXPR_NUM_THREADS=1 \\n'\n",
      "                     '            export OMP_NUM_THREADS=1 \\n'\n",
      "                     '            export OPENBLAS_NUM_THREADS=1\\n'\n",
      "                     '            pwd\\n'\n",
      "                     '            nvidia-smi\\n'\n",
      "                     \"            mv_cmd = f'mv tmp/out/o-$SLURM_JOB_ID.out \"\n",
      "                     \"tmp/out/e-$SLURM_JOB_ID.err $out_dir' \\n\"\n",
      "                     '    ',\n",
      "           'time': '0-12:00:00'},\n",
      " 'submit': <bound method Pyfig.submit of <__main__.Pyfig object at 0x7fc1704fe3e0>>,\n",
      " 'sweep': {'method': 'random',\n",
      "           'metrics': {'goal': 'minimize', 'name': 'validation_loss'},\n",
      "           'n_sweep': 10,\n",
      "           'name': 'sweep',\n",
      "           'parameters': {'batch_size': {'values': [16, 32, 64]},\n",
      "                          'epoch': {'values': [5, 10, 15]},\n",
      "                          'lr': {'max': 0.1, 'min': 0.0001}},\n",
      "           'run_cap': 10,\n",
      "           'sweep_id': ''},\n",
      " 'user': 'amawi',\n",
      " 'wandb': {'entity': 'xmax1', 'job_type': 'training'}}\n"
     ]
    }
   ],
   "source": [
    "class Sub:\n",
    "    \n",
    "    def __init__(_i, parent=None):\n",
    "        _i.parent = parent\n",
    "        _i.__safe_init__()\n",
    "\n",
    "    def __safe_init__(_i):\n",
    "        cls_d = dict(_i.__class__.__dict__)\n",
    "        sub_cls = {k:v for k,v in cls_d.items() if isinstance(v, type)}\n",
    "        [cls_d.pop(k) for k in sub_cls.keys()]\n",
    "        for k,v in cls_d.items():\n",
    "            # print(k, type(v), callable(v))\n",
    "            if k.startswith('__') or k in Pyfig._ignore_attr:\n",
    "                continue # this was around because 'dicts are unhashable types'\n",
    "            if callable(v) or isinstance(v, property):\n",
    "                continue\n",
    "            setattr(_i, k, v)\n",
    "        [setattr(_i, k, v(parent=_i)) for k,v in sub_cls.items()]\n",
    "\n",
    "    @property\n",
    "    def dict(_i,):\n",
    "        d = cls_to_dict(_i, Pyfig._ignore_attr)\n",
    "        for k,v in d.items():\n",
    "            if issubclass(type(v), Sub):\n",
    "                d[k] = cls_to_dict(v, Pyfig._ignore_attr)\n",
    "        return d\n",
    "\n",
    "def cls_to_dict(cls, ignore:list)->dict:\n",
    "    d = {}\n",
    "    for k,v in cls.__dict__.items():\n",
    "        if k.startswith('_') or k in ignore:\n",
    "            continue\n",
    "        if callable(v):\n",
    "            continue\n",
    "        if issubclass(type(v), Sub):\n",
    "            d[k] = cls_to_dict(v, ignore)\n",
    "            continue\n",
    "        d[k] = getattr(cls,k)\n",
    "    return d\n",
    "\n",
    "def cmd_to_dict(cmd:str|list,ref:dict,_d={},delim:str=' --'):\n",
    "    \"\"\"\n",
    "    fmt: [--flag, arg, --true_flag, --flag, arg1]\n",
    "    # all flags double dash because of negative numbers duh \"\"\"\n",
    "    booleans = ['True', 'true', 't', 'False', 'false', 'f']\n",
    "    \n",
    "    cmd = ' '.join(cmd) if isinstance(cmd, list) else cmd\n",
    "    cmd = [x.lstrip().lstrip('--').rstrip() for x in cmd.split(delim)]\n",
    "    cmd = [x.split(' ', maxsplit=1) for x in cmd if ' ' in x]\n",
    "    [x.append('True') for x in cmd if len(x) == 1]\n",
    "    cmd = flat_list(cmd)\n",
    "    cmd = iter([x.strip() for x in cmd])\n",
    "\n",
    "    for k,v in zip(cmd, cmd):\n",
    "        if v in booleans: \n",
    "            v=booleans.index(v)<3  # 0-2 True 3-5 False\n",
    "        if k in ref:\n",
    "            _d[k] = type(ref[k])(v)\n",
    "        else:\n",
    "            try:\n",
    "                _d[k] = literal_eval(v)\n",
    "            except:\n",
    "                _d[k] = str(v)\n",
    "            print(f'Guessing type: {k} as {type(v)}')\n",
    "    return _d\n",
    "\n",
    "def update_cls_with_dict(cls: Any, d:dict):\n",
    "    cls_all = [v for v in cls.__dict__.values() if issubclass(type(v), Sub)]\n",
    "    cls_all.extend([cls])\n",
    "    n_remain = len(d)\n",
    "    for k,v in d.items():\n",
    "        for _cls_assign in cls_all:            \n",
    "            if not hasattr(_cls_assign, k):\n",
    "                continue\n",
    "            else:\n",
    "                if isinstance(cls.__class__.__dict__[k], property):\n",
    "                    print('Tried to assign property, consider your life choices')\n",
    "                    continue\n",
    "                v = type(cls.__dict__)(v)\n",
    "                setattr(_cls_assign, k, v)\n",
    "                n_remain -= 1\n",
    "    return n_remain\n",
    "\n",
    "def cls_to_dict(cls, ignore:list)->dict:\n",
    "    return {k: getattr(cls, k) for k in dir(cls) if not (k.startswith('_') or k in ignore)}\n",
    "\n",
    "def flat_dict(d:dict,items:list=[]):\n",
    "    for k,v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flat_dict(v, items=items).items())\n",
    "        else:\n",
    "            items.append((k, v))\n",
    "    return dict(items)  \n",
    "\n",
    "def dict_to_wandb(d:dict,parent_key:str='',sep:str ='.',items:list=[])->dict:\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, dict): \n",
    "            items.extend(dict_to_wandb(v,new_key,items=items).items())\n",
    "        else:\n",
    "            if isinstance(v, Path):  v=str(v)\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "def count_gpu() -> int: # output = run_cmd('echo $CUDA_VISIBLE_DEVICES', cwd='.')\n",
    "    return sum(c.isdigit() for c in os.environ.get('CUDA_VISIBLE_DEVICES'))\n",
    "\n",
    "def run_cmds(cmd:str|list,cwd:str|Path=None,input_req:str=None):\n",
    "    _out = []\n",
    "    for cmd_1 in (cmd if isinstance(cmd, list) else [cmd]): \n",
    "        cmd_1 = [c.strip() for c in cmd_1.split(' ')]\n",
    "        _out += [subprocess.run(\n",
    "            cmd_1,cwd=cwd,input=input_req, capture_output=True)]\n",
    "        sleep(0.1)\n",
    "    return _out\n",
    "\n",
    "def run_cmds_server(server:str,user:str,cmd:str|list,cwd=str|Path):\n",
    "    _out = []\n",
    "    client = paramiko.SSHClient()\n",
    "    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # if not known host\n",
    "    client.connect(hostname=server, username=user)\n",
    "    client.exec_command(f'cd {cwd}')\n",
    "    with client as _r:\n",
    "        for cmd_1 in (cmd if isinstance(cmd, list) else [cmd]):\n",
    "            _out += [_r.exec_command(f'{cmd_1}')] # in, out, err\n",
    "            sleep(0.1)\n",
    "    return _out\n",
    "\n",
    "class Pyfig(Sub):\n",
    "\n",
    "    seed:               int     = 808017424 # grr\n",
    "    project_root:       str     = Path().home()/'projects'\n",
    "\n",
    "    project:            str     = 'hwat'\n",
    "    project_path:       Path    = property(lambda _: _.project_root / _.project)\n",
    "    server_project_path:Path    = property(lambda _: _.project_path)\n",
    "    n_device:           int     = property(lambda _: count_gpu())\n",
    "\n",
    "    exp_name:           str     = 'junk'\n",
    "    run_path:           Path    = property(lambda _: _.project_path / 'run.py')\n",
    "    data_dir:           Path    = project_root / 'data'\n",
    "    \n",
    "    half_precision:     bool    = True\n",
    "    dtype:              str     = 'f32'\n",
    "    n_step:             int     = 1000\n",
    "\n",
    "    af:                 str     = 'tanh' # activation function\n",
    "    n_layer:            int     = 3\n",
    "    \n",
    "    class data(Sub):\n",
    "        dataset     = 'fashion_mnist'\n",
    "        b_size      = 16\n",
    "        cache       = False\n",
    "        image_size  = 28\n",
    "        channels    = 1\n",
    "\n",
    "    class model(Sub):\n",
    "        dim         = 64\n",
    "        dim_mults   = (1, 2, 4)\n",
    "\n",
    "    class opt(Sub):\n",
    "        optimizer   = 'Adam'\n",
    "        beta1       = 0.9\n",
    "        beta2       = 0.99\n",
    "        eps         = 1e-8\n",
    "        lr          = 0.001\n",
    "        loss        = 'l1'  # change this to loss table load? \n",
    "\n",
    "    class sweep(Sub):\n",
    "        method      = 'random'\n",
    "        name        = 'sweep'\n",
    "        metrics = dict(\n",
    "            goal    = 'minimize',\n",
    "            name    = 'validation_loss',\n",
    "        )\n",
    "        parameters = dict(\n",
    "            batch_size  = {'values' : [16, 32, 64]},\n",
    "            epoch       = {'values' : [5, 10, 15]},\n",
    "            lr          = {'max'    : 0.1, 'min': 0.0001},\n",
    "        )\n",
    "        n_sweep = run_cap = reduce(\n",
    "            lambda i,j:i*j,[len(v['values']) for k,v in parameters.items() if 'values' in v])+1\n",
    "        sweep_id = ''\n",
    "\n",
    "    class wandb(Sub):\n",
    "        job_type:       str     = 'training'\n",
    "        entity:         str     = 'xmax1'\n",
    "\n",
    "    log_sample_step:    int     = 5\n",
    "    log_metric_step:    int     = 5\n",
    "    log_state_step:     int     = 10         # wandb entity\n",
    "    n_epoch:            int     = 20\n",
    "\n",
    "    class slurm(Sub):\n",
    "        output          = TMP/'o-%j.out'\n",
    "        error           = TMP/'e-%j.err'\n",
    "        mail_type       = 'FAIL'\n",
    "        partition       ='sm3090'\n",
    "        nodes           = 1                # n_node\n",
    "        ntasks          = 8                # n_cpu\n",
    "        cpus_per_task   = 1     \n",
    "        time            = '0-12:00:00'     # D-HH:MM:SS\n",
    "        gres            = 'gpu:RTX3090:1'\n",
    "        job_name        = property(lambda _: _.parent.exp_name)  # this does not call the instance it is in\n",
    "        sbatch          = property(lambda _: f\"\"\" \n",
    "            module purge \n",
    "            source ~/.bashrc \n",
    "            module load GCC \n",
    "            module load CUDA/11.4.1 \n",
    "            module load cuDNN/8.2.2.26-CUDA-11.4.1 \n",
    "            conda activate {_.parent.env} \n",
    "            export MKL_NUM_THREADS=1 \n",
    "            export NUMEXPR_NUM_THREADS=1 \n",
    "            export OMP_NUM_THREADS=1 \n",
    "            export OPENBLAS_NUM_THREADS=1\n",
    "            pwd\n",
    "            nvidia-smi\n",
    "            mv_cmd = f'mv {TMP}/o-$SLURM_JOB_ID.out {TMP}/e-$SLURM_JOB_ID.err $out_dir' \n",
    "    \"\"\")\n",
    "\n",
    "    exp_id:             str     = gen_alphanum(n=7)\n",
    "    \n",
    "    iter_exp_dir:       bool    = True\n",
    "    project_exp_dir:    Path    = property(lambda _: _.project_path / 'exp')\n",
    "    project_cfg_dir:    Path    = property(lambda _: _.project_path / 'cfg')\n",
    "    exp_path:           Path    = property(lambda _: iterate_folder(_.project_exp_dir/_.exp_name,_.iter_exp_dir)/_.exp_id)\n",
    "\n",
    "    server:             str     = 'svol.fysik.dtu.dk'   # SERVER\n",
    "    user:               str     = 'amawi'     # SERVER\n",
    "    entity:             str     = 'xmax1'       # WANDB entity\n",
    "    git_remote:         str     = 'origin'      \n",
    "    git_branch:         str     = 'main'        \n",
    "    env:                str     = 'dex'            # CONDA ENV\n",
    "    commit_id:          str     = property(lambda _: _.get_commit_id())\n",
    "    \n",
    "    _sys_arg: list = sys.argv[1:]\n",
    "    _submit_state:     int     = -1\n",
    "    _ignore_attr = ['parent','protected','dict','cmd']\n",
    "\n",
    "    def __init__(_i,args:dict={},cap=40,wandb_mode='online',notebook=False):\n",
    "        super().__init__()\n",
    "        _i.__safe_init__()\n",
    "\n",
    "        update_cls_with_dict(_i,args)\n",
    "        if not notebook:\n",
    "            update_cls_with_dict(cmd_to_dict(sys.argv[1:],_i.dict))\n",
    "\n",
    "        wandb.init(\n",
    "            job_type    = _i.wandb.job_type,\n",
    "            entity      = _i.wandb.entity,\n",
    "            project     = _i.project,\n",
    "            dir         = _i.exp_path,\n",
    "            config      = dict_to_wandb(_i.dict),\n",
    "            mode        = wandb_mode,\n",
    "            settings=wandb.Settings(start_method='fork'), # idk y this is issue, don't change\n",
    "        )\n",
    "\n",
    "        if _i._submit_state > 0:\n",
    "            n_job_running = run_cmds([f'squeue -u {_i.user} -h -t pending,running -r | wc -l'])\n",
    "            if n_job_running > cap:\n",
    "                exit(f'There are {n_job_running} on the submit cap is {cap}')\n",
    "\n",
    "            _slurm = Slurm(**_i.slurm.dict)\n",
    "\n",
    "            n_run, _i._submit_state = _i._submit_state, 0            \n",
    "            for _ in range(n_run):\n",
    "                _slurm.sbatch(_i.slurm.sbatch \n",
    "                + f'out_dir={(mkdir(_i.exp_path/\"out\"))} {_i.cmd} | tee $out_dir/py.out date \"+%B %V %T.%3N\" ')\n",
    "\n",
    "    @property\n",
    "    def cmd(_i,):\n",
    "        d = flat_dict(_i.dict)\n",
    "        return ' '.join([f' --{k}  {str(v)} ' for k,v in d.items()])\n",
    "\n",
    "    @property\n",
    "    def commit_id(_i,)->str:\n",
    "        process = run_cmds(['git log --pretty=format:%h -n 1'], cwd=_i.project_path)[0]\n",
    "        return process.stdout.decode('utf-8') \n",
    "\n",
    "    def submit(_i, sweep=False, commit_msg=None, cap=40):\n",
    "        commit_msg = commit_msg or _i.exp_id\n",
    "        _i._submit_state *= -1\n",
    "        if _i._submit_state > 0:\n",
    "            if sweep:\n",
    "                _i.sweep_id = wandb.sweep(\n",
    "                    env     = f'conda activate {_i.env};',\n",
    "                    sweep   = _i.sweep.dict, \n",
    "                    program = _i.run_path,\n",
    "                    project = _i.project,\n",
    "                    name    = _i.exp_name,\n",
    "                    run_cap = _i.sweep.n_sweep\n",
    "                )\n",
    "                _i._submit_state *= _i.sweep.n_sweep\n",
    "            local_out = run_cmds(['git add .', f'git commit -m {commit_msg}', 'git push'], cwd=_i.project_path)\n",
    "            server_out = run_cmds_server(_i.server, _i.user, f'python -u {_i.run_path} ' +_i.cmd, cwd=_i.server_project_path)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "c = Pyfig(wandb_mode='disabled', notebook=True)\n",
    "pprint(c.dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device: ', device)\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_fashion_loader(b_size, data_dir, mean=None, std=None, n_workers=4):\n",
    "    \n",
    "    data_tr = datasets.FashionMNIST(\n",
    "        root=data_dir, \n",
    "        download=True, \n",
    "        train=True,\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(28),\n",
    "            transforms.CenterCrop(28),\n",
    "            transforms.Normalize((mean), (std)),\n",
    "        ]\n",
    "        ),\n",
    "        target_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    loader_tr = DataLoader(\n",
    "        data_tr, \n",
    "        batch_size=b_size, \n",
    "        shuffle=True, \n",
    "        num_workers=n_workers,\n",
    "    )\n",
    "\n",
    "    data_test = datasets.FashionMNIST(\n",
    "        root=data_dir, \n",
    "        download=True, \n",
    "        train=False,\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(256),\n",
    "            transforms.Normalize((mean), (std)),\n",
    "        ]),\n",
    "        target_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    "    ])\n",
    "    )\n",
    "\n",
    "    loader_test = DataLoader(\n",
    "        data_test, \n",
    "        batch_size=b_size, \n",
    "        shuffle=True, \n",
    "        num_workers=n_workers,\n",
    "    )\n",
    "    return loader_tr, loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(c.seed)\n",
    "rng, subrng = jax.random.split(rng)\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    dynamic_scale: Optional[dynamic_scale_lib.DynamicScale] = None\n",
    "    params_ema: Any = None\n",
    "\n",
    "\n",
    "def create_train_state(rng, dynamic_scale=None):\n",
    "    \n",
    "    model = unet.Unet(\n",
    "        dim = c.model.dim, \n",
    "        out_dim =  c.data.channels,\n",
    "        dim_mults = c.model.dim_mults\n",
    "    )\n",
    "    input_dim = c.data.channels * 2 if c.ddpm.self_condition else c.data.channels\n",
    "    input_shape = (1, c.data.image_size, c.data.image_size, input_dim)\n",
    "\n",
    "    def initialized():\n",
    "        @jax.jit\n",
    "        def init(*args):\n",
    "            return model.init(*args)\n",
    "        variables = init(\n",
    "            {'params': rng}, \n",
    "            jnp.ones(input_shape, model.dtype), # x noisy image\n",
    "            jnp.ones(input_shape[:1], model.dtype) # t\n",
    "            )\n",
    "        return variables['params']\n",
    "    \n",
    "    tx = optax.adam(learning_rate=c.opt.lr , b1=c.opt.beta1, b2 = c.opt.beta2, eps=c.opt.eps)\n",
    "\n",
    "    params = initialized()\n",
    "    state = TrainState.create(\n",
    "        apply_fn=model.apply, \n",
    "        params=params,\n",
    "        params_ema=params,\n",
    "        tx=tx, \n",
    "        dynamic_scale=dynamic_scale\n",
    "    )\n",
    "\n",
    "    return state\n",
    "    \n",
    "state = create_train_state(rng)\n",
    "state = jax_utils.replicate(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ema_decay_schedule(update_after_step, inv_gamma, power, min_value, beta, **kw):\n",
    "\n",
    "    def ema_decay_schedule(step):\n",
    "        count = jnp.clip(step -  update_after_step - 1, a_min = 0.)\n",
    "        value = 1 - (1 + count /  inv_gamma) ** -  power \n",
    "        ema_rate = jnp.clip(value, a_min =  min_value, a_max =  beta)\n",
    "        return ema_rate\n",
    "\n",
    "    return ema_decay_schedule\n",
    "    \n",
    "ema_decay_fn = create_ema_decay_schedule(**c.ema.dict)\n",
    "\n",
    "def cosine_beta_schedule(timesteps):\n",
    "    \"\"\"Return cosine schedule \n",
    "    as proposed in https://arxiv.org/abs/2102.09672 \"\"\"\n",
    "    s=0.008\n",
    "    max_beta=0.999\n",
    "    ts = jnp.linspace(0, 1, timesteps + 1)\n",
    "    alphas_bar = jnp.cos((ts+s)/(1+s)*jnp.pi/2) ** 2\n",
    "    alphas_bar = alphas_bar/alphas_bar[0]\n",
    "    betas = 1 - (alphas_bar[1:] / alphas_bar[:-1])\n",
    "    return jnp.clip(betas, 0, max_beta)\n",
    "\n",
    "class ddpm_param:\n",
    "    timesteps = c.ddpm.timesteps\n",
    "    p2_loss_weight_gamma = c.ddpm.p2_loss_weight_gamma\n",
    "    p2_loss_weight_k = c.ddpm.p2_loss_weight_gamma\n",
    "\n",
    "    betas = cosine_beta_schedule(timesteps)\n",
    "\n",
    "    assert betas.shape == (timesteps,)\n",
    "    alphas = 1. - betas\n",
    "    alphas_bar = jnp.cumprod(alphas, axis=0)\n",
    "    sqrt_alphas_bar = jnp.sqrt(alphas_bar)\n",
    "    sqrt_1m_alphas_bar= jnp.sqrt(1. - alphas_bar)\n",
    "    \n",
    "    # calculate p2 reweighting\n",
    "    p2_loss_weight=  (p2_loss_weight_k + alphas_bar / (1 - alphas_bar)) ** -p2_loss_weight_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(logit, target):\n",
    "    return (logit - target)**2\n",
    "\n",
    "def l1_loss(logit, target): \n",
    "    return jnp.abs(logit - target)\n",
    "\n",
    "loss_table = dict(\n",
    "    l1_loss = l1_loss,\n",
    "    l2_loss = l2_loss\n",
    ")\n",
    "\n",
    "loss_fn = loss_table[c.opt.loss_fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_to_x0(noise, xt, batched_t, alphas_bar, sqrt_alphas_bar):\n",
    "    assert batched_t.shape[0] == xt.shape[0] == noise.shape[0] # make sure all has batch dimension\n",
    "    sqrt_alpha_bar = sqrt_alphas_bar[batched_t, None, None, None]\n",
    "    alpha_bar= alphas_bar[batched_t, None, None, None]\n",
    "    x0 = 1. / sqrt_alpha_bar * xt -  jnp.sqrt(1./alpha_bar-1) * noise\n",
    "    return x0\n",
    "\n",
    "def x0_to_noise(x0, xt, batched_t, alphas_bar, sqrt_alphas_bar):\n",
    "    assert batched_t.shape[0] == xt.shape[0] == x0.shape[0] # make sure all has batch dimension\n",
    "    sqrt_alpha_bar = sqrt_alphas_bar[batched_t, None, None, None]\n",
    "    alpha_bar= alphas_bar[batched_t, None, None, None]\n",
    "    noise = (1. / sqrt_alpha_bar * xt - x0) /jnp.sqrt(1./alpha_bar-1)\n",
    "    return noise\n",
    "\n",
    "def model_predict(state, x, x0, t, ddpm_param: ddpm_param, use_ema=True):\n",
    "    if use_ema:\n",
    "        variables = {'params': state.params_ema}\n",
    "    else:\n",
    "        variables = {'params': state.params}\n",
    "    \n",
    "    if c.ddpm.self_condition:\n",
    "        pred = state.apply_fn(variables, jnp.concatenate([x, x0],axis=-1), t)\n",
    "    else:\n",
    "        pred = state.apply_fn(variables, x, t)\n",
    "\n",
    "    if c.ddpm.is_pred_x0: # if the objective is is_pred_x0, pred == x0_pred\n",
    "        x0_pred = pred\n",
    "        noise_pred =  x0_to_noise(pred, x, t, ddpm_param.alphas_bar, ddpm_param.sqrt_alphas_bar)\n",
    "    else:\n",
    "        noise_pred = pred\n",
    "        x0_pred = noise_to_x0(pred, x, t, ddpm_param.alphas_bar, ddpm_param.sqrt_alphas_bar)\n",
    "    \n",
    "    return x0_pred, noise_pred\n",
    "\n",
    "def q_sample(x, t, noise, ddpm_param: ddpm_param):\n",
    "    sqrt_alpha_bar = ddpm_param.sqrt_alphas_bar[t, None, None, None]\n",
    "    sqrt_1m_alpha_bar = ddpm_param.sqrt_1m_alphas_bar[t,None,None,None]\n",
    "    x_t = sqrt_alpha_bar * x + sqrt_1m_alpha_bar * noise\n",
    "    return x_t\n",
    "\n",
    "def p_loss(rng, state: TrainState, b, loss_fn, ddpm_param: ddpm_param, pmap_axis='batch'):\n",
    "    rng, t_rng, noise_rng, condition_rng = jax.random.split(rng, 4)\n",
    "\n",
    "    assert b.dtype in [jnp.float32, jnp.float64]\n",
    "\n",
    "    B, H, W, C = b.shape\n",
    "    batched_t = jax.random.randint(t_rng, shape=(B,), dtype = jnp.int32, minval=0, maxval=len(ddpm_param.betas))\n",
    "    noise = jax.random.normal(noise_rng, b.shape)\n",
    "\n",
    "    target = b if c.ddpm.is_pred_x0 else noise\n",
    "    \n",
    "    x_t = q_sample(b, batched_t, noise, ddpm_param)\n",
    "\n",
    "    if c.ddpm.self_condition:\n",
    "        zeros = jnp.zeros_like(x_t)\n",
    "        \n",
    "        def estimate_x0(_): # self-conditioning \n",
    "            x0, _ = model_predict(state, x_t, zeros, batched_t, ddpm_param, use_ema=False)\n",
    "            return x0\n",
    "        \n",
    "        x0 = jax.lax.cond(\n",
    "            jax.random.uniform(condition_rng, shape=(1,))[0] < 0.5,\n",
    "            estimate_x0,\n",
    "            lambda _ :zeros,\n",
    "            None,\n",
    "        )\n",
    "        \n",
    "        x_t = jnp.concatenate([x_t, x0], axis=-1)\n",
    "\n",
    "    def compute_loss(params):\n",
    "        pred = state.apply_fn({'params':params}, x_t, batched_t)\n",
    "        loss = loss_fn(flatten(pred),flatten(target))\n",
    "        loss = jnp.mean(loss, axis=1)\n",
    "        assert loss.shape == (B,)\n",
    "        loss = loss * ddpm_param.p2_loss_weight[batched_t]\n",
    "        return loss.mean()\n",
    "    \n",
    "    dynamic_scale = state.dynamic_scale\n",
    "\n",
    "    if dynamic_scale: # dynamic loss takes care of averaging gradients across replicas\n",
    "        grad_fn = dynamic_scale.value_and_grad(compute_loss, axis_name=pmap_axis)\n",
    "        dynamic_scale, is_fin, loss, grads = grad_fn(state.params)\n",
    "    else: #  Re-use same axis_name as in the call to `pmap(...train_step,axis=...)` in the train function\n",
    "        grad_fn = jax.value_and_grad(compute_loss)\n",
    "        loss, grads = grad_fn(state.params)\n",
    "        grads = jax.lax.pmean(grads, axis_name=pmap_axis)\n",
    "    \n",
    "    loss = jax.lax.pmean(loss, axis_name=pmap_axis)\n",
    "    loss_ema = jax.lax.pmean(compute_loss(state.params_ema), axis_name=pmap_axis)\n",
    "\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'loss_ema': loss_ema,\n",
    "    }\n",
    "\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    if dynamic_scale: \n",
    "        # if is_fin == False the gradients contain Inf/NaNs and optimizer state and # params should be restored (= skip this step).\n",
    "        new_state=new_state.replace(\n",
    "            opt_state=jax.tree_map(\n",
    "                functools.partial(jnp.where, is_fin),\n",
    "                new_state.opt_state,\n",
    "                state.opt_state\n",
    "            ),\n",
    "            params=jax.tree_map(\n",
    "                functools.partial(jnp.where, is_fin),\n",
    "                new_state.params,\n",
    "                state.params\n",
    "            ),\n",
    "            dynamic_scale=dynamic_scale\n",
    "        )\n",
    "        metrics['scale'] = dynamic_scale.scale\n",
    "\n",
    "    return new_state, metrics\n",
    "\n",
    "train_step = functools.partial(\n",
    "    p_loss,\n",
    "    ddpm_param  = ddpm_param, \n",
    "    loss_fn     = loss_fn,\n",
    "    pmap_axis   = 'batch'\n",
    ")\n",
    "\n",
    "p_train_step = jax.pmap(train_step, axis_name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior_mean_variance(img, t, x0, v, ddpm):\n",
    "\n",
    "    beta = ddpm_param.betas[t,None,None,None] # only needed when t > 0\n",
    "    alpha = ddpm_param.alphas[t,None,None,None]\n",
    "    alpha_bar = ddpm_param.alphas_bar[t,None,None,None]\n",
    "    alpha_bar_last = ddpm_param.alphas_bar[t-1,None,None,None]\n",
    "    sqrt_alpha_bar_last = ddpm_param.sqrt_alphas_bar[t-1,None,None,None]\n",
    "\n",
    "    coef_x0 = beta * sqrt_alpha_bar_last / (1. - alpha_bar)\n",
    "    coef_xt = (1. - alpha_bar_last) * jnp.sqrt(alpha) / ( 1- alpha_bar)        \n",
    "    posterior_mean = coef_x0 * x0 + coef_xt * img\n",
    "        \n",
    "    posterior_variance = beta * (1 - alpha_bar_last) / (1. - alpha_bar)\n",
    "    posterior_log_variance = jnp.log(jnp.clip(posterior_variance, a_min = 1e-20))\n",
    "\n",
    "    return posterior_mean, posterior_log_variance\n",
    "\n",
    "\n",
    "def ddpm_sample_step(state, rng, x, t, x0_last, ddpm_param: ddpm_param):\n",
    " \n",
    "    batched_t = jnp.ones((x.shape[0],), dtype=jnp.int32) * t\n",
    "    \n",
    "    if c.ddpm.self_condition:\n",
    "        x0, v = model_predict(state, x, x0_last, batched_t, ddpm_param, use_ema=True) \n",
    "    else:\n",
    "        x0, v = model_predict(state, x, None, batched_t, ddpm_param, use_ema=True)\n",
    "    \n",
    "    x0 = jnp.clip(x0,-1.,1.) # make sure x0 between [-1,1]\n",
    "    posterior_mean, posterior_log_variance = get_posterior_mean_variance(x, t, x0, v, ddpm_param)\n",
    "    x = posterior_mean + jnp.exp(0.5 *  posterior_log_variance) * jax.random.normal(rng, x.shape) \n",
    "    return x, x0\n",
    "\n",
    "sample_step = functools.partial(\n",
    "    ddpm_sample_step, \n",
    "    ddpm_param=ddpm_param, \n",
    "    self_condition=c.ddpm.self_condition, \n",
    "    is_pred_x0=c.ddpm.is_pred_x0\n",
    ")\n",
    "\n",
    "p_sample_step = jax.pmap(sample_step, axis_name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_params_to_ema(state):\n",
    "   state = state.replace(params_ema = state.params)\n",
    "   return state\n",
    "\n",
    "def apply_ema_decay(state, ema_decay):\n",
    "    params_ema = jax.tree_map(lambda p_ema, p: p_ema * ema_decay + p * (1. - ema_decay), state.params_ema, state.params)\n",
    "    state = state.replace(params_ema = params_ema)\n",
    "    return state\n",
    "\n",
    "p_apply_ema = jax.pmap(apply_ema_decay, in_axes=(0, None), axis_name='batch')\n",
    "p_copy_params_to_ema = jax.pmap(copy_params_to_ema, axis_name='batch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "def to_wandb_config(d, parent_key: str = '', sep: str ='.'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(to_wandb_config(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            if isinstance(v, Path):\n",
    "                v = str(v)\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "wandb.init(\n",
    "    job_type=c.wandb.job_type,\n",
    "    entity=c.wandb.entity,\n",
    "    project=c.project,\n",
    "    config=to_wandb_config(c.dict),\n",
    "    settings=wandb.Settings(start_method='fork'),  # idk why this is an issue\n",
    "    dir=c.exp_path,\n",
    ")\n",
    "\n",
    "wandb.define_metric(\"*\", step_metric=\"train/step\")\n",
    "\n",
    "# # Display a project workspace\n",
    "# %wandb USERNAME/PROJECT\n",
    "# # Display a single run\n",
    "# %wandb USERNAME/PROJECT/runs/RUN_ID\n",
    "# # Display a sweep\n",
    "# %wandb USERNAME/PROJECT/sweeps/SWEEP_ID\n",
    "# # Display a report\n",
    "# %wandb USERNAME/PROJECT/reports/REPORT_ID\n",
    "# # Specify the height of embedded iframe\n",
    "# %wandb USERNAME/PROJECT -h 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_stats():\n",
    "    def normalize_to_neg_one_to_one(img):\n",
    "        return img * 2 - 1  \n",
    "    data_tr = datasets.FashionMNIST(\n",
    "        root=c.data_dir, \n",
    "        download=True, \n",
    "        train=True,\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(28),\n",
    "            transforms.CenterCrop(28),\n",
    "        ]),\n",
    "        target_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    "    ])\n",
    "    )\n",
    "    imgs = torch.stack([img_t for img_t, _ in data_tr], dim=3)\n",
    "    mean = imgs.view(1,-1).mean(dim=1) \n",
    "    std = imgs.view(1,-1).std(dim=1)\n",
    "    print('Data mean: ', mean, 'Data std: ', std)\n",
    "    return mean, std\n",
    "mean, std = get_data_stats()\n",
    "loader_tr, loader_test = get_fashion_loader(c.data.b_size, c.data_dir, mean=mean, std=std)  # is not an iterator or list\n",
    "img, l = next(iter(loader_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Image.fromarray(np.uint8((img[0, 0]*std+mean).cpu().numpy()*255))\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('1', size=(cols*w, rows*h))\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "\n",
    "    return grid\n",
    "std = float(std.cpu().numpy())\n",
    "mean = float(mean.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = []\n",
    "\n",
    "for ep in range(c.n_epoch):\n",
    "    for step, (b, target) in enumerate(loader_tr):\n",
    "        b = jnp.squeeze(jnp.expand_dims(jnp.array(b.numpy()), axis=(0,-1)), axis=2) # p, B, H, W, C\n",
    "        rng, *train_step_rng = jax.random.split(rng, num=jax.local_device_count() + 1)\n",
    "        train_step_rng = jnp.array(train_step_rng)\n",
    "        \n",
    "        state, metrics = p_train_step(train_step_rng, state, b)\n",
    "\n",
    "        if step <= c.ema.update_after_step:\n",
    "            state = p_copy_params_to_ema(state)\n",
    "\n",
    "        elif step % c.ema.update_every == 0:\n",
    "            ema_decay = ema_decay_fn(step)\n",
    "            state =  p_apply_ema(state, ema_decay)\n",
    "\n",
    "        if step % c.log_metric_step == 0:\n",
    "\n",
    "            train_metrics.append(metrics)\n",
    "            train_metrics = common_utils.get_metrics(train_metrics)\n",
    "            \n",
    "            summary = {\n",
    "                f'train/{k}': v\n",
    "                for k, v in jax.tree_map(lambda x: x.mean(), train_metrics).items()\n",
    "            }\n",
    "\n",
    "            train_metrics = []\n",
    "            b = ((np.array(b)*std+mean)*255).astype(np.uint8)\n",
    "            imgs = [wandb.Image(Image.fromarray(b[0, i].reshape(28, 28))) for i in range(9)]\n",
    "            \n",
    "            wandb.log({\n",
    "                    \"train/step\": step, \n",
    "                    'train/sample': imgs,\n",
    "                    **summary\n",
    "            })\n",
    " \n",
    "    print('Epoch: ', ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.fromarray(b[0, 0].reshape(28, 28, 1))\n",
    "b[0, 0].max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('dex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b4edb4a58a0461d84e636e9142615dc364f099c3851533546c18fbe9e367308"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
