{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: \"WANDB_NOTEBOOK_NAME\"=\"run\" # ❕same as notebook\n"
     ]
    }
   ],
   "source": [
    "# Distribution ✨ jit ❇ Demo 💪 \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,4'\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"run\" # ❕same as notebook\n",
    "# from jax.config import config\n",
    "# config.update('jax_disable_jit', True)\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import re as regex\n",
    "from pprint import pprint\n",
    "\n",
    "import jax\n",
    "from jax import pmap, grad\n",
    "from jax import numpy as jnp\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.core.frozen_dict import FrozenDict\t\n",
    "\n",
    "from pyfig import Pyfig\n",
    "from hwat import FermiNet, sample, compute_ke_b, PotentialEnergy\n",
    "from hwat import PotentialEnergy, sample, compute_ke_b\n",
    "import wandb\n",
    "\n",
    "from utils import flat_any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stats(k, v, new_d, p='tr', suf='', sep='/', sep_long='-'):\n",
    "\tdepth = p.count('/')\n",
    "\tif depth > 1:\n",
    "\t\tsep = sep_long\n",
    "\tif isinstance(v, dict):\n",
    "\t\tfor k_sub,v_sub in v.items():\n",
    "\t\t\tcollect_stats(k, v_sub, new_d, p=(p+sep+k_sub))\n",
    "\telse:\n",
    "\t\tnew_d[p+sep+k+suf] = v\n",
    "\treturn new_d\n",
    "\n",
    "def compute_metrix(d:dict, mode='tr'):\n",
    "\tpattern_ignore = ['Dense']\n",
    "\tfancy = dict(\n",
    "\t\tpe\t\t= r'$V(X)',    \t\t\t\t\n",
    "\t\tke\t\t= r'$\\nabla^2',    \t\t\n",
    "\t\te\t\t= r'$E',\t\t\t\t\t\t\n",
    "\t\tlog_psi\t= r'$\\log\\psi', \t\t\t\n",
    "\t\tdeltar\t= r'$\\delta_\\mathrm{r}',\t\n",
    "\t\tx\t\t= r'$r_\\mathrm{e}',\n",
    "\t)\n",
    "\t_d = {}\n",
    "\tfor k,v in d.items():\n",
    "\t\tk = fancy.get(k, k)\n",
    "\t\tv = jax.device_get(v)\n",
    "\t\tif isinstance(v, FrozenDict):\n",
    "\t\t\tv = v.unfreeze()\n",
    "\t\t\n",
    "\t\tv_mean = jax.tree_map(lambda x: x.mean(), v) if not np.isscalar(v) else v\n",
    "\t\tv_std = jax.tree_map(lambda x: x.std(), v) if not np.isscalar(v) else 0.\n",
    "\n",
    "\t\t_d = collect_stats(k, v_mean, _d, p=mode, suf=r'_\\mu$')\n",
    "\t\t_d = collect_stats(k, v_std, _d, p=mode, suf=r'_\\sigma$')\n",
    "\n",
    "\t# return {k:v for k,v in _d.items() if not any([regex.match(k, pat) for pat in pattern_ignore])}\n",
    "\treturn {k:v for k,v in _d.items() if not any([pat in k for pat in pattern_ignore])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 19:34:24.205990: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:497] The NVIDIA driver's CUDA version is 11.4 which is older than the ptxas CUDA version (11.6.55). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 args unmerged: ✅\n",
      "Path:  /home/amawi/projects/hwat/exp/demo-final/u2HRah9 ✅\n",
      "System \n",
      "{'a': DeviceArray([[0., 0., 0.]], dtype=float32),\n",
      " 'a_z': DeviceArray(4, dtype=int32, weak_type=True),\n",
      " 'acc_target': 0.5,\n",
      " 'corr_len': 20,\n",
      " 'equil_len': 1000,\n",
      " 'init_walker': functools.partial(<function init_walker at 0x7f5d16775d80>, n_b=64, n_u=2, n_d=2, center=DeviceArray([[0., 0., 0.]], dtype=float32), std=0.1),\n",
      " 'l_e': [4],\n",
      " 'n_b': 64,\n",
      " 'n_d': 2,\n",
      " 'n_e': 4,\n",
      " 'n_u': 2}\n",
      "Model \n",
      "{'compute_p_emb': functools.partial(<function compute_emb at 0x7f5d16774790>, terms=['xx']),\n",
      " 'compute_s_emb': functools.partial(<function compute_emb at 0x7f5d16774790>, terms=['x_rlen', 'x']),\n",
      " 'compute_s_perm': functools.partial(<function compute_s_perm at 0x7f5d16775510>, n_u=2),\n",
      " 'n_det': 1,\n",
      " 'n_fb': 16,\n",
      " 'n_fb_out': 64,\n",
      " 'n_pv': 8,\n",
      " 'n_sv': 16,\n",
      " 'terms_p_emb': ['xx'],\n",
      " 'terms_s_emb': ['x_rlen', 'x']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxmax1\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/amawi/projects/hwat/exp/demo-final/u2HRah9/wandb/run-20221130_193428-s8s35l1a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/xmax1/hwat/runs/s8s35l1a\" target=\"_blank\">treasured-waterfall-83</a></strong> to <a href=\"https://wandb.ai/xmax1/hwat\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run:  xmax1/hwat/s8s35l1a ✅\n"
     ]
    }
   ],
   "source": [
    "args = {'l_e':[4,],'n_u': 2,'n_b': 64, 'n_sv': 16, 'n_pv': 8, 'corr_len': 20, 'n_step': 1000, 'log_metric_step': 1,'exp_name':'demo-final'}\n",
    "c = Pyfig(wandb_mode='online', args=args, get_sys_arg=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: x (2, 64, 4, 3) rng (2, 2) ✅\n",
      "Model ✅\n",
      "Equil ✅\n"
     ]
    }
   ],
   "source": [
    "part_sample = partial(sample, acc_target=c.data.acc_target)\n",
    "\n",
    "rng = c.rng_init\n",
    "x = c.data.init_walker(rng, n_b=c.data.n_b)\n",
    "deltar = jnp.ones((c.n_device, 1), dtype=x.dtype)*0.02\n",
    "print(f'Init: x {x.shape} rng {rng.shape} ✅')\n",
    "\n",
    "@partial(jax.pmap, axis_name='b', in_axes=(0,0))\n",
    "def create_train_state(rng, x):\n",
    "    model = c.partial(FermiNet)  \n",
    "    params = model.init(rng, x)\n",
    "    state = TrainState.create(apply_fn=model.apply, params=params, tx=c.opt.tx)\n",
    "    return state\n",
    "\n",
    "state = create_train_state(rng, x)\n",
    "print('Model ✅')\n",
    "\n",
    "@partial(jax.pmap, axis_name='b', in_axes=(0,0,0,0))\n",
    "def equil(rng, state, x, deltar):\n",
    "    x, v_sam = part_sample(rng, state, x, deltar)\n",
    "    return x, v_sam\n",
    "print('Equil ✅')\n",
    "\n",
    "wandb.define_metric(\"*\", step_metric=\"eq/step\")\n",
    "for step in range(1, c.data.equil_len//c.data.corr_len):\n",
    "    x, v_sam = equil(rng, state, x, deltar)\n",
    "    rng, deltar, acc = v_sam['rng'], v_sam['deltar'], v_sam['acc']\n",
    "    if not (step % c.log_metric_step):\n",
    "        wandb.log({'eq/step':step, **v_sam})\n",
    "print('Walkers ✅ Training Variables ✅')\n",
    "\n",
    "@partial(pmap, in_axes=(0, 0, 0, 0))\n",
    "def train_step(rng, state, x, deltar):\n",
    "    x, v_sam = part_sample(rng, state, x, deltar)\n",
    "\n",
    "    pe = partial(PotentialEnergy(a=c.data.a, a_z=c.data.a_z).apply, {})(x)\n",
    "    ke = compute_ke_b(state, x)\n",
    "    e = pe + ke\n",
    "\n",
    "    def loss(_params):\n",
    "        return (e * state.apply_fn(_params, x)).mean()\n",
    "    \n",
    "    grads = grad(loss)(state.params)\n",
    "\n",
    "    v = dict(grads=grads, pe=pe, ke=ke, e=e, deltar=v_sam['deltar'],\n",
    "                    rng=rng, x=x, acc=v_sam['acc'])\n",
    "                    \n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, v\n",
    "\n",
    "state, data = train_step(rng, state, x, deltar)\n",
    "print('Train Step ✅')\n",
    "\n",
    "print('Go seek: ', c.wandb_c.wandb_run_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check model top to bottoms\n",
    "# check acc and deltar\n",
    "# test 1\n",
    "# add r to electron inputs\n",
    "\n",
    "\n",
    "wandb.define_metric(\"*\", step_metric=\"tr/step\")\n",
    "for step in range(1, c.n_step+1):\n",
    "    state, data = train_step(rng, state, x, deltar)\n",
    "    rng, deltar, x = data['rng'], data['deltar'], data['x']\n",
    "\n",
    "    if not (step % c.log_metric_step):\n",
    "        metrix = compute_metrix(data)\n",
    "        wandb.log({'tr/step':step, **metrix})\n",
    "        m = ' '.join([f'{k} {v:.5f} ' for k,v in metrix.items() if 'E_' in k])\n",
    "        print(f'Step {step} {m}')\n",
    "    if not (step % c.log_state_step):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:44\u001b[0;36m\u001b[0m\n\u001b[0;31m    (loss, new_model_state), grads = jax.value_and_grad(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# 💓 \n",
    "class tr_data:\n",
    "    x:jnp.ndarray=x\n",
    "    rng:jnp.ndarray=rng\n",
    "\n",
    "# Likely needed for updating outside the loop, not sure\n",
    "@jax.pmap\n",
    "def update_model(state, grads):\n",
    "  return state.apply_gradients(grads=grads)\n",
    "print('Update ✅')\n",
    "\n",
    "\n",
    "# add mutable states to the trian state where the parallelism is handled? \n",
    "from flax.training import train_state\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "  batch_stats: flax.core.FrozenDict[str, Any]\n",
    "\n",
    "# how to include other variables\n",
    "def loss_fn(params):\n",
    "    outputs, new_model_state = state.apply_fn(\n",
    "        {'params': params, 'batch_stats': state.batch_stats},\n",
    "        inputs,\n",
    "        mutable=['batch_stats'])\n",
    "    loss = xent_loss(outputs, labels)\n",
    "    return loss, new_model_state\n",
    "\n",
    "  (loss, new_model_state), grads = jax.value_and_grad(\n",
    "      loss_fn, has_aux=True)(state.params)\n",
    "  new_state = state.apply_gradients(\n",
    "      grads=grads,\n",
    "      batch_stats=new_model_state['batch_stats'],\n",
    "  )\n",
    "\n",
    "class arg_cls(Pyfig):\n",
    "    def __init__(_i):\n",
    "        pass\n",
    "arg_cls.data.n_e = 6\n",
    "arg_cls.data.n_u = 6\n",
    "arg_cls.data.n_b = 6\n",
    "arg_cls.n_step = 20\n",
    "arg_cls.log_metric_step = 5\n",
    "arg_cls.exp_name = 'demo'\n",
    "args = flat_any(arg_cls().d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('dex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b4edb4a58a0461d84e636e9142615dc364f099c3851533546c18fbe9e367308"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
