{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: \"WANDB_NOTEBOOK_NAME\"=\"run\" # â•same as notebook\n"
     ]
    }
   ],
   "source": [
    "# Distribution âœ¨ jit â‡ Demo ðŸ’ª \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,4'\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"run\" # â•same as notebook\n",
    "# from jax.config import config\n",
    "# config.update('jax_disable_jit', True)\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import re as regex\n",
    "from pprint import pprint\n",
    "\n",
    "import jax\n",
    "from jax import pmap, grad\n",
    "from jax import numpy as jnp\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.core.frozen_dict import FrozenDict\t\n",
    "\n",
    "from pyfig import Pyfig\n",
    "from hwat import FermiNet, sample, compute_ke_b, PotentialEnergy\n",
    "from hwat import PotentialEnergy, sample, compute_ke_b\n",
    "import wandb\n",
    "\n",
    "from utils import flat_any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stats(k, v, new_d, p='tr', suf='', sep='/', sep_long='-'):\n",
    "\tdepth = p.count('/')\n",
    "\tif depth > 1:\n",
    "\t\tsep = sep_long\n",
    "\tif isinstance(v, dict):\n",
    "\t\tfor k_sub,v_sub in v.items():\n",
    "\t\t\tcollect_stats(k, v_sub, new_d, p=(p+sep+k_sub))\n",
    "\telse:\n",
    "\t\tnew_d[p+sep+k+suf] = v\n",
    "\treturn new_d\n",
    "\n",
    "def compute_metrix(d:dict, mode='tr'):\n",
    "\tpattern_ignore = ['Dense']\n",
    "\tfancy = dict(\n",
    "\t\tpe\t\t= r'$V(X)',    \t\t\t\t\n",
    "\t\tke\t\t= r'$\\nabla^2',    \t\t\n",
    "\t\te\t\t= r'$E',\t\t\t\t\t\t\n",
    "\t\tlog_psi\t= r'$\\log\\psi', \t\t\t\n",
    "\t\tdeltar\t= r'$\\delta_\\mathrm{r}',\t\n",
    "\t\tx\t\t= r'$r_\\mathrm{e}',\n",
    "\t)\n",
    "\t_d = {}\n",
    "\tfor k,v in d.items():\n",
    "\t\tk = fancy.get(k, k)\n",
    "\t\tv = jax.device_get(v)\n",
    "\t\tif isinstance(v, FrozenDict):\n",
    "\t\t\tv = v.unfreeze()\n",
    "\t\t\n",
    "\t\tv_mean = jax.tree_map(lambda x: x.mean(), v) if not np.isscalar(v) else v\n",
    "\t\tv_std = jax.tree_map(lambda x: x.std(), v) if not np.isscalar(v) else 0.\n",
    "\n",
    "\t\t_d = collect_stats(k, v_mean, _d, p=mode, suf=r'_\\mu$')\n",
    "\t\t_d = collect_stats(k, v_std, _d, p=mode, suf=r'_\\sigma$')\n",
    "\n",
    "\t# return {k:v for k,v in _d.items() if not any([regex.match(k, pat) for pat in pattern_ignore])}\n",
    "\treturn {k:v for k,v in _d.items() if not any([pat in k for pat in pattern_ignore])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 19:34:24.205990: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:497] The NVIDIA driver's CUDA version is 11.4 which is older than the ptxas CUDA version (11.6.55). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 args unmerged: âœ…\n",
      "Path:  /home/amawi/projects/hwat/exp/demo-final/u2HRah9 âœ…\n",
      "System \n",
      "{'a': DeviceArray([[0., 0., 0.]], dtype=float32),\n",
      " 'a_z': DeviceArray(4, dtype=int32, weak_type=True),\n",
      " 'acc_target': 0.5,\n",
      " 'corr_len': 20,\n",
      " 'equil_len': 1000,\n",
      " 'init_walker': functools.partial(<function init_walker at 0x7f5d16775d80>, n_b=64, n_u=2, n_d=2, center=DeviceArray([[0., 0., 0.]], dtype=float32), std=0.1),\n",
      " 'l_e': [4],\n",
      " 'n_b': 64,\n",
      " 'n_d': 2,\n",
      " 'n_e': 4,\n",
      " 'n_u': 2}\n",
      "Model \n",
      "{'compute_p_emb': functools.partial(<function compute_emb at 0x7f5d16774790>, terms=['xx']),\n",
      " 'compute_s_emb': functools.partial(<function compute_emb at 0x7f5d16774790>, terms=['x_rlen', 'x']),\n",
      " 'compute_s_perm': functools.partial(<function compute_s_perm at 0x7f5d16775510>, n_u=2),\n",
      " 'n_det': 1,\n",
      " 'n_fb': 16,\n",
      " 'n_fb_out': 64,\n",
      " 'n_pv': 8,\n",
      " 'n_sv': 16,\n",
      " 'terms_p_emb': ['xx'],\n",
      " 'terms_s_emb': ['x_rlen', 'x']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxmax1\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/amawi/projects/hwat/exp/demo-final/u2HRah9/wandb/run-20221130_193428-s8s35l1a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/xmax1/hwat/runs/s8s35l1a\" target=\"_blank\">treasured-waterfall-83</a></strong> to <a href=\"https://wandb.ai/xmax1/hwat\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run:  xmax1/hwat/s8s35l1a âœ…\n"
     ]
    }
   ],
   "source": [
    "args = {'l_e':[4,],'n_u': 2,'n_b': 64, 'n_sv': 16, 'n_pv': 8, 'corr_len': 20, 'n_step': 1000, 'log_metric_step': 1,'exp_name':'demo-final'}\n",
    "c = Pyfig(wandb_mode='online', args=args, get_sys_arg=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: x (2, 64, 4, 3) rng (2, 2) âœ…\n",
      "Model âœ…\n",
      "Equil âœ…\n"
     ]
    }
   ],
   "source": [
    "part_sample = partial(sample, acc_target=c.data.acc_target)\n",
    "\n",
    "rng = c.rng_init\n",
    "x = c.data.init_walker(rng, n_b=c.data.n_b)\n",
    "deltar = jnp.ones((c.n_device, 1), dtype=x.dtype)*0.02\n",
    "print(f'Init: x {x.shape} rng {rng.shape} âœ…')\n",
    "\n",
    "@partial(jax.pmap, axis_name='b', in_axes=(0,0))\n",
    "def create_train_state(rng, x):\n",
    "    model = c.partial(FermiNet)  \n",
    "    params = model.init(rng, x)\n",
    "    state = TrainState.create(apply_fn=model.apply, params=params, tx=c.opt.tx)\n",
    "    return state\n",
    "\n",
    "state = create_train_state(rng, x)\n",
    "print('Model âœ…')\n",
    "\n",
    "@partial(jax.pmap, axis_name='b', in_axes=(0,0,0,0))\n",
    "def equil(rng, state, x, deltar):\n",
    "    x, v_sam = part_sample(rng, state, x, deltar)\n",
    "    return x, v_sam\n",
    "print('Equil âœ…')\n",
    "\n",
    "wandb.define_metric(\"*\", step_metric=\"eq/step\")\n",
    "for step in range(1, c.data.equil_len//c.data.corr_len):\n",
    "    x, v_sam = equil(rng, state, x, deltar)\n",
    "    rng, deltar, acc = v_sam['rng'], v_sam['deltar'], v_sam['acc']\n",
    "    if not (step % c.log_metric_step):\n",
    "        wandb.log({'eq/step':step, **v_sam})\n",
    "print('Walkers âœ… Training Variables âœ…')\n",
    "\n",
    "@partial(pmap, in_axes=(0, 0, 0, 0))\n",
    "def train_step(rng, state, x, deltar):\n",
    "    x, v_sam = part_sample(rng, state, x, deltar)\n",
    "\n",
    "    pe = partial(PotentialEnergy(a=c.data.a, a_z=c.data.a_z).apply, {})(x)\n",
    "    ke = compute_ke_b(state, x)\n",
    "    e = pe + ke\n",
    "\n",
    "    def loss(_params):\n",
    "        return (e * state.apply_fn(_params, x)).mean()\n",
    "    \n",
    "    grads = grad(loss)(state.params)\n",
    "\n",
    "    v = dict(grads=grads, pe=pe, ke=ke, e=e, deltar=v_sam['deltar'],\n",
    "                    rng=rng, x=x, acc=v_sam['acc'])\n",
    "                    \n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, v\n",
    "\n",
    "state, data = train_step(rng, state, x, deltar)\n",
    "print('Train Step âœ…')\n",
    "\n",
    "print('Go seek: ', c.wandb_c.wandb_run_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check model top to bottoms\n",
    "# check acc and deltar\n",
    "# test 1\n",
    "# add r to electron inputs\n",
    "\n",
    "\n",
    "wandb.define_metric(\"*\", step_metric=\"tr/step\")\n",
    "for step in range(1, c.n_step+1):\n",
    "    state, data = train_step(rng, state, x, deltar)\n",
    "    rng, deltar, x = data['rng'], data['deltar'], data['x']\n",
    "\n",
    "    if not (step % c.log_metric_step):\n",
    "        metrix = compute_metrix(data)\n",
    "        wandb.log({'tr/step':step, **metrix})\n",
    "        m = ' '.join([f'{k} {v:.5f} ' for k,v in metrix.items() if 'E_' in k])\n",
    "        print(f'Step {step} {m}')\n",
    "    if not (step % c.log_state_step):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:44\u001b[0;36m\u001b[0m\n\u001b[0;31m    (loss, new_model_state), grads = jax.value_and_grad(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# ðŸ’“ \n",
    "class tr_data:\n",
    "    x:jnp.ndarray=x\n",
    "    rng:jnp.ndarray=rng\n",
    "\n",
    "# Likely needed for updating outside the loop, not sure\n",
    "@jax.pmap\n",
    "def update_model(state, grads):\n",
    "  return state.apply_gradients(grads=grads)\n",
    "print('Update âœ…')\n",
    "\n",
    "\n",
    "# add mutable states to the trian state where the parallelism is handled? \n",
    "from flax.training import train_state\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "  batch_stats: flax.core.FrozenDict[str, Any]\n",
    "\n",
    "# how to include other variables\n",
    "def loss_fn(params):\n",
    "    outputs, new_model_state = state.apply_fn(\n",
    "        {'params': params, 'batch_stats': state.batch_stats},\n",
    "        inputs,\n",
    "        mutable=['batch_stats'])\n",
    "    loss = xent_loss(outputs, labels)\n",
    "    return loss, new_model_state\n",
    "\n",
    "  (loss, new_model_state), grads = jax.value_and_grad(\n",
    "      loss_fn, has_aux=True)(state.params)\n",
    "  new_state = state.apply_gradients(\n",
    "      grads=grads,\n",
    "      batch_stats=new_model_state['batch_stats'],\n",
    "  )\n",
    "\n",
    "class arg_cls(Pyfig):\n",
    "    def __init__(_i):\n",
    "        pass\n",
    "arg_cls.data.n_e = 6\n",
    "arg_cls.data.n_u = 6\n",
    "arg_cls.data.n_b = 6\n",
    "arg_cls.n_step = 20\n",
    "arg_cls.log_metric_step = 5\n",
    "arg_cls.exp_name = 'demo'\n",
    "args = flat_any(arg_cls().d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('dex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b4edb4a58a0461d84e636e9142615dc364f099c3851533546c18fbe9e367308"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
