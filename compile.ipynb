{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init sub classes\n",
      "updating configuration\n",
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: b90fa37 stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n",
      "ip not in ref\n",
      "127.0.0.1 None\n",
      "stdin not in ref\n",
      "9003 None\n",
      "control not in ref\n",
      "9001 None\n",
      "hb not in ref\n",
      "9000 None\n",
      "Session.signature_scheme not in ref\n",
      "\"hmac-sha256\" None\n",
      "Session.key not in ref\n",
      "b\"1f823ea7-fc9c-45d6-8cb6-fa275249fe03\" None\n",
      "shell not in ref\n",
      "9002 None\n",
      "transport not in ref\n",
      "\"tcp\" None\n",
      "iopub not in ref\n",
      "9004 None\n",
      "f not in ref\n",
      "/home/amawi/.local/share/jupyter/runtime/kernel-v2-24743wxAnGShDLL5i.json None\n",
      "ip 127.0.0.1 not assigned\n",
      "stdin 9003 not assigned\n",
      "control 9001 not assigned\n",
      "hb 9000 not assigned\n",
      "Session.signature_scheme hmac-sha256 not assigned\n",
      "Session.key b'1f823ea7-fc9c-45d6-8cb6-fa275249fe03' not assigned\n",
      "shell 9002 not assigned\n",
      "transport tcp not assigned\n",
      "iopub 9004 not assigned\n",
      "f /home/amawi/.local/share/jupyter/runtime/kernel-v2-24743wxAnGShDLL5i.json not assigned\n",
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: b90fa37 stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n",
      "running script\n",
      "setting exp_path\n",
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: b90fa37 stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxmax1\u001b[0m (\u001b[33mhwat\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>dump/exp/demo-43/JhA0iAU/wandb/run-20221227_182620-23ldqwe7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hwat/hwat/runs/23ldqwe7\" target=\"_blank\">polished-waterfall-228</a></strong> to <a href=\"https://wandb.ai/hwat/hwat\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ 1 GPUs available\n"
     ]
    }
   ],
   "source": [
    "# TORCH MNIST DISTRIBUTED EXAMPLE\n",
    "\n",
    "\"\"\"run.py:\"\"\"\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from pyfig import Pyfig\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)   # ‚ùó Ensure works when default not set AND can go float32 or 64\n",
    "\n",
    "arg = dict(\n",
    "    charge = 0,\n",
    "    spin  = 0,\n",
    "    a = np.array([[0.0, 0.0, 0.0],]),\n",
    "    a_z  = np.array([4.,]),\n",
    "    n_b = 256, \n",
    "    n_sv = 32, \n",
    "    n_pv = 16, \n",
    "    n_corr = 40, \n",
    "    n_step = 10000, \n",
    "    log_metric_step = 5, \n",
    "    exp_name = 'demo',\n",
    "    # sweep = {},\n",
    ")\n",
    "\n",
    "c = Pyfig(wb_mode='online', arg=arg, submit=False, run_sweep=False)\n",
    "\n",
    "n_device = c.n_device\n",
    "print(f'ü§ñ {n_device} GPUs available')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "cuda:  cuda n_dev:  1 device cuda name:  NVIDIA TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1234)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)   # ‚ùó Ensure works when default not set AND can go float32 or 64\n",
    "n_device = torch.cuda.device_count()\n",
    "print(n_device)\n",
    "current_device = torch.cuda.current_device()\n",
    "device = torch.cuda.device(0)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device_name = torch.cuda.get_device_name(0)\n",
    "print('cuda: ', device, 'n_dev: ', n_device, 'device', device, 'name: ', device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/amawi/projects/hwat\n",
      "stdout: b90fa37 stderr: \n",
      "Run: ['hostname'] at .\n",
      "stdout: oceanus  stderr: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/home/amawi/projects/hwat/hwat_func.py\", line 119, in forward\n\t\tassert orb_u.shape == (1, ii.n_u, ii.n_u)\n\t\n\t\tlog_psi, sgn = logabssumdet([orb_u, orb_d])\n                 ~~~~~~~~~~~~ <--- HERE\n\t\n\t\tif ii.with_sign:\n  File \"/home/amawi/projects/hwat/hwat_func.py\", line 151, in logabssumdet\n\t\tlogdet = logdets[0] * logdets[1]\n\t\t# sign_in, logdet = reduce(_red_slogdet, slogdets)  \t\t\t\t\t\t# take product of n_u or n_d!=1 cases\n\t\tmaxlogdet = torch.max(logdet)\t\t\t\t\t\t\t\t\t\t\t# adjusted for new inputs\n  ~~~~~~~~~ <--- HERE\n\t\tdet = sign_in * dets * torch.exp(logdet-maxlogdet)\t\t\t\t\t\t# product of all these things is determinant\n\t\nRuntimeError: Cannot input a tensor that requires grad as a scalar argument\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m mod \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mscript(model)\n\u001b[1;32m     35\u001b[0m r\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 36\u001b[0m mod(r[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/.conda/envs/lumi/lib/python3.9/site-packages/torch/nn/modules/module.py:1482\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1480\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1481\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1482\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1483\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/home/amawi/projects/hwat/hwat_func.py\", line 119, in forward\n\t\tassert orb_u.shape == (1, ii.n_u, ii.n_u)\n\t\n\t\tlog_psi, sgn = logabssumdet([orb_u, orb_d])\n                 ~~~~~~~~~~~~ <--- HERE\n\t\n\t\tif ii.with_sign:\n  File \"/home/amawi/projects/hwat/hwat_func.py\", line 151, in logabssumdet\n\t\tlogdet = logdets[0] * logdets[1]\n\t\t# sign_in, logdet = reduce(_red_slogdet, slogdets)  \t\t\t\t\t\t# take product of n_u or n_d!=1 cases\n\t\tmaxlogdet = torch.max(logdet)\t\t\t\t\t\t\t\t\t\t\t# adjusted for new inputs\n  ~~~~~~~~~ <--- HERE\n\t\tdet = sign_in * dets * torch.exp(logdet-maxlogdet)\t\t\t\t\t\t# product of all these things is determinant\n\t\nRuntimeError: Cannot input a tensor that requires grad as a scalar argument\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### model (aka Trainmodel) ### \n",
    "from hwat_func import Ansatz_fb\n",
    "from torch import nn\n",
    "from hwat_func import compute_ke_b, compute_pe_b\n",
    "from hwat_func import init_r, get_center_points\n",
    "import wandb\n",
    "from hwat_func import keep_around_points, sample_b\n",
    "from utils import compute_metrix\n",
    "\n",
    "from functorch import vmap, make_functional, grad\n",
    "\n",
    "_dummy = torch.randn((1,))\n",
    "dtype = _dummy.dtype\n",
    "device='cuda'\n",
    "c._convert(device=device, dtype=dtype)\n",
    "model = c.partial(Ansatz_fb).to(device).to(dtype)\n",
    "\n",
    "center_points = get_center_points(c.data.n_e, c.data.a)\n",
    "r = init_r(n_device, c.data.n_b, c.data.n_e, center_points, std=0.1)[0]\n",
    "deltar = torch.tensor([0.02]).to(device).to(dtype)\n",
    "\n",
    "# from torch.jit import Final    - https://pytorch.org/docs/stable/jit.html type safety and optimisations\n",
    "\n",
    "# from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "#     with record_function(\"model_inference\"):\n",
    "#         model(r[0])\n",
    "\n",
    "# # print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "# print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "\n",
    "mod = torch.jit.script(model)\n",
    "\n",
    "r.requires_grad_(False)\n",
    "mod(r[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### fix sampler\n",
    "### fix train step \n",
    "### metrix conversion\n",
    "\n",
    "model_fn, params = make_functional(model)\n",
    "# model_v = torch.compile(model_fn)\n",
    "model_v = torch.jit.script(model_fn)\n",
    "model_v = vmap(model_fn, in_dims=(None, 0))\n",
    "\n",
    "model_v(params, r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.RAdam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "wandb.define_metric(\"*\", step_metric=\"tr/step\")\n",
    "for step in range(1, c.n_step+1):\n",
    "    \n",
    "    r, acc, deltar = sample_b(model_v, params, r, deltar, n_corr=c.data.n_corr)  # ‚ùóneeds testing \n",
    "    r = keep_around_points(r, center_points, l=2.) if step < 1000 else r\n",
    "    \n",
    "    model_ke = lambda _r: model_v(params, _r).sum()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ke = compute_ke_b(model_ke, r)\n",
    "        pe = compute_pe_b(r, c.data.a, c.data.a_z)\n",
    "        e = pe + ke\n",
    "        e_mean_dist = torch.mean(torch.abs(torch.median(e) - e))\n",
    "        e_clip = torch.clip(e, min=e-5*e_mean_dist, max=e+5*e_mean_dist)\n",
    "\n",
    "    # opt.zero_grad()\n",
    "    loss_fn = lambda _params: ((e_clip - e_clip.mean())*model_v(_params, r)).mean()\n",
    "    \n",
    "    grads = grad(loss_fn)(params)\n",
    "    for p, g in zip(model.parameters(), grads):\n",
    "        p.grad = torch.nn.utils.clip_grad_norm_(g.clone(), max_norm=1.)\n",
    "\n",
    "    opt.step()\n",
    "    \n",
    "    params = [p.detach() for p in model.parameters()]\n",
    "    grads = [p.grad.detach() for p in model.parameters()]\n",
    "    \n",
    "    v_tr = dict(\n",
    "        params=params, grads=grads,\n",
    "        e=e, pe=pe, ke=ke, r=r,\n",
    "    )\n",
    "    \n",
    "    if not (step % c.log_metric_step):\n",
    "        metrix = compute_metrix(v_tr)  # ‚ùó needs converting to torch, ie tree maps\n",
    "        wandb.log({'tr/step':step, **metrix})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b38d4d910256a52431c1e658e4ec4972e8b118bd5f76052a504536bbce1f208"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
