{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis '}' does not match opening parenthesis '(' on line 18 (320511946.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [1], line 30\u001b[0;36m\u001b[0m\n\u001b[0;31m    }\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis '}' does not match opening parenthesis '(' on line 18\n"
     ]
    }
   ],
   "source": [
    "### Distribution ‚ú® ‚ùá Demo üí™ ### \n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "### fancy logging variables, philosophically reminding us of the goal ###\n",
    "fancy = dict(\n",
    "\t\tpe\t\t= r'$V(X)',    \t\t\t\t\n",
    "\t\tke\t\t= r'$\\nabla^2',    \t\t\n",
    "\t\te\t\t= r'$E',\t\t\t\t\t\t\n",
    "\t\tlog_psi\t= r'$\\log\\psi', \t\t\t\n",
    "\t\tdeltar\t= r'$\\delta_\\mathrm{r}',\t\n",
    "\t\tx\t\t= r'$r_\\mathrm{e}',\n",
    ")\n",
    "\n",
    "### pyfig ###\n",
    "from pyfig import Pyfig\n",
    "\n",
    "args = dict(\n",
    "\tcharge = 0,\n",
    "\tspin  = 0,\n",
    "\ta = np.array([[0.0, 0.0, 0.0],]),\n",
    "\ta_z  = np.array([4.,]),\n",
    "\tn_b = 512, \n",
    "\tn_sv = 32, \n",
    "\tn_pv = 32, \n",
    "\tn_corr = 20, \n",
    "\tn_step = 10000, \n",
    "\tlog_metric_step = 50, \n",
    "\texp_name = 'junk'\n",
    "}\n",
    "\n",
    "args['sweep'] = {\n",
    "\tNone:None\n",
    "}\n",
    "\n",
    "c = Pyfig(wandb_mode='online', args=args, get_sys_arg=False)\n",
    "# c.submit()  # for remote submission\n",
    "\n",
    "n_device = c.n_device\n",
    "print(f'ü§ñ {n_device} GPUs available')\n",
    "\n",
    "# from pprint import pprint\n",
    "# pprint(c.d)\n",
    "\n",
    "\"\"\" live plotting in another notebook \"\"\"\n",
    "\"\"\" copy lines and run in analysis while the exp is live \"\"\"\n",
    "# api = wandb.Api()\n",
    "# run = api.run(\"<run-here>\")\n",
    "# c = run.config\n",
    "# h = run.history()\n",
    "# s = run.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp/actual | \n",
      "\trng    : (2,)/(2,) \n",
      "\trng_p  : (1, 2)/(1, 2) \n",
      "\tcps    : (4, 3)/(4, 3)\n",
      "\tr      : (1, 512, 4, 3)/(1, 512, 4, 3)\n",
      "\tdeltar : (1, 1)/(1, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### model (aka TrainState) ### \n",
    "from functools import partial\n",
    "import jax\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "from hwat import FermiNet\n",
    "\n",
    "@partial(jax.pmap, axis_name='dev', in_axes=(0,0))\n",
    "def create_train_state(rng, r):\n",
    "\tmodel = c.partial(FermiNet)\n",
    "\tparams = model.init(rng, r)\n",
    "\topt = optax.chain(optax.clip_by_block_rms(1.),optax.adamw(0.001))\n",
    "\treturn TrainState.create(apply_fn=model.apply, params=params, tx=opt)\n",
    "\n",
    "\n",
    "### train step ###\n",
    "from jax import numpy as jnp\n",
    "from hwat import compute_ke_b, compute_pe_b\n",
    "from typing import NamedTuple\n",
    "\n",
    "@partial(jax.pmap, in_axes=(0, 0))\n",
    "def train_step(state, r_step):\n",
    "\n",
    "\tke = compute_ke_b(state, r_step)\n",
    "\tpe = compute_pe_b(r_step, c.data.a, c.data.a_z)\n",
    "\te = pe + ke\n",
    "\t\n",
    "\te_mean_dist = jnp.mean(jnp.abs(jnp.median(e) - e))\n",
    "\te_clip = jnp.clip(e, a_min=e-5*e_mean_dist, a_max=e+5*e_mean_dist)\n",
    "\n",
    "\tdef loss(params):\n",
    "\t\treturn ((e_clip - e_clip.mean())*state.apply_fn(params, r_step)).mean()\n",
    "\t\n",
    "\tgrads = jax.grad(loss)(state.params)\n",
    "\tstate = state.apply_gradients(grads=grads)\n",
    "\t\n",
    "\tv_tr = dict(\n",
    "\t\tparams=state.params, grads=grads,\n",
    "\t\te=e, pe=pe, ke=ke,\n",
    "\t\tr=r_step\n",
    "\t)\n",
    "\n",
    "\treturn state, v_tr\n",
    "\n",
    "\n",
    "### init variables ###\n",
    "from utils import gen_rng\n",
    "from hwat import init_r, get_center_points\n",
    "from jax import random as rnd\n",
    "\n",
    "rng, rng_p = gen_rng(rnd.PRNGKey(c.seed), c.n_device)\n",
    "center_points = get_center_points(c.data.n_e, c.data.a)\n",
    "r = init_r(rng_p, c.data.n_b, c.data.n_e, center_points, std=0.1)\n",
    "deltar = jnp.array([0.02])[None, :].repeat(n_device, axis=0)\n",
    "\n",
    "print(f\"\"\"exp/actual | \n",
    "\trng    : {(2,)}/{rng.shape} \n",
    "\trng_p  : {(c.n_device,2)}/{rng_p.shape} \n",
    "\tcps    : {(c.data.n_e,3)}/{center_points.shape}\n",
    "\tr      : {(c.n_device, c.data.n_b, c.data.n_e, 3)}/{r.shape}\n",
    "\tdeltar : {(c.n_device, 1)}/{deltar.shape}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "### init functions ### \n",
    "from hwat import sample_b\n",
    "\n",
    "state = create_train_state(rng_p, r)\n",
    "metro_hast = jax.pmap(partial(sample_b, n_corr=c.data.n_corr), in_axes=(0,0,0,0))\n",
    "\n",
    "\n",
    "### train ###\n",
    "import wandb\n",
    "from hwat import keep_around_points\n",
    "from utils import compute_metrix\n",
    "\n",
    "wandb.define_metric(\"*\", step_metric=\"tr/step\")\n",
    "for step in range(1, c.n_step+1):\n",
    "\trng, rng_p = gen_rng(rng, c.n_device)\n",
    "\n",
    "\tr, acc, deltar = metro_hast(rng_p, state, r, deltar)\n",
    "\tr = keep_around_points(r, center_points, l=2.) if step < 1000 else r\n",
    "\t\n",
    "\tstate, v_tr = train_step(state, r)\n",
    "\n",
    "\tif not (step % c.log_metric_step):\n",
    "\t\tmetrix = compute_metrix(v_tr)\n",
    "\t\twandb.log({'tr/step':step, **metrix})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ```{toggle} env vars and jax debug config notes\n",
    "# ‚ùáÔ∏è Magic & debug not currently used\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %env CUDA_VISIBLE_DEVICES='3'\n",
    "# %env \"WANDB_NOTEBOOK_NAME\" \"run.ipynb\" # ‚ùïsame as notebook\n",
    "\n",
    "# from jax.config import config\n",
    "# config.update('jax_disable_jit', True)\n",
    "# ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eab66b48d160ed597e1c84bfc923f11a8415eedd02bf3953f166fcfe4ac828bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
